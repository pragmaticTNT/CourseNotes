\documentclass[11pt]{article}
\usepackage{structure}

\begin{document}
	\tableofcontents
	\newpage
	
	\makeheader{\today}{}{Circuit Complexity}{Lily Li}
	
	\section{Administrivia}
	\label{sec:admin}
	\begin{itemize}
		\item \textbf{Instructor:} Ben Rossman. 
		\item \textbf{Course Info:} Available at the \href{http://www.math.toronto.edu/rossman/CSC2429.html}{course website}. Just in case the website is down: lectures are Thursdays from 16:00 to 18:00 in Bahen B026. Office hours are by appointment.
		\item \textbf{Textbook:} \emph{Boolean Function Complexity} by Stasys Junkha. This is available as a free eBook through the University of Toronto library. 
		\item \textbf{Prerequisites:} None, but a previous complexity course is useful. Please read Appendix A.1 of the textbook and understand the material.
		\item \textbf{Workload:} Homework assignment(s), scribe notes, paper report (5 to 10 pages), and presentation if you so choose. No exams.
	\end{itemize}
	
	\section{Basic Definitions}
	\subsection{Boolean Functions}
	\label{section:definitions}
	\begin{definition}
		A \textbf{$n$-ary Boolean function} $f$ is a function of the form $f: \{0,1\}^n \rightarrow \{0,1\}$. Usually we interpret $(0,1)$ as $(\false, \true)$ or as $(1,-1)$ --- this makes sense if you think of it as $(-1)^0$ and $(-1)^{1}$.
	\end{definition}
	
	Let $\{0,1\}^* = \cup_{n \in \NN} \{0,1\}^n$. We typically refer to a family of Boolean function(s) $f: \{0,1\}^* \rightarrow \{0,1\}$. This corresponds to a sequence of functions $f_n: \{0,1\}^n \rightarrow \{0,1\}$ and to a language $L \subseteq \{0, 1\}^*$ described by its characteristic function $f_L: \{0,1\}^* \rightarrow \{0,1\}$.
	
	\begin{example}
		The following are some examples of $n$-ary Boolean functions:
		\begin{enumerate}
			\item $\ff{PARITY}(x_1, ..., x_n) = \sum_{i = 1}^{n} x_i \mod 2$.
			\item $\ff{MOD}_p(x_1, ..., x_n) = 1 \iff \sum_{i = 1}^{n} x_i \equiv 0 \mod p$.
			\item $\ff{MAJORITY}_n(x_1, ..., x_n) = 1 \iff \sum_{i=1}^{n} x_i \geq \ceil{n/2}$.
			\item $k-\ff{CLIQUE}: \{0,1\}^{\binom{n}{2}} \rightarrow \{0,1\}$. Think of each graph $G$ as an indicator vector $\ind_G$ over its $\binom{n}{2}$ edges. Then $k-\ff{CLIQUE}(\ind_G) = 1$ if and only if $G$ has a $k$-clique.
		\end{enumerate}
	\end{example}
	
	Let us consider DeMorgan circuits. These contain logical connectives $\{\lor, \land, \lnot\}$, input variables $\{x_1, ..., x_n\}$, and constants $\{0,1\}$. 
	
	\begin{definition}
		\label{def:demorgancircuit}
		A \textbf{$n$-ary DeMorgan circuit} is a finite directed acyclic graph (DAG) with nodes labelled as follows:
		\begin{itemize}
			\item Nodes of in-degree zero (``inputs'') are labelled by a variable or a constant.
			\item Non-input nodes (``gates'') of in-degree one are labelled with $\lnot$. Gates of in-degree two are labelled with $\lor$ or $\land$.
			\item A subset of the nodes are designated as ``outputs'' (default: \emph{the} node with out-degree zero).
		\end{itemize}
		Two circuits are \textbf{equivalent} if they compute the same function.
	\end{definition}
	
	\textbf{Formulas} are tree-like circuits. Since different branches in a formula depend on different copies of the variables, formulas are memory-less. See Figure \ref{fig:circuitandformula}. Proving that formulas are polynomially weaker than circuits is still an open problem.
	
	\begin{figure}[ht]
		\centering
		\begin{tikzpicture}[->, >=stealth', shorten >= 1pt, auto, node distance=4em, baseline=(current bounding box.center)]
		\node[state,accepting]	(out) 					{$\land$};
		\node[state]	(or)[below left of=out]	{$\lor$};
		\node[state]	(not)[below right of=out]{$\lnot$};
		\node[state]	(and)[below right of=not]{$\land$};
		\node[state]	(x1)[below left of=or]	{$x$};
		\node[state]	(y1)[below right of=or]	{$y$};
		\node[state]	(x2)[below left of=and]	{$x$};
		\node[state]	(y2)[below right of=and]{$y$};
		\path 	(x1)	edge	node{} 	(or)
		(y1)	edge	node{}	(or)
		(x2)	edge	node{}	(and)
		(y2)	edge	node{}	(and)
		(or)	edge	node{}	(out)
		(and)	edge	node{}	(not)
		(not)	edge	node{}	(out);
		\end{tikzpicture}
		\qquad
		\begin{tikzpicture}[->, >=stealth', shorten >= 1pt, auto, node distance=4em, baseline=(current bounding box.center)]
		\node[state,accepting]	(out)							{$\land$};
		\node[state]	(not)[below right of=out]		{$\lnot$};
		\node[state]	(and)[below of=not]		{$\land$};
		\node[state]	(or)[left of=and]				{$\lor$};
		\node[state]	(z)[below of=and]				{$z$};	
		\node[state]	(outsub)[left of=z]		 		{$\land$};
		\node[state]	(notsub)[left of=or]			{$\lnot$};
		\node[state]	(andsub)[below of=notsub]		{$\land$};	
		\node[state]	(orsub)[left of=notsub]			{$\lor$};
		\node[state]	(x)[below of=orsub]				{$x$};
		\node[state]	(y)[below of=andsub]			{$y$};
		\path 	(x)		edge	node{} 	(orsub)
		(y)		edge	node{}	(orsub)
		(x)		edge	node{}	(andsub)
		(y)		edge	node{}	(andsub)
		(orsub)	edge	node{}	(outsub)
		(andsub)	edge	node{}	(notsub)
		(notsub)	edge	node{}	(outsub)
		(outsub)	edge	node{}	(or)
		(outsub)	edge	node{}	(and)
		(z)		edge	node{}	(or)
		(z)		edge	node{}	(and)
		(or)	edge	node{}	(out)
		(and)	edge	node{}	(not)
		(not)	edge	node{}	(out);
		\end{tikzpicture}
		\caption{(Left) Formula computing $x \xor y$. (Right) Circuit computing $x \xor y \xor z$.}
		\label{fig:circuitandformula}
	\end{figure}
	
	\begin{definition}
		\label{def:circuitsize}
		The \textbf{size} of a circuit is the number of $\lor$ and $\land$ gates it contains. 
		
		The \textbf{leaf-size} of a formula is the number of leaves in its associated DAG. This is one more than the circuit size as defined above. 
		
		The \textbf{circuit size} of an $n$-ary Boolean function $f: \{0,1\}^n \rightarrow \{0,1\}$, written $\circuitsize(f)$, is the minimum size of a circuit computing $f$. Similarly, the \textbf{formula (leaf) size} of $f$, written $\leafsize(f)$, is the minimum size of a formula computing $f$. 
		
		The \textbf{depth} of a circuit is the maximum number of $\land$ and $\lor$ gates on any input to output path.
	\end{definition}
	
	\begin{example}
		It is a major open problem to compute the circuit and leaf size lower bounds for various Boolean functions. A couple of known results are as follows.
		\begin{center}
			\begin{tabular}{c|c|c}
				$f$ 		& $\leafsize(f)$ 	& $\circuitsize(f)$\\ \hline
				&&\\[-1em]
				$AND_n$ 	& $n$ 				& $n-1$\\ \hline
				&&\\[-1em]
				$PARITY_n$ 	& $\Theta\left(n^2\right)$ 	& $3(n-1)$\\
			\end{tabular}
		\end{center}
		The results for $AND_n$ are tight since the output depends on all the inputs. Improving the gap size between $\leafsize(PARITY_n)$ and $\circuitsize(PARITY_n)$ would separate $\class{NC}_1$ from $\class{P}$.
	\end{example}
	
	\subsection{Other Ways of Measuring Size}
	Other ways of counting the size of a circuit include: (1) counting the number of wires and (2) counting all gate types (including $\lnot$ gates). It turns out that the result of these calculations differ from our definition above by at most a factor of two. It should be easy to see why this is in the former case. Every $\land$ and $\lor$ gate has two incoming wires. Claim \ref{claim:negationnotaproblem} shows this in the latter case.
	
	\begin{definition}
		\label{def:negationnormalform}
		The input to every $\lnot$ gate in a circuit in \textbf{negation normal form} is a variable. See Figure \ref{fig:negationnormalform}.
	\end{definition}
	\begin{figure}[ht]
		\centering
		\begin{tikzpicture}[->, >=stealth', shorten >= 1pt, auto, node distance=4em, baseline=(current bounding box.center)]
		\node[state,accepting]	(not1) 					{$\lnot$};
		\node[state]	(and1)[below of=not1]	{$\land$};
		\node[state]	(x)[below left of=and1]	{$x$};
		\node[state]	(y)[below right of=and1]{$y$};
		\path 	(x)	edge	node{} 	(and1)
		(y)	edge	node{}	(and1)
		(and1)	edge	node{}	(not1);
		\end{tikzpicture}
		\qquad$\implies$\qquad
		\begin{tikzpicture}[->, >=stealth', shorten >= 1pt, auto, node distance=4em, baseline=(current bounding box.center)]
		\node[state,accepting]	(or)					{$\lor$};
		\node[state]	(not1)[below left of=or]{$\lnot$};
		\node[state]	(not2)[below right of=or]{$\lnot$};
		\node[state]	(x)[below of=not1]		{$x$};
		\node[state]	(y)[below of=not2]		{$y$};
		\path 	(x)	edge	node{} 	(not1)
		(y)	edge	node{}	(not2)
		(not1)	edge	node{}	(or)
		(not2)	edge	node{}	(or);
		\end{tikzpicture}
		\caption{Apply DeMorgan's Law to all $\lnot$ gates whose inputs are not literals on the left circuit to get the equivalent right circuit in negation normal form.}
		\label{fig:negationnormalform}
	\end{figure}
	
	\begin{claim}
		\label{claim:negationnotaproblem}
		Every circuit $C$ of size $m$ is equivalent to a circuit in negation normal form of size $\leq 2m$. 
	\end{claim}
	\begin{proof}
		Apply DeMorgan's law to every $\lnot$ gate whose input is not a variable. This switches the order of $\lnot$ and $\land$/$\lor$ in the DAG and adds an additional $\lnot$ gate. By the end of the process we have added at most $m$ $\lnot$ gates.  
	\end{proof}
	
	Thus we can push all $\lnot$ gates to the bottom and interpret the inputs as literals (variables and their negation). We can also modify the definition of leaf-size to only count leaves leading to literals (never-mind the constants).
	
	\subsection{General Basis}
	A \textbf{basis} $B$ is a set of Boolean functions (or ``gate types''). Examples of basis include: 
	\begin{itemize}
		\item DeMorgan basis: $\{\land, \lor, \lnot\}$.
		\item Full binary basis: all Boolean functions $\{0,1\}^2 \rightarrow \{0,1\}$ (for example, you would get $\xor$).
		\item Monotone basis: $\{\land, \lor\}$ (NOT universal).
		\item $\class{AC}^0$ basis: $\{\land^{k}, \lor^k, \lnot: k \in \NN\}$ which are unbounded fan-in $\land$ and $\lor$ gates.
	\end{itemize}
	For a function $f$, let $\leafsize_B(f)$ and $\circuitsize_B(f)$ be the leaf and circuit size of $f$ with formulas and circuits built from gates of basis $B$. A basis is \textbf{universal} if it computes all functions. For two universal basis $B_1$ and $B_2$ it is possible to build a circuit using gates from $B_1$ which simulates any gate from $B_2$. If all functions in $B_1$ and $B_2$ have constant arity, it follows that $\circuitsize_{B_2}(f) = O(\circuitsize_{B_1}(f))$; for formula size, the relation is $\leafsize_{B_2}(f) = \leafsize_{B_1}(f)^{O(1)}$. This polynomial blow-up is unavoidable in some cases. Recall the function $PARITY_n$: $\leafsize_{\{\land, \lor, \lnot\}}(PARITY_n) = \Theta(n^2)$ whereas $\leafsize_{\{\xor\}}(PARITY_n) = n-1$.
	
	\subsection{Models of Computation}
	\label{sec:uniformvsconcrete}
	\begin{definition}
		\label{def:mmodelsofcomputation} 
		A \textbf{uniform model of computation} is a single machine/program with a finite description which operates on all inputs in $\{0,1\}^*$. Examples range from simple finite automata (where we have lower bounds ala the pumping lemma) to complex Turing Machines (lower bounds much harder to come by).
		
		Recall that a language $L \subseteq \{0,1\}^*$ can be interpreted as a sequence of functions $(f_0, f_1, ...)$ where $f_n: \{0,1\}^n \rightarrow \{0,1\}$ and $f_n(\vv{x}) = 1 \iff \vv{x} \in L$ for any $\vv{x} \in \{0,1\}^n$. A \textbf{non-uniform (concrete) model of computation} is a sequence $(C_0, C_1, ...)$ of combinatorial objects (namely circuits) where $C_n$ computes $f_n$. Examples include: circuits in the DeMorgan basis, restricted class of circuits (formulas, monotone model), decision trees, etc.  
	\end{definition}
	
	Observe that the non-uniform model of computation is more powerful than the uniform one since the finite program can be used as every combinatorial objects in the sequence. It follows that lower bounds in the non-uniform model also imply lower bounds in the uniform model. While upper bounds in the uniform model imply upper bounds in the non-uniform model. \emph{We want: unconditional lower bounds.} 
	
	Circuits efficiently simulate Turing Machines. 
	\begin{lemma}
		Any Turing Machine (TM) $M$ with running time $t(n)$ can be simulated by a circuit (family of) of size $O\left(t(n)^2\right)$.
	\end{lemma}
	Exercise for the reader. \emph{Hint: think about configurations of the Turing Machines as a $t(n) \times t(n)$ grid and construct a circuit for every grid cell.} Fischer and Pipenger (1979) proved an $O(t(n)\log t(n))$ upper bound on \emph{oblivious Turing Machines}\footnote{An oblivious TM is one whose head motion depends only on the size of the input and not its particular bits. Take a look at \href{https://rjlipton.wordpress.com/2009/07/28/oblivious-turing-machines-and-a-crock/}{this} blog post for some entertainment.}. It is unknown if we can do better. 
	
	\begin{corollary}
		If there is a super polynomial lower-bound (better than $\Omega(n^c)$ for all constants $c > 0$) on the circuit size of any language in $\class{NP}$, then $\class{P} \neq \class{NP}$. 
	\end{corollary}
	Finding the lower-bound would actually show $\class{NP} \not\subseteq \class{P/poly}$ where $\class{P/poly}$ is the class of languages decidable by $\poly(n)$-size circuits.
	
	We will see some polynomial lower bounds for formulas in the DeMorgan basis later on. As a historical curio, the following is a catalogue of lower bound results for an explicit Boolean function:
	\begin{enumerate}
		\item $\Omega(n^{1.5})$ Subboboskay '61
		\item $\Omega(n^{2})$ Khrapchenko '71
		\item $\Omega(n^{2.5 - o(1)})$ Andreev '83
		\item $\Omega(n^{3 - o(1)})$ H\aa stad '98 (this is the state of the art until very recently). 
	\end{enumerate}
	
	\section{DeMorgan Basis}
	\subsection{Balancing Formulas}
	Next we consider the relationship between circuit size and depth. First observe that every circuit of depth $d$ is equivalent to a formula of size  at most $2^d$. To see this, take the circuit and duplicate any branches that gets reused. The resulting binary tree has at most as many nodes as a perfect binary tree of depth $d$ which itself has circuit size $2^d$.   
	
	The next theorem shows the converse: every formula of size $s$ can be ``balanced'' to obtain a formula of depth $O(\log s)$.
	\begin{theorem}
		\textbf{(Spira 1971).} Every formula with leaf-size $s$ is equivalent to a formula of depth $O(\log s)$ ($2\log_{3/2}(s)$ to be exact) and thus size at most $s^{O(1)}$ ($s^{2/\log_2(3/2)}$).
	\end{theorem}
	\begin{proof}
		By induction on $s$. The base case is trivial. Let $F$ be the original formula and $g$ be some gate. Let $F_g$ be the sub-formula rooted at $g$. For $b \in \{0,1\}$, let $F^{(g \leftarrow b)}$ be the formula with $g$ replaced with the constant value $b$. See Figure \ref{fig:spira1971}. Note that $\leafsize(F) = \leafsize\left(F_g\right) + \leafsize\left(F^{\left(g \leftarrow b\right)}\right)$. Minimize $\leafsize(F)$ by balancing the two terms on the RHS. By Claim \ref{claim:pickgoodgate}, we can find a gate $g$ such that $\frac{s}{3} \leq \leafsize(F_g) \leq \frac{2s}{3}$. 
		
		\fig{spira1971}{1}{Illustration of gate $g$ and formulas $F_g$ and $F^{g \leftarrow b}$.}
		
		Note that $F \equiv \left(F_g \land F^{(g \leftarrow 1)}\right) \lor \left((\lnot F_g) \land F^{(g \leftarrow 0)}\right)$; $F_g$ must evaluate to $0$ or $1$ and the formula does just that. Apply the induction hypothesis to the four formulas $F_g$, $F^{(g \leftarrow 1)}$, $\lnot F_g$, and $F^{(g \leftarrow 0)}$ to get formulas of depth $\leq 2 \log_{3/2}(2s/3)$. The original formula $F$ can only grow by at most depth two so
		\begin{align*}
			\depth(F) &\leq \max\left\{\depth\left(F_g\right), \depth\left(F^{(g \leftarrow 1)}\right), \depth\left(\lnot F_g\right), \depth\left(F^{(g \leftarrow 0)}\right)\right\} + 2\\
			&\leq 2\log_{3/2}\frac{2s}{3} + 2\\ 
			&= (2\log_{3/2}s - 2) + 2\\
			&= 2\log_{3/2}s
		\end{align*} 
		Thus there exists a formula equivalent to $F$ of depth at most $O(\log s)$.
	\end{proof}
	
	\begin{claim}
		\label{claim:pickgoodgate}
		There exists a gate $g$ such that $F_g$ has leaf-size between $\frac{s}{3}$ and $\frac{2s}{3}$ leaves.
	\end{claim}
	\begin{proof}
		Let $r \rightsquigarrow \ell$ be a root to leaf path in the DAG containing the most $\land$/$\lor$ gates. At the root $r$, $\leafsize(F_r) = \leafsize(F) = s$ and at the leaf $\ell$, $\leafsize(F_{\ell}) = 1$. Starting at $r$ and moving down to $\ell$, the successive leaf-sizes can at most halve after each step. Thus there must exists a gate $g$ for which $\frac{s}{3} \leq \leafsize(F_g) \leq \frac{2s}{3}$.
	\end{proof}
	
	\subsection{Circuit Size of General Boolean Functions}
	\label{ssec:circuitsize}
	
	\subsubsection{Upper Bound}
	\label{sssec:circuitsizeupperbd}
	Given a function $f: \{0,1\}^n \rightarrow \{0,1\}$, let us consider some upper bounds for $\circuitsize(f)$.
	\begin{enumerate}
		\item Brute force DNF: $O(n2^n)$. There are $2^n$ rows in the truth table of $f$. Each row specifies the output given the $n$ inputs. Thus a clause with $n-1$ $\land$ gates represents each row of the table. Formally we consider the expression
		\[f(\vv{x}) = \bigvee_{\vv{a} \in f^{-1}(1)} (\vv{x} = \vv{a}) = \bigvee_{\vv{a} \in  \in f^{-1}(1)} \left(l_1 \land l_2 \land \cdots \land l_{n-1} \land l_n\right)\]
		where $l_i = x_i$ if $a_i = 1$ and $l_i = \overline{x}_i$ otherwise.
		\item Function decomposition: $O(2^n)$. Observe that 
		\[f(\vv{x}) \equiv \left(x_n \land f_1(\vv{x})\right) \lor \left(\overline{x}_n \land f_0(\vv{x})\right).\]
		where $f_1 = f(x_1, ..., x_{n-1}, 1)$ and $f_0 = f(x_1, ..., x_{n-1}, 0)$. Thus
		\[\circuitsize(f) \leq \circuitsize(f_1) + \circuitsize(f_0) + 3.\]
		Apply the decomposition recursively to $f_1$ and $f_0$. Generally at step $k$,
		\[\circuitsize(f) \leq \sum_{\vv{a} \in \{0,1\}^k}\circuitsize(f_{\vv{a}}) + 3(2^k - 1)\]
		where $f_{\vv{a}}(\vv{x}) = f(x_1, ..., x_{n-k}, a_1, ..., a_k)$. Since $f(\vv{a})$ is a constant at the $n$\textsuperscript{th} step, $3(2^n - 1)$ is an upper bound on the circuit size of $f$. 
		\item Computation reuse: $O(2^n/n)$. See Theorem \ref{thm:lupanov1958-circuitsizeupperbd} below. 
	\end{enumerate}
	
	Let $ALL_n: \{0,1\}^n \rightarrow \{0,1\}^{2^{2^{n}}}$ be the function which calculates all the $n$-ary Boolean functions at the same time\footnote{To see that the range of $ALL_n$ is indeed $2^{2^{n}}$, recall that the domain of every $n$-ary Boolean function is $2^{n}$. There is a bijection between the set of functions and the power set of $\{0,1\}^n$ (of size $2^{2^{n}}$).}. That is $(ALL_n(\vv{x}))_f \coloneqq f(\vv{x})$ for any $n$-ary Boolean function $f$.
	
	\begin{claim}
		\label{claim:upperbdalln}
		$\circuitsize(ALL_n) \leq O(2^{2^{n}})$.
	\end{claim}
	\begin{proof}
		Similar to the function decomposition analysis. For every function $f$ in the output of $ALL_n$, $f(\vv{x}) \equiv \left(x_n \land f_{1}(\vv{x})\right) \lor \left(\overline{x}_n \land f_{0}(\vv{x})\right)$ where $f_1 = f(x_1, ..., x_{n-1}, 1)$ and $f_0 = f(x_1, ..., x_{n-1}, 0)$. Note that $f_1$ and $f_0$ are outputs of $ALL_{n-1}$. See Figure \ref{fig:circuitsizealln}. Since $ALL_n$ has $2^{2^{n}}$ outputs,
		\[\circuitsize(ALL_n) \leq \circuitsize(ALL_{n-1}) + 3\left(2^{2^{n}}\right) = c\left(2^{2^{n-1}}\right) + 3\left(2^{2^{n}}\right) \in O\left(2^{2^{n}}\right)\]
		for some constant $c$. 
		\fig{circuitsizealln}{1}{Obtaining a circuit for $ALL_n$ from a circuit for $ALL_{n-1}$.}
	\end{proof}
	
	\begin{theorem}
		\label{thm:lupanov1958-circuitsizeupperbd}
		\textbf{(Lupanov 1958).} Every $n$-ary Boolean function has circuit size $O(2^n/n)$
	\end{theorem}
	\begin{proof}
		The key idea is to use $ALL_{n-k}$ in place of $\{f_{\vv{a}}: \vv{a} \in \{0,1\}^k\}$ in the analysis of function decomposition. Formally, we have 
		\[\circuitsize(f) \leq \sum_{\vv{a} \in \{0,1\}^k}\circuitsize(f_{\vv{a}}) + 3(2^k - 1) \leq \circuitsize(ALL_{n-k}) + 3(2^k - 1) \leq O\left(2^{2^{n-k}}\right) + O(2^k)\]
		where the last inequality follows from Claim \ref{claim:upperbdalln}. Observe that the two terms on the RHS are balanced when $k = n - \log(n - \log n)$ since
		\begin{align*}
			O\left(2^{2^{n-k}}\right) + O\left(2^k\right) &= O\left(2^{2^{\log(n - \log n)}}\right) + O\left(2^{n - \log(n - \log n)}\right)\\
			&= O\left(2^{n - \log n}\right)\\
			&= O\left(2^n/n\right)
		\end{align*}
		It follows that the circuit complexity of all $n$-ary Boolean function is bounded above by $O(2^n/n)$.
	\end{proof}
	
	\subsubsection{Lower Bound}
	\label{sssec:circuitsizelowerbd}
	Prior to Lupanov's result above, Shannon showed a matching lower bound. 
	\begin{theorem}
		\label{thm:shannon1949-circuitsizelowerbd}
		\textbf{(Shannon 1949).} Almost all $n$-ary Boolean functions (as $n \rightarrow \infty$) have circuit size $O(2^n/n)$.
	\end{theorem}
	\begin{proof}
		Use the counting argument. Recall the number of $n$-ary Boolean functions is $2^{2^{n}}$ and let $s = \frac{2^n}{n}$. We will show that the number of Boolean functions which can be computed by circuits of size $s$ is $\ll 2^{2^{n}}$. Let $A$ be the set of all $n$-ary circuits with $2n$ literals, $x_1, ..., x_n, \overline{x}_1, ..., \overline{x}_n$, and $s$ gates, denoted $g_1, ..., g_s$. We obtain an upper bound on the number of circuits in $A$ as follows. Each circuit can use any subset of the $s$ gates. Each $\land$/$\lor$ gate can pick two inputs from the $2n$ literals and $s-1$ other gates. If $n$ is sufficiently large (say $n \geq 100$), then $s + 2n < 3s$ so
		\[|A| \leq 2^s(s + 2n)^{2s} \leq 18^ss^{2s}.\] 
		Observe that every $n$-ary function with $\circuitsize(f) \leq s$ is computed by at least $s!$ distinct circuits in $A$ since we can permute the labels on the $s$ gates. Thus the total number of Boolean functions computed by circuits in $A$ is at most $\frac{|A|}{s!}$. Recall that $s! \geq \left(\frac{s}{e}\right)^s$. For $s = \frac{2^n}{n}$,
		\begin{align*}
			\frac{|A|}{s!} \leq \frac{18^ss^{2s}}{\left(s/e\right)^s} \leq 50^s s^{s} = 50^{2^n/n}\left(\frac{2^n}{n}\right)^{2^{n}/n} = \left(\frac{50}{n}\right)^{2^n/n}\left(2^{n}\right)^{2^{n}/n} \leq 2^{2^n-2^n/n}
		\end{align*}
		since $n \geq 100$. Thus at least $2^{s}$ Boolean formulas have circuit size greater than $s$.
	\end{proof}
	
	\subsection{Circuit Size Hierarchy}
	\begin{theorem}
		\label{thm:circuitsizehierarchy}
		If $n \leq s(n) \leq \frac{2^{n-2}}{n}$, then $\class{SIZE}[s] \subsetneqq \class{SIZE}[4s]$.
	\end{theorem}
	\begin{proof}
		Use a combination of Shannon (Theorem \ref{thm:shannon1949-circuitsizelowerbd}) and Lupanov (Theorem \ref{thm:lupanov1958-circuitsizeupperbd}). Pick\footnote{Such an $m$ must exists. When $m = 1$, $2^{m}/m \leq s(n)$ and when $m = n-1$, $2^m/m \geq s(n)$ so there must be some $m$ such that $2^{m}/m \leq s(n)$ and $2^{m+1}/(m+1) \geq s(n)$. If $2^{m+1}/(m+1) \geq 2\cdot s(n)$ then 
			\[s(n) \leq \frac{2^{m}}{m+1} \leq \frac{2^{m}}{m}\]	
			which contradicts our original choice of $m$.} 
		an $m < n$ such that
		\[s(n) \leq \frac{2^m}{m} \leq 2s(n).\]
		By Shannon, there exists a function $f: \{0, 1\}^m \rightarrow \{0,1\}$ such that 
		\[\circuitsize(f) > \frac{2^m}{m} \geq s(n).\]
		Thus $f \notin \class{SIZE}[s]$. By the tight bound from Lupanov's theorem, $\circuitsize(f) \leq 2^m/m + o(2^m/m)$ so 
		\[\circuitsize(f) \leq \frac{2\cdot 2^m}{m} \leq 4s(n)\]
		and $f \in \class{SIZE}[4s]$. 
	\end{proof}
	
\section{Lower Bounds for Explicit Functions}
	\subsection{(DeMorgan) Linear Algebra Method: \texorpdfstring{$\leafsize(PARITY_n) \in \Omega(n^2)$}{L(PARITYn) in Omega(n2)}}
	Let us define $PARITY_n$ as $\XOR_n$ and $1 - PARITY_n$ as $\overline{\XOR}_n$. Recall\footnote{Construct a circuit with $n-1$ $\xor$-gates and substituting three DeMorgan gates $(x \land \lnot y) \lor (\lnot x \land y)$ for each $x \xor y$.} that $\circuitsize(\XOR_n) \leq 3(n-1)$ and $\leafsize(\XOR_n) \leq 2^{2\ceil{\log n}}$. We will show that these bounds are tight. 
	
	Notation: $\lambda(\mm{P})$ is the largest eigenvalue of a symmetric matrix $\mm{P}$. Recall\footnote{I think this can be shown as follows. Take the largest eigen-vector $\vv{x}$ of $\mm{P}$ and decompose it in the eigen-basis of $\mm{Q}$. Then right-multiplying $\mm{P} + \mm{Q}$ by $\vv{x}$.} that 
	\[\lambda(\mm{P} + \mm{Q}) \leq \lambda(\mm{P}) + \lambda(\mm{Q}).\]
	For non-empty $A, B \subseteq \{0,1\}^n$, the matrix $\mm{M} \subseteq \{0,1\}^{A\times B}$ is the matrix 
	\[\mm{M}_{a,b} = \begin{cases}
	1 &\mbox{if $a_i \neq b_i$ for exactly one $i$}\\
	0 &\mbox{otherwise}
	\end{cases}\]
	you can read this as ``the hamming distance of $\vv{a}$ and $\vv{b}$ differs by exactly one''. Note that $\mm{M}^{\intercal}\mm{M} \in \NN^{B \times B}$ with entry $(i,j)$ interpreted as ``the number of vectors $\vv{a} \in A$ such that both $\vv{b}_i$ and $\vv{b}_j$ are one away from $\vv{a}$''. Similarly $\mm{M}\mm{M}^{\intercal} \in \NN^{A \times A}$ with entry $(i,j)$ interpreted as ``the number of vectors $\vv{b} \in B$ such that both $\vv{a}_i$ and $\vv{a}_j$ are one away from $\vv{b}$''. It is a fact from linear algebra that $\mm{M}^{\top}\mm{M}$ and $\mm{M}\mm{M}^{\top}$ have the same non-zero eigen-values. In particular,  $\lambda(\mm{M}^{\intercal}\mm{M}) = \lambda(\mm{M}\mm{M}^{\intercal})$.
	
	\begin{theorem}
		\textbf{(Koutsoupias 1993).} For any $f: \{0, 1\}^n \rightarrow \{0,1\}$, $A \subseteq f^{-1}(0)$, and $B \subseteq f^{-1}(1)$,
		\[\leafsize(f) \geq \lambda(\mm{M}^{\intercal}\mm{M}).\]
	\end{theorem}
	\begin{proof}
		By induction on $\leafsize(f)$. The base case occurs when $\leafsize(f) = 1$ and the circuit only reads in one out of the $n$ variables of the input. W.l.o.g assume that the input to the leaf is $x_1$. Then $f(\vv{x}) = x_1$ or $f(\vv{x}) = 1 - x_1$; assume the former. Let $A = f^{-1}(0)$ and $B = f^{-1}(1)$. Then $A = \{0s: s \in \{0,1\}^{n-1}\}$ and $B = \{1s: s \in \{0,1\}^{n-1}\}$. Recall that entry $(i,j)$ of $\mm{M}^{\top}\mm{M}$ is the number of elements $\vv{a} \in A$ such that both $\vv{b}_i$ and $\vv{b}_j$ differ from $\vv{a}$ by one. Notice that $\vv{a} = 0s$ and $\vv{b} = 1s'$ differ by exactly one if and only if $s = s'$. Thus $\mm{M}^{\top}\mm{M}$ is exactly the identity matrix with dimension $|B| \times |B|$ and $\lambda(\mm{M}^{\top}\mm{M}) = 1$ satisfying the theorem.  
		
		In the inductive step, let $F$ be a formula which computes $f$ of size $\leafsize(f)$. Suppose that $F = F_1 \land F_2$ for some circuits $F_1$ and $F_2$. Let $f_1$ and $f_2$ be the functions computed by $F_1$ and $F_2$ respectively. Notices that $\leafsize(f) = \leafsize(f_1) + \leafsize(f_2)$. Let $A_1 = f_1^{-1}(0)$ and $A_2 = A \backslash A_1$. Since $F = F_1 \land F_2$, $A_2 \subset f_2^{-1}(0)$ as at least one of $F_1$ or $F_2$ must evaluate to $0$. Consider matrices $\mm{M}_1 \in \NN^{A_1 \times B}$ and $\mm{M}_2 
		\in \NN^{A_2 \times B}$. Notice that $\mm{M}^{\top}\mm{M} = \mm{M}_1^{\top}\mm{M}_1 + \mm{M}_2^{\top}\mm{M}_2$ since $A_1 \cup A_2 = A$ and each matrix product counts the number of off-by-one vectors $\vv{a}$ matched to by $\vv{b} \in B$. Then
		\begin{align*}
			\lambda(\mm{M}^{\top}\mm{M}) &= \lambda(\mm{M}_1^{\top}\mm{M}_1 + \mm{M}_2^{\top}\mm{M}_2) &\mbox{(definition)}\\
			&\leq \lambda(\mm{M}_1^{\top}\mm{M}_1) + \lambda(\mm{M}_2^{\top}\mm{M}_2) &\mbox{(symmtric matrix prop.)}\\ 
			&\leq \leafsize(f_1) + \leafsize(f_2) &\mbox{(induction hyp.)}\\ 
			&= \leafsize(f)
		\end{align*} 
		The same is true if $F = F_1 \lor F_2$, but this requires decomposing $B$. Remember however that $\lambda(\mm{M}^{\top}\mm{M}) = \lambda(\mm{M}\mm{M}^{\top})$ so it does not make much of a difference.
	\end{proof}
	
	\begin{corollary}
		\label{cor:khrapchenko1071-quadraticlb}
		\textbf{(Khrapchenko 1971).}
		\[\leafsize(f) \geq \frac{\left(\sum_{\vv{a} \in A}\sum_{\vv{b} \in B} \mm{M}_{a,b}\right)^2}{|A|\cdot|B|}\]
	\end{corollary}
	\begin{proof}
		\footnote{:) I like this} The idea is to write $\lambda(\mm{M}^{\top}\mm{M})$ as a Rayleigh quotient and then substitute in $\ind$ to get that lower bound.
		\begin{align*}
			\lambda\left(\mm{M}^{\top}\mm{M}\right) &= \max_{\vv{z} \in \RR^{B}\backslash \emptyset} \frac{\vv{z}^{\top}\mm{M}^{\top}\mm{M}\vv{z}}{\vv{z}^{\top}\vv{z}}\\
			&\geq \frac{\ind^{\top}\mm{M}^{\top}\mm{M}\ind}{|B|}\\
			&=\frac{\sum_{a \in A}\left(\sum_{b \in B}\mm{M}_{a,b}\right)^2}{|B|}\\
			&\geq \frac{\left(\sum_{a \in A} \sum_{b \in B} \mm{M}_{a,b}\right)^2}{|A|\cdot|B|}
		\end{align*}
		where the last inequality follows by Cauchy-Schwartz\footnote{The application of Cauchy-Schwartz here is subtle. The key is to multiply top and bottom by $\left(\sum_{a \in A} 1^2\right)$ and combine the two sum of squares.}.
	\end{proof}
	
	We use the above Corollary \ref{cor:khrapchenko1071-quadraticlb} to show that $\leafsize(\XOR_n) \geq n^2$. Take $A$ and $B$ to be the set of even and odd strings\footnote{Here the parity of the string $s$ corresponds to the parity of the sum of ones in $s$.} in $\{0,1\}^n$ respectively. Then, by the above,
	\[\leafsize(f) \geq \frac{\left(\sum_{a \in A}\sum_{b \in B} \mm{M}_{a,b}\right)^2}{|A|\cdot|B|} = \frac{\left(n2^{n-1}\right)^2}{2^{n-1}\cdot 2^{n-1}} = n^2.\]
	This technique can achieve gaps of at most $n^2$. Exercise: (1)\footnote{Hint: Take $A = \{s \in \{0,1\}^n:s\mbox{ has exactly $\ceil{n/2}-1$ ones}\}$ and $B = \{t \in \{0,1\}^n:t\mbox{ has exactly $\ceil{n/2}$ ones}\}$.} prove lower-bound $\leafsize(MAJ_n) \geq \Omega(n^2)$ and (2) can you devise an upper bound of $\leafsize(MAJ_n) \leq n^{O(1)}$.
	
	\subsection{(General) Gate Elimination: \texorpdfstring{$\circuitsize(PARITY_n) \in \Omega(n)$}{C(PARITYn) in Omega(n)}}
	\begin{definition}
		For $i \in [n]$ and $b \in \{0,1\}$ the \textbf{1-bit restriction}, $x_i \leftarrow b$ is the $n$-ary function $f^{(x_i \leftarrow b)}$. The substitution can be done \emph{syntactically} for circuits $C$, namely, $C^{(x_i \leftarrow b)}$. The technique is to substitute $x_i \leftarrow b$ and $\overline{x}_i \leftarrow 1 - b$ and performing the relevant simplifications.
	\end{definition}
	
	There are a couple of observations to note. (1) If $C$ computes $f$, then $C^{(x_i \leftarrow b)}$ computes $f^{(x_i \leftarrow b)}$. (2) If $x_i$ appears below a gate in $C$ then for both settings of $b \in \{0,1\}$, $\size\left(C^{(x_i \leftarrow b)}\right) \leq \size(C) - 1$ i.e. any setting of $b$ will knock out one gate in $C$. (2) There exists a setting of $b$ for each gate, such that $\size\left(C^{(x_i \leftarrow b)}\right) \leq \size(C) - 2$ i.e. the setting of $b$ knocks out two gates in $C$.
	
	\begin{theorem}
		\label{thm:schnorr1979-XORlb}
		\textbf{(Schnorr 1979).} $\circuitsize(PARITY_n) \geq 3(n-1)$.
	\end{theorem}
	\begin{proof}
		By induction. The base case where $n = 1$ is trivial. The crucial observation is as follows. If a literal is below $k$ $\land/\lor$ gates (of the same type), then there is a setting of the literal such that you can knock out at least $k$ gates. Just think about the different settings of the literal.
		
		Consider any circuit $C$ which calculates the $PARITY_n$ function. Identify three gates in $C$:
		\begin{enumerate}
			\item A gate whose inputs are two literals. Let these be $x_i$ and $x_j$. 
			\item Pick a literal of the previous gate, say $x_i$. Find another gate with $x_i$ as an input. Suppose such a gate does not exist. Then, by setting $x_j$ appropriately, we could knock out the gate in step $1$ and the output would not depend on $x_i$. This would not calculate the $PARITY_n$ function.
			\item The gate above the one in step 2. Such a gate exists if the gate from step $2$ is not the output of the circuit. Suppose for a contradiction that it was. Then a setting of $x_i$ would fix the output. This would also not calculate the $PARITY_n$ function.
		\end{enumerate}  
		By setting $x_i$ appropriately, we can kills all three gates above. See Figure \ref{fig:schnorr1979}.
		
		\fig{schnorr1979}{0.7}{The three gates that get eliminated when we restrict $x_i$. The actual setting of $x_i$ depends on the gate type.}
		
		By the induction hypothesis, $C^{(x_i \leftarrow b)}$ has at least $3(n - 2)$ gates. Since we were able to eliminate three gates by setting $x_i$, we know that $C$ has to have $3(n-1)$ gates.  
	\end{proof}
	More sophisticated versions of gate elimination allow for slightly better lower bounds. The current record is $5n - o(n)$ for DeMorgan circuits and $\left(3 + \frac{1}{86}\right)n$ for circuits in the full binary basis. 

	\subsection{(DeMorgan) Random Restriction: \texorpdfstring{$\leafsize(ANDREEV_{k,m}) \in \Omega(n^3)$}{L(ANDREEVkm) in Omega(n3)}}
	\subsubsection{Subbotovskaya's Method} 
	\begin{definition}
		A formula $F$ is \textbf{nice} if for every sub-formula of the form $x_i \land F'$, $\overline{x}_i \land F'$, $x_i \lor F'$, $\overline{x}_i \lor F'$, the variable $x_i$ does \emph{not} occur in $F'$.
	\end{definition}
	
	\begin{lemma}
		Every formula is equivalent to a \emph{nice} formula of the same (or less) leaf size.
	\end{lemma}
	\begin{proof}
		Given sub-formulas of the form $x_i \land F$, $\overline{x}_i \land F$, $x_i \lor F$, and $\overline{x}_i \lor F$ where $F$ contains literals $x_i$ or $\overline{x}_i$, repeatedly apply 
		\begin{align*}
			x_i \land F \rightarrow x_i &\land F^{(x_i \leftarrow 1)}\\
			\overline{x}_i \land F \rightarrow \overline{x}_i &\land F^{(x_i \leftarrow 0)}\\
			x_i \lor F \rightarrow x_i &\lor F^{(x_i \leftarrow 0)}\\
			\overline{x}_i \lor F \rightarrow \overline{x}_i &\lor F^{(x_i \leftarrow 1)}\\
		\end{align*}
		This shows that every minimal formula for a function $f$ is \emph{nice}.
	\end{proof}
	
	\begin{lemma}
		\label{lem:onebitrestrictionbd}
		For every $f: \{0,1\}^n \rightarrow \{0,1\}$,
		\[\expected_{i \in [n], b\in \{0,1\}}\left[\leafsize\left(f^{(x_i \leftarrow b)}\right)\right] \leq \left(1 - \frac{1}{n}\right)^{1.5} \leafsize(f).\]
	\end{lemma}
	\begin{proof}
		Let $F$ be a minimal nice formula for $f$. Let $\ell_i$ be the all leaves of $F$ labelled with $x_i$ or $\bar{x}_i$. Then $\leafsize(f) = \sum_{i = 1}^{n} \ell_i$.  Notice that every gate $g$ with a leaf $\lambda$ has an associated sub-formula $F'$ such that $\lambda$ does not occur in $F'$.
		
		For a bit $b \in \{0,1\}$, the random restriction $F^{(x_i \leftarrow b)}$ will kill leaf $x_i$ with probability $1$ and kill all leaves in $F'$ with probability $\frac{1}{2}$. Thus in expectation, $1.5$ leaves are killed under the 1-bit restriction $F^{(x_i \leftarrow b)}$. For each $i \in [n]$ we have
		\[\expected_{b \in \{0,1\}}\left[\leafsize(F) - \leafsize\left(F^{(x_i \leftarrow b)}\right)\right] \geq 1.5 \ell_i.\] 
		Averaging over all choices of $i$, we have that 
		\[\expected_{i \in [n], b \in \{0,1\}}\left[\leafsize(F) - \leafsize\left(F^{(x_i \leftarrow b)}\right)\right] \geq \frac{1.5}{n}\sum_{i = 1}^{n} \ell_i = \frac{1.5\leafsize(F)}{n}.\]
		Rearranging the above, we have
		\[\expected_{i \in [n], b \in \{0,1\}}\left[\leafsize\left(F^{(x_i \leftarrow b)}\right)\right] \leq \left(1 - \frac{1.5}{n}\right)\leafsize(F) \leq \left(1 - \frac{1}{n}\right)^{1.5} \leafsize(F)\]
		where the last inequality follows as $1 - ax \leq (1 - x)^a$.
	\end{proof}
	Apparently, this lemma implies that $\leafsize(\xor_n) \geq n^{1.5}$.
	
	\begin{definition}
		\label{def:restrictions}
		A \textbf{restriction} $\rho$ is a function $\rho: [n] \rightarrow \{0,1,*\}$ which can be thought of as a partial assignment of an $n$-ary Boolean function $f$. Denote the restriction of $f$ under $\rho$ as $f \upharpoonright \rho: \{0,1\}^{\rho^{-1}(*)} \rightarrow \{0,1\}$. Further $\rho$ is a \textbf{$k$-star restriction} if $|\rho^{-1}(*)| = k$. 
		
		Let $p\in [0,1]$. In a \textbf{$p$-random restriction} where you set
		\[R_p(i) = \begin{cases}
		*&\mbox{with probability } p\\
		0&\mbox{with probability } \frac{1-p}{2}\\
		1&\mbox{with probability } \frac{1-p}{2}
		\end{cases}\]
	\end{definition}
	
	\begin{theorem}
		\label{thm:subbotovskaya-restrictionlb}
		Let $f: \{0,1\}^n \rightarrow \{0,1\}$ and let $\rho$ be a uniform random $k$-start restriction. Then 
		\[\expected\left[\leafsize(f\upharpoonright \rho)\right] \leq \left(\frac{k}{n}\right)^{1.5}\leafsize(f).\]
	\end{theorem}
	\begin{proof}
		Repeatedly apply Lemma \ref{lem:onebitrestrictionbd} to restrict $k$ bits to get
		\[\expected\left[f \upharpoonright \rho\right] \leq \left(1 - \frac{1}{n}\right)^{1.5}\cdot\left(1 - \frac{1}{n-1}\right)^{1.5}\cdots\left(1 - \frac{1}{k+1}\right)^{1.5} \leafsize(f) = \left(\frac{k}{n}\right)^{1.5}\leafsize(f).\]
	\end{proof}
	
	\begin{corollary}
		\label{cor:extsubbotovskayatorandrestrictions}
		\textbf{(Subbotovskaya 1961).} 
		\[\expected\left[\leafsize\left(f\upharpoonright R_p\right)\right] \leq O\left(p^{1.5}\leafsize(f) + 1\right).\]
	\end{corollary}
	According to H\aa stad (19 something or other) and Tal (2014), this can be improved to $O(p^2\leafsize(f) + 1)$.
	
	\textbf{Open problem}: what is the shrinkage exponent of monotone formulas? (this is known to be between $2$ and $\left(\log (\sqrt{5}) - 1\right)^{-1} = 3.27$).

	\subsubsection{DEF: Composition of Boolean Functions}
	\begin{definition}
		Let $f:\{0,1\}^k \rightarrow \{0,1\}$ and $g: \{0,1\}^m \rightarrow \{0,1\}$. Let $f \comp g: (\{0,1\}^m)^k \rightarrow \{0,1\}$ is defined as
		\[(f \comp g)(\vv{x}_1, ..., \vv{x}_k) = f\left(g(\vv{x}_1), ..., g(\vv{x})\right).\]
		In essence the composition is of the form $f \comp g = f \circ g^k$. 
	\end{definition}
	Think of the input of the composition as a matrix $\mm{X} \in \{0,1\}^{k \times m}$ with rows $\vv{x}_1, ..., \vv{x}_k$. Apply $g$ to each row, then apply $f$ to the resulting column vector. Observe that $\leafsize(f \comp g) \leq \leafsize(f) \cdot \leafsize(g)$. 
	
	\begin{conjecture}
		\textbf{(KRW).} For all functions $f$ and $g$,
		\[\leafsize\left(f \comp g\right) = \tilde{\Omega}\left(\leafsize(f)\cdot \leafsize(g)\right)\]
		where $\tilde{\Omega}(t(n)) = \Omega(t(n))/\left(\log t(n)\right)^{O(1)}$ for any function $t(n)$.
	\end{conjecture}
	
	The following is an explicit $n$-ary Boolean function for which the lower bound is true.
	\begin{lemma}
		\label{lem:bdcompfwithxor}
		For all $k, m \geq 1$ and $f: \{0,1\}^k \rightarrow \{0,1\}$,
		\[\leafsize\left(f\comp XOR_m\right) \geq \leafsize(f) \cdot \Omega\left(\left(\frac{m}{\log k}\right)^2\right).\]
	\end{lemma}
	\begin{proof}
		Let $p = \frac{2\ln k}{m}$. Apply $R_p$ on $k\times m$ variables of $f \comp XOR_m$. If $R_p$ has a $*$ in every row then 
		\[\leafsize\left(\left(f\comp XOR_m\right) \upharpoonright R_p\right) \geq \leafsize(f)\]
		since a formula which calculates the LHS can be used to calculate the RHS. In particular, if there is a $*$ in some row $i$, then from the perspective of $XOR_m$, the value of row $i$ is undetermined. If every single row is undetermined, then the input to $f$ is undetermined. Thus $\left(f \comp XOR_m\right) \upharpoonright R_p$ would be able to compute $f(s)$ for any $s \in \{0,1\}^k$.
		
		Let $E$ be the event that there exists a $*$ in every row of the input matrix after applying $R_p$. We bound $\Pr[E]$ below by bounding $\Pr\left[\overline{E}\right]$ above. Let $B_i$ be the event that some row $i$ is \emph{bad} i.e. does not have a $*$ after applying $R_p$. Since every element is fixed with probability $1-p$, $\Pr[B_i] = (1 - p)^{m}$ for all $i \in [k]$. Then 
		\[\Pr\left[\overline{E}\right] \leq \sum_{i = 1}^{k} \Pr[B_i] = k(1-p)^{m} \approx k\exp(-pm) \leq \frac{1}{k}\] 
		where the first inequality follows by union-bound and the last by the definition of $p$ above. Thus we have the following lower bound
		\begin{equation}
		\label{eq:lbdlembdcomfwithxor}
		1 - \frac{1}{k} \leq \Pr\left[E\right] 
		\end{equation} 
		
		To get an upper bound for $\Pr[E]$, observe that $\Pr[E] \leq \Pr\left[\leafsize\left(\left(f \comp XOR_m\right) \upharpoonright R_p\right) \geq \leafsize(f)\right]$. Apply Markov's inequality to get
		\[\Pr\left[\leafsize\left(\left(f \comp XOR_m\right) \upharpoonright R_p\right) \geq \leafsize(f)\right] \leq \frac{\expected\left[\leafsize\left(\left(f \comp XOR_m\right) \upharpoonright R_p\right)\right]}{\leafsize(f)}\]
		By the improvement noted after Corollary \ref{cor:extsubbotovskayatorandrestrictions}, $\expected\left[f \upharpoonright R_p\right] \leq O\left(p^2\leafsize(f) + 1\right)$, we have
		\begin{equation}
		\label{eq:ubdlembdcomfwithxor}
		\Pr[E] \leq \frac{\expected\left[\leafsize\left(\left(f \comp XOR_m\right) \upharpoonright R_p\right)\right]}{\leafsize(f)} = O\left(\frac{p^2\leafsize(f \comp XOR_m) + 1}{\leafsize(f)}\right). 
		\end{equation} 
		
		Combining the lower bound from Equation (\ref{eq:lbdlembdcomfwithxor}) and the upper bound from Equation (\ref{eq:ubdlembdcomfwithxor}), we have
		\[1 - \frac{1}{k} \leq \Pr[E] \leq \frac{p^2\leafsize(f \comp XOR_m) + 1}{\leafsize(f)}\]
		Rearrange with respect to $\leafsize(f \comp XOR_m)$, taking care to observe that $1 - \frac{1}{k} - \frac{1}{\leafsize(f)} \in O(1)$, to obtain
		\[\leafsize\left(f \comp XOR_m\right) \geq \frac{\leafsize(f)}{p^2} = \leafsize(f) \cdot \Omega\left(\left(\frac{m}{\log k}\right)^2\right)\]
		as required. 
	\end{proof}
	
	\subsubsection{FUN: \texorpdfstring{$ANDREEV_{k,m}$}{ANDREEVkm}}
	Let us construct an explicit function with cubic lower bound on the leaf size using Lemma \ref{lem:bdcompfwithxor}.
	\begin{definition}
		For parameters $k, m \in \NN$,
		\[ANDREEV_{k,m}: \{\mbox{$k$-ary Boolean function}\} \times \{0,1\}^{k\times m} \rightarrow \{0,1\}\] 
		such that $ANDREEV(f, \mm{X}) = \left(f\comp XOR_m\right)(\mm{X})$.
		
		Think of this as follows: consider the $(k + 1) \times 2^{k}$ table $T$ of $k$-ary Boolean strings and the evaluation of $f$ on these strings. See Table \ref{table:andreev}. The input matrix $\mm{X} \in \{0,1\}^{k \times m}$. Apply the $XOR_m$ function to each row of $\mm{X}$ to obtain a $k$-bit string $s$. Find the column of $T$ corresponding to $s$ and return $f(s)$.
		\begin{table}[!ht]
			\centering
			\begin{tabular}{|c|c|c|}
				\hline
				$f(0^k)$ & $\cdots$ & $f(1^k)$\\
				\hline
				$0$ & $\cdots$ & $1$\\
				$\vdots$ & $\vdots$ & $\vdots$\\
				$0$ & $\cdots$ & $1$\\
				\hline
			\end{tabular}
			\caption{Table $T$ of function $f$.}
			\label{table:andreev}
		\end{table}	
	\end{definition}
	When $m = 1$, $ANDREEV_{k,1}$ is just the multiplexor function. Let $n = 2^k + mk$. Then $ANDREEV_{k,m}$ can be thought of as an $n$-ary Boolean function with $\circuitsize(ANDREEV_{k, m}) = O(n)$. 
	
	\begin{theorem}
		\label{thm:L-DmFormulaCubicExplicit-Andreev}
		For every $f: \{0,1\}^k \rightarrow \{0,1\}$ we have
		\[\leafsize\left(ANDREEV_{k,m}\right)\geq \leafsize\left(f\comp XOR_m\right) \geq \leafsize(f)\cdot\Omega\left(\left(\frac{m}{\log k}\right)^2\right).\]
	\end{theorem}
	\begin{proof}
		By fixing $2^k$ values $f(s)$ for $s \in \{0,1\}^k$, in the formula for $ANDREEV_{k,m}$, we can calculate $f \comp XOR_m$. Thus $\leafsize(ANDREEV_{k,m}) \geq \leafsize(f \comp XOR_m)$. By Shannon's Theorem \ref{thm:shannon1949-circuitsizelowerbd}, there exists a $k$-ary Boolean function $f$ with circuit size, and thus leaf size, $\Omega(2^k/k)$. Let $m = 2^k/k$ and note that $n = 2^k + mk \in \Theta(2^k)$. Then, by Lemma \ref{lem:bdcompfwithxor},
		\[\leafsize(ANDREEV_{k,m}) \geq \Omega\left(\frac{2^k}{k}\right) \cdot \Omega\left(\left(\frac{m}{\log k}\right)^2\right) = \Omega\left(\frac{n^3}{(\log n)^3(\log \log n)^2}\right).\]
		Thus $\leafsize(ANDREEV_{k,m}) \in \tilde{\Omega}(n^3)$.
	\end{proof}
	This lower bound for the $ANDREEV_{m,k}$ is nearly tight since $\leafsize(ANDREEV_{k,m}) \in \tilde{O}(n^3)$. 
	
	\subsection{(Full Binary) Subset Subfunction: \texorpdfstring{$\leafsize(ED_n) \in \Omega(n^2)$}{L(EDn) in Omega(n2)}}
	\subsubsection{DEF: \texorpdfstring{$V$}{V}-Subfunctions}
	Let $B_2$ be the full binary basis (all 2-ary gate types). Unfortunately, the random restriction idea does not work in this setting since it is \emph{not true} that
	\[\expected[\leafsize_{B_2}(f \upharpoonright R_p)] \leq O\left(p^{1 + \epsilon}\leafsize_{B_2}(f) + 1\right)\]
	for any $\epsilon > 0$. Do you see why?\footnote{Let $f$ be the parity function.} 
	
	\begin{definition}
		\label{def:NOTE-Vsubfunctions}
		For $f: \{0,1\}^n \rightarrow \{0,1\}$ and $V \subset [n]$, 
		\[\sub_V(f) = \{f\upharpoonright \rho: \rho:[n] \rightarrow \{0,1,*\} \mbox{ such that } \rho^{-1}(*) = V\}\] 
		be the set of \textbf{$V$-subfunctions} of $f$.
		
		Further, define 
		\[\sub_{V}^{*}(f) = \{f', 1 - f', \underline{0}, \underline{1}: f' \in \sub_V(f)\}\] where $\underline{b}$ is the constant $b$ function for $b \in \{0,1\}$. Note that $|\sub_{V}^{*}(f)| \leq 4\cdot|\sub_{V}(f)|$ (actually $|\sub_{V}^{*}(f)| \leq 2\cdot|\sub_{V}(f)| + 2$). 
		
		Let $F$ be an $n$-ary formula and $V \subset [n]$ as before. Then \textbf{the number of leaves of $F$ labelled by variables in $V$} be denoted $\ell_{V}(F)$. Note that $\leafsize(F) = \ell_V(F) + \ell_{[n]\backslash V}(F)$.
	\end{definition}
	
	\begin{example}
		Consider $MAJ_3(x_1, x_2, x_3)$ and $V = \{1, 2\}$. Then 
		\[\sub_V(MAJ_3) = \{x_1 \land x_2, x_1 \lor x_2\}\]
		when $x_3$ is restricted to $0$ and $1$ respectively. 
	\end{example}
	
	\subsubsection{Nechiporuk's Bound}
	Here are two important properties to note:
	\begin{enumerate}
		\item Suppose $F = \gate(G,H)$ for some $\gate: \{0,1\}^2 \rightarrow \{0,1\}$. Then 
		\[\sub_V(F) \subseteq \{\gate(g, h): g \in \sub_V(G) \mbox{ and } h \in \sub_V(H)\}.\]
		and $|\sub_V^*(F)| \leq |\sub_V^*(G)| \cdot |\sub_V^*(H)|$. This should be pretty obvious; let $f_V$ be any function in $\sub_V(f)$. Then this function must be equivalent to the composition of the function computed by $\gate$ and some two functions $g \in \sub_V(G)$ and $h \in \sub_V(H)$.
		\item Suppose that $F = \gate(G, H)$ and $\ell_V(H) = 0$. Then $\sub_V^*(F) \subseteq \sub_V^*(G)$. Note that $\ell_V(H)$ means that none of the leaves in $H$ are labelled with any indices from $V$. When considering the $V$-functions of $H$, these can only be the constant functions $\underline{0}$ and $\underline{1}$. When composing the function calculated by $\gate$ with some $g \in \sub_V(G)$ and a function in $\{\underline{0}, \underline{1}\}$ we can only get $\{g, 1-g, \underline{0}, \underline{1}\}$. Thus every function in $\sub_V^*(F)$ is also in $\sub_V^{*}(G)$.
	\end{enumerate}
	
	\begin{lemma}
		\label{lem:U-Vsubfunctions}
		If $F$ is an $n$-ary formula, $V \subseteq [n]$, and $\ell_{V}(F) \geq 1$, then \[|\sub_V^*(F)| \leq 4 \cdot 16^{\ell_V(F) - 1}.\] 
	\end{lemma}
	\begin{proof}
		By induction on $\leafsize(F)$. The base case where $\leafsize(F) = 1$ is trivial. The inductive case is also not that bad considering the two observations above. Suppose $F = \gate(G, H)$. If one of $\ell_V(G) = 0$ or $\ell_V(H) = 0$, then we can use the second observation. W.l.o.g assume $\ell_V(H) = 0$. By the induction hypothesis we have that  
		\[|\sub_V^{*}(F)| \leq |\sub_V^*(G)| \leq 4 \cdot 16^{\ell_V(G) - 1} \leq 4 \cdot 16^{\ell_V(F) - 1}.\]
		
		Next suppose that $\ell_V(G) \geq 1$ and $\ell_V(H) \geq 1$. Then by the induction hypothesis, we have that 
		\[|\sub_V^{*}(F)| \leq |\sub_V^*(G)| \cdot |\sub_V^*(G)| = 16^{\ell_V(G) + \ell_V(H) - 1} = 16^{\ell_V(F) - 1}\leq 4 \cdot 16^{\ell_V(F) - 1}\]
		as required.
	\end{proof}
	
	\begin{corollary}
		\label{cor:U-Vsubfunctions}
		Let $F$ and $V$ be as above, then $|\sub_V(F)| \leq 16^{\ell_V(F)}$.
	\end{corollary}
	This is immediate when $\leafsize(F) \geq 1$. Only $\underline{b}$, $b \in \{0,1\}$, have leaf-size $0$, but $\left|\sub_V\left(\underline{b}\right)\right| = 1 \leq 16^{0}$. 
	
	\begin{theorem}
		\label{thm:L-B2Formula-Nechiporuk}
		\textbf{(Nechiporuk's Bound).} For any $f: \{0,1\}^n \rightarrow \{0,1\}$ and any partition of $[n]$ into $t$ disjoint components $V_1 \uplus \cdots \uplus V_t$
		\[\leafsize_{B_2}(f) \geq \frac{1}{4}\sum_{i = 1}^{t}\log|\sub_{V_i}(f)|.\]
	\end{theorem}
	\begin{proof}
		Direct application of Corollary \ref{cor:U-Vsubfunctions}. Let $F$ be the minimal formula computing $f$. Then
		\[\leafsize_{B_2}(f) = \sum_{i = 1}^{t}\ell_{V_i}(F) \geq \sum_{i = 1}^{t}\log_{16}|\sub_{V_i}(F)| = \frac{1}{4} \sum_{i = 1}^{t}\log|\sub_{V_i}(F)|.\]
	\end{proof}
	
	\subsubsection{FUN: Element Distinctness \texorpdfstring{$ED_n$}{EDn}}
	Let us apply Theorem \ref{thm:L-B2Formula-Nechiporuk} to an explicit function in the full binary basis to get a lower bound.
	\begin{definition}
		\label{def:FUN-elementdistinctness}
		For $k \in \NN$, let $n = 2^k \cdot 2k$. The \textbf{element distinctness} function $ED_n$ is
		\[\ff{ED}_n: \{0,1\}^{2^k \times 2k} \rightarrow \{0,1\}\] where
		\[\ff{ED}_n(X_1, ..., X_{2^k}) = \begin{cases}
		1 &\mbox{if $X_1, ..., X_{2k}$ are distinct elements of $\{0,1\}^{2k}$}\\
		0 &\mbox{otherwise}
		\end{cases}\]
		Think of this as being given $2^k$ binary strings of length $2k$ and asked if they are all distinct.
	\end{definition} 
	
	\begin{theorem}
		\label{thm:L-B2FormulaQuadraticExplicit}
		\[\leafsize_{B_2}(\ff{ED}_n) = \Omega\left(\frac{n^2}{\log n}\right).\]
	\end{theorem}
	\begin{proof}
		Apply Nechiporuk's bound with $2^k$ disjoints sets $V_i$ were each is a block of length $2k$ corresponding to the coordinates of $X_i$. Since these blocks are symmetric, we have
		\[\leafsize_{B_2}(\ff{ED}_n) \geq \frac{1}{4}\sum_{i=1}^{2^k}\log |\sub_{V_i}(\ff{ED}_n)| = 2^{k-2}\log|\sub_{V_1}(\ff{ED}_n)|.\]
		Thus it suffices to bound $|\sub_{V_1}(\ff{ED}_n)|$. To ensure that the restriction does not result in a constant function, the $2^k-1$ fixed $2k$-bit strings in the input to $\ff{ED}_n$ must all be distinct. There are $\binom{2^{2k}}{2^k-1}$ subsets of $2^k - 1$ distinct length $2k$ Boolean strings. Thus $|\sub_{V_1}(\ff{ED}_n)| \geq \binom{4^k}{2^k-1}$. Using the bound $\binom{2^{2k}}{2^k-1} \geq 2^{k(2^k - 1)}$ and recalling that $n = 2^k(2k)$, 
		\[\leafsize_{B_2}(\ff{ED}_n) \geq 2^{k-2}\log|\sub_{V_1}(\ff{ED}_n)| \geq 2^{k-2}k2^k - O(1) \in \Omega(n^2/\log n).\]  
	\end{proof}
	Exercise: show that $\Omega(n^2/\log n)$ is the limit on the lower bound achievable by Nechiporuk's method.
	

\section{Non-uniformity is More Powerful than Randomness}
	\begin{definition}
		\label{def:randomizedcircuit}
		A \textbf{randomized circuit} for a function $f: \{0,1\}^n \rightarrow \{0,1\}$ is a circuit $C$ with $n + m$ variables $x_1, ..., x_n$ and $y_1, ..., y_m$ (think of $\vv{x}$ as the input and $\vv{y}$ as a random seed) such that for every $\vv{x} \in \{0,1\}^n$
		\[\Pr_{\vv{y} \in \{0,1\}^m}\left[C(\vv{x}, \vv{y}) = 1\right] \begin{cases}
		\geq \frac{2}{3} &\mbox{if $f(\vv{x}) = 1$}\\
		\leq \frac{1}{3} &\mbox{if $f(\vv{x}) = 0$}
		\end{cases}\]
	\end{definition}
	Let $\class{BPP/poly}$ be the class of Boolean functions computable by poly-sized randomized circuits --- think of $\class{BPP/poly}$ as the non-uniform version of $\class{BPP}$. Generally $\frac{1}{3}$ and $\frac{2}{3}$ can be replaced with any $a$, $b$ satisfying $0 < a < b < 1$. 
	
	\begin{theorem}
		\textbf{(Adelman 1978).} If $f$ is computable by poly-size randomized circuit, then it is computable by poly-sized (deterministic) circuits i.e. $\class{BPP/poly} \subseteq \class{P/poly}$.
	\end{theorem}
	\begin{proof}
		The intuition is to improve the probability of success by doing repeated trials, taking the majority, then use the probabilistic method to show that there was a good choice of $\vv{y}$ which we can hard-wired into the circuit.
		
		Let $f$ be the given $n$-ary function with $\class{BPP/poly}$ circuit $C(\vv{x}, \vv{y})$. Recall that $MAJ_k$ has $O(k)$ sized circuits. Construct the composite function $g_k: \{0, 1\}^{n} \times \{0,1\}^{k \times m} \rightarrow \{0,1\}$ such that 
		\[g_k(\vv{x}, \mm{Y}) = MAJ_k\left(C(\vv{x}, \vv{y}_1), ..., C(\vv{x}, \vv{y}_k)\right)\] 
		where each $\vv{y}_i$ is the $i$\textsuperscript{th} row of $\mm{Y}$. Observe that $\circuitsize(g_k) \leq k \cdot \circuitsize(f) + O(k)$ since we can replace each of the $k$ inputs of $MAJ_k$ by a circuit of size $\circuitsize(f)$. If $k$ is $\poly(n)$ then $g_k \in \class{P/poly}$.
		
		On a randomly sampled seed $\vv{y}_i$, let $X_i$ be the indicator r.v. for $C(\vv{x}, \vv{y}_i) \neq f(\vv{x})$. Let $X = X_1 + \cdots + X_k$. Observe that 
		\begin{align*}
			\Pr_{\mm{Y} \in \{0,1\}^{k \times m}}\left[g_k(\vv{x}, \mm{Y}) \neq f(\vv{x})\right] &= \Pr_{\vv{y}_1, ..., \vv{y}_k \in \{0,1\}^{m}}\left[MAJ_k\left(C(\vv{x}, \vv{y}_1), ..., C(\vv{x}, \vv{y}_k)\right) \neq f(\vv{x})\right]\\
			&= \Pr\left[X \geq \frac{k}{2}\right]\\
			&= \Pr\left[X \geq (1 + \epsilon)pk\right]
		\end{align*}
		where $p = \Pr[C(\vv{x}, \vv{y}) \neq f(\vv{x})]$ and $\epsilon = \frac{1 - 2p}{2p}$. From Definition \ref{def:randomizedcircuit}, we have $p = \frac{1}{3}$ and $\epsilon = \frac{1}{2}$. Thus by Chernoff bound we have
		\[\Pr_{\mm{Y} \in \{0,1\}^{k \times m}}\left[g_k(\vv{x}, \mm{Y}) \neq f(\vv{x})\right] = \Pr\left[X \geq (1 + \epsilon)pk\right] \leq \exp\left(\frac{-\epsilon^2pk}{2 + \epsilon}\right) = \exp\left(\frac{-k}{30}\right).\]
		When $k > 30$, $\Pr\left[g_k(\vv{x}, \mm{Y}) \neq f(\vv{x})\right] \leq 2^{-n}$ and there exists a $\mm{Y}$ for which $g_k(\vv{x}, \mm{Y}) = f(\vv{x})$ with probability greater than $1 - 2^{-n}$. Since there are only $2^{n}$ inputs $\vv{x}$, $g_k(\vv{x}, \mm{Y})$ matches $f(\vv{x})$ on every input. Hard-wiring $\mm{Y}$ into the circuit for $g_k$ produces a deterministic $\poly(n)$ circuit for $f$. 
	\end{proof}
	
	\section{Restricted Setting}
	\subsection{Monotone Circuits}
	\begin{definition}
		A \textbf{monotone function} is a $n$-ary Boolean function $f$ where $S' \subseteq S \subseteq [n]$ implies $f(\ind_{S'}) = 1$ if $f(\ind_{S}) = 1$. A \textbf{monotone circuit} is a circuit corresponding to a monotone function. 
	\end{definition}
	
		\subsubsection{Upper Bound: Majority}
		Recall that $\ff{MAJ}_n(\vv{x}) = 1$ if and only if $|\vv{x}| > \frac{n}{2}$. If $MAJ_n(\vv{x}) = 0$, then a random bit $x_i$ chosen uniformly among $\{x_1, ..., x_n\}$ satisfies $\Pr[x_i = 1] \leq \frac{1}{2} - \frac{1}{2n}$.
		
		\begin{definition}
			\label{def:NOTE-RandomProjection}
			Let $m > n$. Then $\pi$ is a \textbf{random projection} from $(y_1, ..., y_m) \rightarrow \{x_1, ..., x_n\}^{m}$ if $\pi(y_i)$ is mapped independently and uniformly among all $\{x_1, ..., x_n\}$.
			
			For monotone formula $F(\vv{y})$, let $F_{\pi}(\vv{x})$ be the monotone formula obtained from $F$ by replacing each variable $y_i$ with $\pi(y_i)$.
		\end{definition}
		
		We will use random projections to construct a poly-sized monotone formula for $\ff{MAJ}_n(\vv{x})$. The formula consists of a perfect tree of $\ff{MAJ}_3$ gate. At the bottom of this tree we sample i.i.d from the elements of $\vv{x}$. Intuitively this works by amplifying the difference between the number of zeros and ones in $\vv{x}$. If there are more ones in $\vv{x}$, then you would expect $\Pr[\ff{MAJ}_3(\pi(y_1), \pi(y_2), \pi(y_3)) = 1] > \frac{1}{2}$ and similarly when there are more zeros. This increased likelihood is then propagated up the tree so that the output at the root matches the output of $\ff{MAJ}_n$ in expectation. Using standard probabilistic methods, there exists a projection which exactly matches the output of $\ff{MAJ}_n$. To formalize this intuition, we need a couple of definitions.
		
		\begin{definition}
			\label{def:FUN-OutputProb}
			For $f: \{0,1\}^m \rightarrow \{0,1\}$, the \textbf{output probability} $\mu_f: [0,1] \rightarrow [0,1]$ is the probability that $f(\vv{y}) = 1$ given that each $y_i \sim \bern(p)$ i.e.
			\[\mu_{f}(p) = \Pr_{\vv{y} \sim \bern(p)^{m}}[f(\vv{y}) = 1].\]
			
			For example, $\mu_{MAJ_3}(p) = p^3 + 3p(1-p)$.
		\end{definition}
	
		When $f$ is a non-constant, monotone function, then $\mu_{f}$ is increasing with $\mu_f(0) = 0$ and $\mu_f(1) = 1$.
		
		\begin{lemma}
			 $\mu_{f \comp g}(p) = \mu_f(\mu_g(p))$.
		\end{lemma}	
		\begin{proof}
			Let $f$ and $g$ be a $k$ and $m$-ary Boolean functions respectively. The inputs of $f \comp g$ is a matrix $\mm{X} \in \{0,1\}^{k \times m}$ where you apply $g$ to the rows $\vv{x}_i$ of $\mm{X}$ and then apply $f$ to the resulting column vector. All $km$ bits of $\mm{X}$ are distributed like $\bern(p)$. From the perspective of $g$, each bit is independently set to one with probability $p$ so $\mu_g(p) = \Pr[g(\vv{x}_i) = 1]$ for $i \in [k]$. From the perspective of $f$, each bit is independently set to one with probability $\mu_g(p)$ so $\mu_f(\mu_g(p)) = \Pr[f(g(\vv{x}_1), ..., g(\vv{x}_k)) = 1] = \mu_{f \comp g}(p)$.   
		\end{proof}
		\begin{corollary}
			$\mu_{f^{\comp k}}(p) = \mu^{(k)}_{f}(p)$.
		\end{corollary}
		
		\begin{lemma}
			\label{lem:MAJ3Bd-Outputprobability}
			There is a constant $c < 3$ such that $\mu_{\ff{MAJ}_3}^{(c\log n)}(p) < \frac{1}{2^n}$ for $0 \leq p < \left(\frac{1}{2} - \frac{1}{2n}\right)$ and $\mu_{\ff{MAJ}_3}^{(c\log n)}(p) > 1 -  \frac{1}{2^n}$ for $\frac{1}{2} + \frac{1}{2n} \leq p \leq 1$.
		\end{lemma}
		\begin{proof}
			Keep in mind is the graph of $\mu_{\ff{MAJ}_3}$ shown in Figure \ref{fig:outputprobability}. The second derivative is positive when $p < \frac{1}{2}$ and negative when $p > \frac{1}{2}$. For positive integer $k$, $\lim_{k \rightarrow \infty} \mu_{\ff{MAJ}_3}^{(k)}(p) = 0$ in the former and $\lim_{k \rightarrow \infty}\mu_{\ff{MAJ}_3}^{(k)}(p) = 1$ in the latter case.
 
			\fig{outputprobability}{7}{A graph of $\mu_{\ff{MAJ}_3}$. Pay attention to the interval between $[0,1]$.}
			
			We evaluate the rate at which the composed function approaches its limit in two phases: (1) $\frac{1}{2} > p \geq \frac{1}{4}$ and (2) $\frac{1}{4} > p > \frac{1}{2^n}$. 
			\begin{enumerate}
				\item Write $p = \frac{1}{2} - \delta$ where $\delta < \frac{1}{4}$. We want to find a $c_1 > 1$ such that $\mu_{\ff{MAJ}_3}(p) = \frac{1}{2} - c_1\delta$. Then $\mu^{(k_1)}_{\ff{MAJ}_3}(p) \leq \frac{1}{2} - c^{k_1}_1\delta$. Take $k_1 \in O(\log n)$ in order for the RHS to be less than $\frac{1}{4}$. To find $c_1$
				\begin{align*}
				\left(\frac{1}{2} - \delta\right)^3 + 3\left(\frac{1}{2} - \delta\right)^2\left(\frac{1}{2} + \delta\right) &= \frac{1}{2} - \frac{3\delta}{2} + 2\delta^3\\
				&\leq \frac{1}{2} - \left(\frac{3}{2} - \frac{2}{16}\right) \delta \\
				&\leq \frac{1}{2} - \frac{5}{4} \delta.
				\end{align*}
				Thus we can choose $c_1 = 5/4$. 
				\item Here $p < \frac{1}{4}$ so again we want to find a constant $c_2 < 1$ such that $\mu_{\ff{MAJ}_3}(p) \leq c_2 p$. Then $\mu^{(k_2)}_{\ff{MAJ}_3}(p) \leq c^{k_2}_2p$. Take $k_2 \in O(\log n)$ in order for the RHS to be less than $\frac{1}{2^n}$. To find $c_2$
				\begin{align*}
					p^3 + 3p^2(1-p) &= 3p^2 - 2p^3\\
									&\leq 3p^2 \leq \frac{3}{4}p.
				\end{align*}
				Thus we can choose $c_2 = 3/4$.
			\end{enumerate}
		Since $\mu_{\ff{MAJ}_3}(p)$ is rotationally symmetric about $p = \frac{1}{2}$, the claim also holds for $p \geq \frac{1}{2} + \frac{1}{2n}$. 
		\end{proof}
		
		\begin{theorem}
			\label{thm:U-DmMonoFormula-Valiant1984}
			\textbf{(Valiant 1984).} $\ff{MAJ}_n$ has poly-sized monotone circuits\footnote{And monotone formulas apparently!}.
		\end{theorem}
		\begin{proof}
			We are going to combine the above ideas of amplification and projection. Let $k = c\log n$, where $c$ is the constant from Lemma \ref{lem:MAJ3Bd-Outputprobability}. Build a depth $k$ tree $T$ out of $\ff{MAJ}_3$ formulas of minimum size. Since there exists a formula of size five for $\ff{MAJ}_3$, see Figure \ref{fig:Maj3Formula}, $T$ will have at most $5^{k}$ leaves. 
			
			\begin{figure}[ht]
				\centering
				\begin{tikzpicture}[->, >=stealth', shorten >= 1pt, auto, node distance=2em, baseline=(current bounding box.center)]
				\node[state]	(or1) 				{$\lor$};
				\node[state]	(and1)	[below left=of or1]{$\land$};			
				\node[state]	(and2)	[below right=of or1]{$\land$};
				\node[state]	(x21)	[below=of and1]{$x_2$};
				\node[state]	(x11)	[left=of x21]{$x_1$};
				\node[state]	(x3)	[below=of and2]{$x_3$};	
				\node[state]	(or2)	[right=of x3]{$\lor$};		
				\node[state]	(x12)	[below left=of or2]{$x_1$};	
				\node[state]	(x22)	[below right=of or2]{$x_2$};	
				\path 	
				(and1)	edge	node{} 	(or1)
				(and2)	edge	node{}	(or1)
				(x11)	edge	node{} 	(and1)
				(x21)	edge	node{}	(and1)
				(x3)	edge	node{}	(and2)
				(or2)	edge	node{}	(and2)
				(x12)	edge	node{}	(or2)
				(x22)	edge	node{}	(or2);
				\end{tikzpicture}
				\caption{$\ff{MAJ}_3$ formula with leafsize five.}
				\label{fig:Maj3Formula}
			\end{figure}
			
			Let $F(\vv{y})$ be the function computed by $T$. For a random projection $\pi$, $\Pr[F_{\pi}(\vv{x}) \neq \ff{MAJ}_n(\vv{x})] < \frac{1}{2^n}$ so there exists a projection $\pi^{*}$ such that $F_{\pi^*}(\vv{x}) = \ff{MAJ}_n(\vv{x})$ on all inputs $\vv{x}$. Hard-wire $\pi^*$ into $T$ to get a formula which exactly computes $\ff{MAJ}_n$. Thus $\leafsize(\ff{MAJ}_n) \leq 5^k = n^{c\log 5} \in O(n^7)$.
		\end{proof}
		
		\subsubsection{Slice Functions}
		The following is a striking consequence of Theorem \ref{thm:U-DmMonoFormula-Valiant1984}.
		\begin{definition}
			\label{def:FUN-slice}
			$f: \{0,1\}^n \rightarrow \{0,1\}$ is a \textbf{Slice functions} if there exists $k \in \{0, ..., n\}$ such that $f(x) = 0$ if $|x| < k$ and $f(x) = 1$ if $|x| > k$.
		\end{definition}
	
		A particular type of slice functions is the threshold function.
		\begin{definition}
			\label{def:FUN-threshold}
			A \textbf{threshold function} is a function $\ff{THR}_{k,n}: \{0,1\}^n \rightarrow \{0,1\}$ such that $\ff{THR}_{k,n} = 1$ if and only if $|\vv{x}| \geq k$. This generalizes $\ff{MAJ}_n$.
		\end{definition}
	
		\begin{theorem}
			\textbf{(Berkowitz 1982).} If $f$ is a slice function then $\leafsize_{mon}(f) \leq \leafsize(f) \cdot \poly(n)$ and $\circuitsize_{mon}(f) \leq \circuitsize(f) + \poly(n)$.
		\end{theorem}
		\begin{proof}
			We will describe the procedure for turning the monotone poly-size formula for $\ff{MAJ}_n$ into a monotone formula for any slice function. First observe that you can construct a monotone formula for $\ff{THR}_{k,n}$ by padding $\ff{MAJ}$ with an appropriate number of $0$s or $1$s\footnote{If $2k > n$ then you want to add $2k-n$ zeros to $\ff{MAJ}_n$. Similarly for when $2k < n$.}.
			
			Let $F$ be a DeMorgan formula computing $k$-slice function $f$. Write $F$ in negation normal form. Let $F'$ be $F$ with each $\bar{x}_i$ replace with $\ff{THR}_{k,n}(x_1, ..., x_{i-1}, 0, x_{i+1}, ..., x_n)$. Then observe that $\ff{THR}_{k,n} \land F'$ is a monotone formula with leafsize $\leafsize{f}\cdot \poly(n)$ which computes $f$. 
			
			If fewer than $k$ variables are true, then $\ff{THR}_{k,n}(\vv{x}) = 0$. If more than $k$ variables are true, then each of $\ff{THR}_{k,n}(x_1, ..., x_{i-1}, 0, x_{i+1}, ..., x_n) = 1$ and $F'(\vv{x}) = 1$ since all its inputs are true. Finally, if exactly $k$ variables are true, then $\ff{THR}_{k,n}(\vv{x}) = 1$ and $\ff{THR}_{k,n}(x_1, ..., x_{i-1}, 0, x_{i+1}, ..., x_n) = \bar{x}_i$ and $F'(\vv{x}) = F(\vv{x})$.
		\end{proof}
		Observe that for all monotone functions $f$, $\circuitsize(f) \leq \circuitsize_{mon}(f)$ and $\leafsize(f) \leq \circuitsize_{mon}(f)$ so for monotone circuits a tight bound is known for its leaf and circuit-sizes.
	
	\subsection{Bounded Depth Circuits: \texorpdfstring{$\class{AC}^0$}{AC0}}
	
	\begin{definition}
		\label{def:classac0}
		An \textbf{$\class{AC}^0$ circuit} is one consisting of unbounded fan-in $\land$ and $\lor$ gates. 
		
		The \textbf{depth} of an $\class{AC}^0$ circuit is the length of its longest root to leaf path.
		
		Every $\class{AC}^0$ circuit can be composed of alternating layers of $\land$ and $\lor$-gates since two adjacent levels with the same gate type can be merged.
		
		Let $\circuitsize_{d}(f)$ ($\leafsize_{d}(f)$) be the minimum circuit (resp. leaf)-size of a depth $d$ $\class{AC}^0$ circuit (resp. formula) for $f$. 
	\end{definition}
	$\class{AC}^0$ circuits correspond to algorithms which are very efficient to parallelize. In particular, constant-time on polynomially many processors.
	
	\begin{example}
		Consider the function $+_{n}$, addition of two $n$ bit numbers. Let the inputs to $+_{n}$ be $\vv{x} = (x_0, ..., x_{n-1})$ and $\vv{y} = (y_0, ..., y_{n-1})$ with values $x = \sum_{i = 0}^{n-1}2^{i}x_i$ and $y = \sum_{j = 0}^{n-1}2^{j}y_j$ respectively. We want to output $n+1$ bits $z_0, ..., z_{n}$ such that $\sum_{k = 0}^{n} 2^{k}z_k = x + y$. 
		
		Though this might not seem possible in constant depth since the carry bit is defined recursively, it can be seen in another way. Suppose $x_{n} = y_{n} = 0$, then
		\begin{align*}
			z_k &= x_k \xor y_k \xor c_k\\
			c_k &= \bigvee_{i = 0}^{k-1} (x_i \land y_i) \bigwedge_{j = i+1}^{k}(x_j \lor y_j)
		\end{align*}
		The way to think about the carry bit $c_k$ at index $k$ is as the result of some $x_i \land y_i$ and for all indices $j \in \{i+1, ..., k\}$, the carry is kept alive by $x_j \lor y_j$.
	\end{example}
	Some common functions which are not in $\class{AC}^0$ include: multiplication of two $n$-bit integers and the $n$-bit parity function $\ff{XOR}_n$.
	
	Since $\class{AC}^0$ circuits can be arranged with alternating levels of $\land$ and $\lor$-gates, define $\Pi_d$ and $\Sigma_d$ to be the subclasses of $\class{AC}^0$ who root consists of a $\land$-gate and a $\lor$-gate respectively. For functions $f$, denote the size of the  minimal circuits of $f$ in $\Pi_d$ and $\Sigma_d$ by $\circuitsize_{\pi_d}(f)$ and $\circuitsize_{\sigma_d}(f)$. 
	
	For the parity function $\ff{XOR}_n$, $\circuitsize_{\pi_d}(\ff{XOR}_n) = \circuitsize_{\sigma_d}(\ff{XOR}_n)$. To see this, note that $\circuitsize_{\pi_d}(f) = \circuitsize_{\sigma_d}(1 - f)$ for any function and that $\circuitsize_{\pi_d}(\ff{XOR}_n) = \circuitsize_{\pi_d}(\overline{\ff{XOR}}_n)$ by negating the first input of $\ff{XOR}_n$. The same relationship holds with regards to leafsize.
	
	\textbf{Challenge.} Let us consider the leaf size of an $n$-ary Boolean function $f$ in $\class{AC}^0$ with depth at most $d$. Naively, we have that $\leafsize_2(\ff{XOR}_n) \leq n2^n$ as you can take an ``or'' of $2^n$ ``and'' gates with fan-in $n$ each specifying a setting of the $n$ input variables\footnote{It is also easy to see that $\circuitsize_2(f) \leq 2^n$ and $\leafsize_2(f) \leq n2^n$ for an arbitrary function $f$ by writing $f$ as a DNF $F$.}.
	
	\begin{lemma}
		\label{lem:U-AC0recurrence}
		For $k$ and $n_1, ..., n_k$ where $\sum_{i = 1}^{k} n_i = n$ and $d \geq 2$, 
		\[\leafsize_{d+1}(\ff{XOR}_n) \leq 2^{k-1}\sum_{i = 1}^{k}\leafsize_{d}(\ff{XOR}_{n_i}) \mbox{ and } \circuitsize_{d+1}(\ff{XOR}_n) \leq 2^{k-1} + 2\sum_{i = 1}^{k}\circuitsize_d(\ff{XOR}_{n_i}).\]
	\end{lemma}
	\begin{proof}
		Decompose $\ff{XOR}_n$ into $\ff{XOR}_{n_1}, ..., \ff{XOR}_{n_k}$ and then take an $\ff{XOR}$ of these results. Each $\ff{XOR}_{n_i}$ has leafsize $\leafsize(\ff{XOR}_{n_i})$. Further there exists a formula for the top level $\ff{XOR}$ with leafsize $k2^{k-1}$ and depth $2$, using the trivial construction. By collapsing adjacent levels with the same gate type, $\leafsize_{d+1}(\ff{XOR}_n)$ is bounded as required. The circuit-size bound can be similarly constructed. 
	\end{proof}
	
	Using $\leafsize_2(\ff{XOR}_n) \leq n2^n$ for the base case and the recurrence shown in Lemma (\ref{lem:U-AC0recurrence}), we can show 
	\[\leafsize_{d+1}(\ff{XOR}_n) \leq n2^{dn^{1/d}}\]
	for any $n$ and $d \geq 2$. Simply choose $n_i = \ceil{n^{(d-1)/d}}$ for all $i \in [k]$ and $k = \ceil{n^{1/d}}$ to obtain 
	\[\leafsize_{d+1}(\ff{XOR}_n) \leq 2^{\ceil{n^{1/d}} - 1}\ceil{n^{1/d}}\cdot\leafsize_{d}(\ff{XOR}_{n_i}) \leq 2^{\ceil{n^{1/d}} - 1}\ceil{n^{1/d}}\cdot\ceil{n^{(d-1)/d}}2^{(d-1)n^{1/d}} \leq n2^{dn^{1/d}}\]	
	However, when $n$ is a power of $2$, we can get a slightly tighter bound of 
	\[\leafsize_{d+1}(\ff{XOR}_n) \leq n2^{d\left(n^{1/d} - 1\right)}.\]
	
	Ben suspects that the above inequality holds for all $n$, not just powers of two, since for $d = \ceil{\log n}$ it is know that $n^{1/d} - 1 = 2^{\log n/ \log n} - 1 = 1$ and the exponent of $2$ is sufficient i.e. it is known that
	\[\leafsize_{\ceil{\log n}}(\ff{XOR}_n) \in O(n^2).\]
	Further, Ben believes that it is sufficient to achieve this tighter bound by analyzing the recurrence relation more carefully. 
	
	\section{H\aa stad's Switching Lemma}
	\begin{definition}
		\label{def:decisiontrees}
		A \textbf{decision tree} (DT) is a rooted binary tree whose leaves are labelled by $\{0,1\}$ and whose internal nodes are labelled by variables. The \textbf{depth} of a decision tree is the length of the longest root-to-leaf path. For $f: \{0,1\}^n \rightarrow \{0,1\}$, let $\DTdepth(f)$ denote depth of the minimum depth DT that computes $f$. 
				
		It is useful to consider a function as a DNF or CNF. The \textbf{width} of a DNF/CNF formula is the maximum number of literals in any clause. 
	\end{definition}
	
	\begin{definition}
		\label{def:canonicaldecisiontree} 
		Let $F = C_1 \lor \cdots \lor C_m$ be a $k$-DNF with an arbitrary fixed order on the clauses and variables. Let $\rho: \{x_1, ..., x_n\} \rightarrow \{*, 0, 1\}$ be a restriction. Then the \textbf{canonical decision tree} of $F \upharpoonright \rho$, denoted $\CDT(F, \rho)$, is constructed as follows. 
		\begin{enumerate}
			\item Let $F_1 = F \upharpoonright \rho$ and $C_{1,1} \lor \cdots \lor C_{1,k_1}$ be the set of clauses which have not been set to $1$ or $0$ by $\rho$. 
			\item Construct a perfect binary tree where each level of the tree is labelled by a in $C_{1,1}$ in the predetermined order e.g. if $C_{1,1} = x_ix_j$ in $F_1$ and $i < j$, then set the root to $x_i$ and label its two children $x_j$. 
			\item Let $p$ be a path in our partially constructed tree. Denote by $\rho_{p}$ a which sets each variable in $C_{1,1}$ according to the path $p$. Let $\ell$ be one of the $2^{|C_{1,1}|}$ leafs in our partially constructed tree. If $p_{\ell}$ is the root-to-$\ell$ path, append $\CDT(F', \rho_{p})$ to $\ell$. 
		\end{enumerate}
		See the following example. 
	\end{definition}
	
	\begin{example}
		\label{ex:canonicaldecisiontree}
		\textbf{(Constructing a canonical decision tree).} Let our $k$-DNF be
		\[F = \bar{x}_1x_3x_5 \lor x_1x_2\bar{x}_3 \lor x_2\bar{x}_4x_5 \lor x_3x_4\bar{x}_6 \lor x_1\bar{x}_4\bar{x}_7\]
		and our restriction $\rho = \{x_1 \mapsto 1, x_4 \mapsto 0\}$. Apply the restriction $\rho$ to $F$, to obtain the following
		\[F \upharpoonright \rho = \textcolor{red}{0} \lor x_2\bar{x}_3 \lor x_2x_5 \lor \textcolor{red}{0} \lor \bar{x}_4\bar{x}_7.\]
		Look at the first un-falsified clause $x_2\bar{x}_3$ and build a tree with these variables on the first and second levels. For every root-to-leaf path, construct a restriction by setting the variables $x_2$ and $\bar{x}_3$ according to the edge labels. Continue down the tree until all the variables have been added. The first two restrictions are shown in Figure \ref{fig:dtcanonicalform}. 
		
		\begin{figure}[ht]
			\centering
			\begin{tikzpicture}[->, >=stealth', shorten >= 1pt, auto, node distance=6em, baseline=(current bounding box.center)]
			\node[state]	(x2) 					{$x_2$};
			\node[state]	(x30)[below left of=x2]	{$x_3$};
			\node[state]	(x31)[below right of=x2]{$x_3$};
			\node[state]	(x511)[below right of=x31]	{$x_5$};
			\node[state]	(x510)[left of=x511] 	{$x_5$};
			\node[state]	(x400)[below left of=x30]	{$x_4$};
			\node[state]	(x401)[right of=x400]{$x_4$};
			\node			(r)[below = 0pt of x2] {$F \upharpoonright \rho$};
			\node			(r00)[below = 0pt of x510] {$F' \upharpoonright \rho_{1,0}$};
			\node			(r01)[below = 0pt of x511] {$F' \upharpoonright \rho_{1,1}$};
			\node			(r10)[below = 0pt of x400] {$F' \upharpoonright \rho_{0,0}$};
			\node			(r11)[below = 0pt of x401] {$F' \upharpoonright \rho_{0,1}$};
			\path 	(x2)	edge	node{$1$} 			(x31)
							edge	node[swap]{$0$}		(x30)
					(x30)	edge	node[swap]{$0$}		(x400)
							edge	node{$1$}			(x401)
					(x31)	edge	node[swap]{$0$}		(x510)
							edge	node{$1$}			(x511);
			\end{tikzpicture}
			\caption{Given the $3$-DNF $F$ and the restriction $\rho$ construct the canonical DT for $F \upharpoonright \rho$. Let $F' = F \upharpoonright \rho$ and $\rho_{a,b} = \{x_2\mapsto a, x_3\mapsto b\}$.}
			\label{fig:dtcanonicalform}
		\end{figure}
	\end{example}
	
	Observe that every depth $d$ decision tree (DT) is equivalent to a $d$-DNF ($d$-CNF) by tracing every root-to-leaf path in the tree which end in a one (resp. end in a zero then apply DeMorgan's rule). It follows that an $\lor$ of depth $d$ $DT$s is a $d$-DNF. Further, if $f$ is equivalent to both a $k$-DNF and $l$-CNF, then $\DTdepth(f)\leq k \cdot l$. Do you see why this is\footnote{Let $f$ be equivalent to the $k$-DNF $F = A_1 \lor \cdots \lor A_s$ and $\bar{f}$ be equivalent to the $l$-DNF $\bar{F} = B_1 \lor \cdots \lor B_t$. Notice that every pair $(A_i, B_j)$ must share a common variable of opposite sign or else there will be a setting of the variable which simultaneously satisfies $F$ and $\bar{F}$. The proof is by induction on the number of variables. When there is only one variable the claim is true. Suppose $f$ has $n$ variables. Build a decision tree of width $A_1$ which considers all variables in the clause $A_1$. Consider a restriction $\rho$ corresponding to a root-to-leaf path in the DT we have just created. The DNF-width of $F \upharpoonright \rho$ is at most $k$. The DNF-width of $\bar{F}$ is \emph{less than} $l$ since at least one variable of each clause was knocked out. Further, by the induction hypothesis, we have
		\[\DTdepth(f \upharpoonright \rho) \leq \DNFwidth(F \upharpoonright \rho) \cdot \DNFwidth(\bar{F} \upharpoonright \rho).\]
		By appending the restricted decision tree to every leaf of the depth $k$ DT that we created for the variables in $A_1$, we have that
		\[\DTdepth(f) \leq k + \DNFwidth(F \upharpoonright \rho) \cdot \DNFwidth(\bar{F} \upharpoonright \rho) \leq k + k(l-1) = kl.\]}?
	
	Before looking at the proof of the Switching Lemma, let us consider a simple warm-up pertaining to the depth of a decision tree hit by a random restriction. 
	\begin{theorem}
		If $\DTdepth(f) = k$, then for any $t$
		\[\Pr\left[\DTdepth(f \upharpoonright R_p) \geq t\right] \leq (2p)^t \binom{k}{t}.\]
	\end{theorem}
	\begin{proof}
		By induction on $k$. In the base case where $k = 0$, $f$ is a constant function. Suppose that $\DTdepth(f) = d$. Let $T$ be a DT of $f$ of depth $d$ with root labelled $x_1$. Further let $T_0$ and $T_1$ be the left and right sub-trees of $T$ respectively. Either $x_1$ gets restricted or not. If $x_1$ gets restricted, then we can apply the induction hypothesis to one of $T_0$ or $T_1$, w.l.o.g suppose $T_0$. This happens with probability $1 - p$. If $x_1$ is unrestricted then it suffices to find the probability that a restriction of $T_0$ or $T_1$ has depth $\geq t-1$; w.l.o.g. suppose $T_0$ is larger. This happens with probability $p$. Thus 
		\begin{align*}
			\begin{split}
				\Pr\left[\DTdepth(T \upharpoonright R_p) \geq t\right] &= (1-p)\Pr\left[\DTdepth(T_0 \upharpoonright R_p) \geq t\right]+\\ & \qquad\quad p\Pr\left[\left(\DTdepth(T_0 \upharpoonright R_p) \geq t-1\right) \lor \left(\DTdepth(T_1 \upharpoonright R_p) \geq t-1\right)\right]
			\end{split}
			\\
			&\leq (1-p)\Pr\left[\DTdepth(T_0 \upharpoonright R_p)\right] + 2p\Pr\left[\DTdepth(T_0 \upharpoonright R_p) \geq t-1\right]\\
			&\leq (1-p)\cdot(2p)^{t}\binom{d-1}{t} + 2p\cdot(2p)^{t-1}\binom{d-1}{t-1}\\
			&\leq (2p)^{t}\left(\binom{d-1}{t} + \binom{d-1}{t-1}\right)\\
			&= (2p)^{t}\binom{d}{t}
		\end{align*}
		where the second inequality follows from a union bound and the equality on the last line is an identity\footnote{Remember the binomial coefficients are terms in Pascal's triangle. There we are adding two adjacent terms row $d-1$ to get a term in row $d$}.
	\end{proof}
	
	\begin{theorem}
		\label{thm:haastadswitchinglemma}
		\textbf{(H\aa stad's Switching Lemma).} If $F$ is a $k$-DNF (or $k$-CNF) then
		\[\Pr[\DTdepth(F \upharpoonright R_p) \geq t] \leq (5pk)^t.\]
	\end{theorem}
	\begin{proof}
		This proof due to Razborov as explained in the exposition by Beame. Let\footnote{In many proofs the set $B_t$ is actually called $BAD_t$. This is illustrative of how you want to think about $B_t$. It is the set of all \emph{bad} restrictions which cannot be converted into a depth $t$ decision tree.} 
		\[B_t \coloneqq \{\rho: \DTdepth(\CDT(F, \rho)) = t\}.\]
		We will show that the probability a random restriction $R_p \in B_t$ is bounded by $O(pk)^t$. Let $n$ be the number of variables and $m$ the number of clauses in $F$.
		
		For each $\rho$ in $B_t$ we want to construct a pair $(\rho^*, \textsc{code})$. $\rho^*$ is a fixed restriction setting a further $t$ variables. $\textsc{code}$ is some information needed to uniquely identify $\rho$. Note that $\textsc{code} = \left(\vv{s}, \vv{\pi}, \vv{n}\right)$ where $\vv{s} \in [k]^t$ indicates the location of variables within a clause, $\vv{\pi} \in \{0,1\}^t$ is the appropriate setting of the variable, and $\vv{n} \in \{0,1\}^t$ indicates if we move onto the \textbf{n}ext clause. Then, with the pair $(\rho^*, \textsc{code})$ and $F$ in-hand, we can reconstruct $\rho$.
		
		In the encoding step we are given the $k$-DNF $F$ and the restriction $\rho \in B_t$. We will build $\rho^* = \rho\sigma_1\cdots\sigma_j$ and $\textsc{code} = \gamma_1\cdots\gamma_j$ incrementally.
		\begin{enumerate}
			\item Let $T = T_1 = \CDT(F, \rho)$ and let $C_{1}, ..., C_{j_1}$ be the set of clauses with free variables. Let $p$ be the lexicographically first path of length $t$ in $T$. Such a longest path exists since $\rho \in B_t$ and $f$ is not the constant function.
			\item Let $\sigma_1$ be the unique setting of the free variables in $C_{1}$ such that $C_{1} \upharpoonright \sigma_1 \equiv 1$. Let $\pi_{1}$ record the actual setting of the free variables in $C_{1}$ along path $p$ as well as their location and number. In particular, $\gamma_1 = (\vv{s}_1, \vv{\pi}_1, \vv{n}_1)$ which are the indices, settings, and \emph{is-last-in-clause} properties of the free variables in $C_{1}$.
			
			Consider this first encoding step on Example \ref{ex:canonicaldecisiontree}. Suppose the long path $p$ aims to set $x_2 = 1$ and $x_3 = 1$. Then $\sigma_1 = \{x_2 \mapsto 1, x_3 \mapsto 0\}$, $\vv{s}_1 = [2,3]$, $\vv{\pi}_1 = \{x_2 \mapsto 1, x_3 \mapsto 1\}$, and $\vv{n} = [0,1]$.
			\item Proceed down $p$ past all the free variables in $C_{1}$. Let $T_1$ be the remaining subtree. Repeat the process until all $t$ variable on $p$ have been considered.    
		\end{enumerate}
		
		In the decoding step we are given $F$ and the pair $(\rho^*, \textsc{code})$ where $\rho^* = \rho\sigma_1\cdots\sigma_j$ and $\textsc{code} = \gamma_1\cdots\gamma_j$ and want to reconstruct $\rho$.
		\begin{enumerate}
			\item Evaluate $F \upharpoonright \rho^*$. Let $C_1'$ be the first clause set equal to $1$. Observe that $C_1' = C_{1}$ by construction. Use $\vv{n}_1$ to determine how many variables were set during the encoding step and use $\vv{\pi}_1$ and $\vv{v}_1$ to reset these values in $\rho^*$ so they match the first $|C_1|$ variables on the longest path $p$. Let $\rho^*_1 = \rho\pi_1\sigma_2\cdots\sigma_j$.
			\item Repeat with $\rho^*_1$ in the place of $\rho^*$ until we have used up all $t$ variables.
		\end{enumerate}
		All variables consider in the above procedure were originally stars in $\rho$. 
		
		Suppose that $\rho$ has $s$ stars. Then the set $\mathcal{R}_s$ of all restrictions with $s$ stars is of size $\binom{n}{s}2^{n-s}$. Similarly, the set of all restrictions with $s - t$ stars is of size $\binom{n}{s-t}2^{n-s+t}$. Then, since the codes come from a domain of size $(4\log k)^t$,
		\[\Pr[\DTdepth(F \upharpoonright R_p) \geq t] = \frac{|B_t|}{|\mathcal{R}_s|} \leq  \frac{|\mathcal{R}_{s-t}|(4\log k)^t}{|\mathcal{R}_s|} = \frac{\binom{n}{s-t}2^{n-s+t}(4\log k)^t}{\binom{n}{s}2^{n-s}} \leq (8pk)^t.\]
		Considering all bad restrictions with paths of length at least $t$ increases the above probability by a constant. The eight in the upper bound can be improved to five with a more careful analysis.
	\end{proof}
	
	It is often natural to choose $p = \frac{1}{10k}$ so that the bound is inverse exponential in $t$. 
	\begin{corollary}
		If $F$ is $k$-DNF, then 
		\[\Pr\left[F \upharpoonright R_p \mbox{ is not a $t$-CNF }\right] \leq (5pk)^t.\]
	\end{corollary}
	This corollary follows from Theorem \ref{thm:haastadswitchinglemma} since every depth-$t$ DT is a $t$-CNF ($t$-DNF).   
	
	\subsection{(LB) Parity Circuit-size}
	\begin{theorem}
		\label{thm:dtdepthunderrandomrestriction}
		Let $C$ be an $\class{AC}^0$ circuit of depth $d + 1$ and size $S$. Let $p = 10^{-d - 1}(2\log S)^{-d}$.
		\[\Pr[\DTdepth(C \upharpoonright R_p) \geq \ell] \leq \frac{1}{2^\ell} + \frac{1}{S}.\] 
	\end{theorem}
	\begin{proof}
		The key idea is to repeatedly apply Theorem \ref{thm:haastadswitchinglemma} being careful to choose appropriate values of $k$, $t$ and $p$. Consider the bottom level of $C$ (closest to the literals) and w.l.o.g assume that this is a conjunction ($\land$-gate). Since the fan-in consists of literals, you can think of them as width one clauses. Apply the theorem with $k = 1$, $t = 2\log S$, $p_1 = \frac{1}{10}$. According to the switching lemma, we fail\footnote{That is to say: this node cannot be converted to a short DT under $R_p$.} at each $\land$-node with probability at most $2^{-2\log S} = S^{-2}$. Let $E_1$ be the event that we successfully completed the switch from $1$-CNF to $2\log S$-DNF. Conditioning on $E_1$, we can collapse this layer of $\lor$-gates with the $\lor$-gates of the level above. 
		
		Apply the switching lemma a further $d-1$ iterations. At step $i$ set $k = 2\log S$, $t = 2\log S$, and $p_{i} = \frac{p_{i-1}}{20\log S}$ each time conditioning on the success of the previous iterations. At the end of these $d$ iterations, take a union bound over the at most $S$ gates we have considered. Since we fail on each level with probability at most $S^{-2}$, we fail to reach the top-most level with probability at most $S^{-1}$. 
		
		Consider our final iteration. W.l.o.g. the root consists of a $\lor$-gate sitting on top of depth $2\log S$ decision trees (i.e. width $2\log S$-CNF clauses). For the final application of the switching lemma, set $k = 2\log S$, $t = \ell$, and $p_{d+1} = \frac{p_d}{20\log S}$. Thus this iteration fail with probability at most $2^{-\ell}$. In total, this procedure fails with probability $2^{-\ell} + S^{-1}$.
	\end{proof}
	
	\begin{corollary}
		$\circuitsize_{d+1}(XOR_n) = 2^{\Omega(n^{1/d})}$.
	\end{corollary}
	By Theorem \ref{thm:dtdepthunderrandomrestriction}, the probability that the restriction has depth $\geq 1$ is $< 1$. Thus there is some restriction which sets $C$ to a constant. Since $p = 10^{-d-1}(2\log S)^{-d}$ and $S = 2^{\Omega(n^{1/d})}$, there is a good likely-hood that some variables are un-restricted.
	
	\subsection{Switching Lemma for Formulas}
	The lower bound\footnote{A result of Ben's!} of $\Omega(dn^{1/d})$ matches the existing upper bound. 
	
\section{LB: \texorpdfstring{$\class{AC}^0[p]$}{AC0[p]} by the Polynomial Method}
	\textbf{Goal}: want an upper bound on the approximate degree of $\ff{AND}$, $\ff{OR}$, and $\ff{MOD}_p$ functions.
	\begin{definition}
		\label{def:EpsApprox}
		Let $\vv{x} = (x_1,..., x_n)$ then $MOD_p(\vv{x}) = 1$ if and only if $\sum_{i = 1}^{n} x_i \equiv 0 \mod p$. 
	
		Let $A \in \field_p[x_1, ..., x_n]$ be a \textbf{random polynomial} over $\field_p$. The \textbf{degree} of $A$ is the maximum degree of a polynomial in the support\footnote{Imagine that you have a subset $\family{F} \subset \field_p[\vv{x}]$ and that $A$ as a random variable in a distribution over $\family{F}$. Then $\family{F} = \support(A)$.} for $A$. 
		
		Let $f: \{0,1\}^n \rightarrow \{0,1\}$. $A$ is an $\epsilon$-approximating for $f$ if
		\[\Pr_A[A(x) \neq f(x)] \leq \epsilon \]
		for every $x \in \{0,1\}^n$. 
		
		The \textbf{$\epsilon$-approximate degree of $f$}, denoted $\deg_{\epsilon}(f)$, is the minimum degree of an $\epsilon$-approximating random polynomial of $f$. Note that this value is invariant under negation of the inputs and outputs\footnote{Please understand why.}.
	\end{definition}
	
	\begin{lemma}
		\label{lem:EpsApprox-ExistWellApproxSupport}
		If $A$ is an $\epsilon$-approximating random polynomial for $f$, then $\exists \alpha \in \support(A)$ such that
		\[\Pr_{\vv{x} \in \{0,1\}^{n}}[\alpha(x) \neq f(x)] \leq \epsilon\]
	\end{lemma}
	\begin{proof}
		Let $A$ be an $\epsilon$-approximating polynomial for $f$. Consider the probability for a randomly selected $n$ bit string $\vv{x}$ that $A(\vv{x}) \neq \vv{x}$.
		\[\Pr_{\alpha \in \support(A)}\left[\Pr_{\vv{x} \in \{0,1\}^n}[A(\vv{x}) \neq f(\vv{x})] > \epsilon\right] < \frac{\expected_{\alpha \in \support(A)}\left[\Pr_{\vv{x} \in \{0,1\}^n}[\alpha(\vv{x}) \neq f(\vv{x})]\right]}{\epsilon}\]
		by Markov's inequality. Since the RHS is equal to one as $A$ is $\epsilon$-approximating, by the probabilistic method some polynomial $\alpha \in \support(A)$ must satisfy the above $\leq$ with $\geq$. 
	\end{proof}
		
	\begin{lemma}
		\label{lem:EpsApprox-degofcompfunction}
		Suppose $f(x) = g(h_1(x), ..., h_m(x))$. Then for any $\delta, \epsilon_1, ..., \epsilon_m$, 
		\[\deg_{\delta + \epsilon_1 + \cdots + \epsilon_m}(f) \leq \deg_{\delta}(g)\max_{i}\deg_{\epsilon_i}(h_i)\]
	\end{lemma}
	\begin{proof}
		Let $G$ and $H_i$ be $\delta$ and $\epsilon_i$-approximating polynomials for $g$ and $h_i$ respectively. We will show that $F' \coloneqq G(H_1, ..., H_m)$ is a $\delta + \epsilon_1 + \cdots \epsilon_m$ approximating polynomial for $f$. Note that 
		\[\Pr_{\vv{x} \in \{0,1\}^n}[f(\vv{x}) \neq F'(\vv{x})] = \Pr_{\vv{x} \in \{0,1\}^n}\left[F'(\vv{x}) \neq g(H_1(\vv{x}), ..., H_m(\vv{x})) \lor \bigvee_{i = 1}^{m} H_i(\vv{x}) \neq h_i(\vv{x})\right]\leq \delta + \sum_{i = 1}^{m}\epsilon_i\]
		where the inequality follows from a union bound. 
	\end{proof}
	
	Assume that the following are all $n$-ary Boolean functions.
	\begin{lemma}
		\label{lem:DegEpsApprox-modp}
		\textbf{($\ff{MOD}_p$ Function).} For any $\epsilon > 0$ and $n$,
		\[\deg_{\epsilon}(\ff{MOD}_p) \leq \deg_{0}(\ff{MOD}_p) \leq p-1\]
	\end{lemma}
	\begin{proof}
		Observe that $\ff{MOD}_{p,n} = 1 - (x_1 + \cdots + x_n)^{p-1}$ by Fermat's Little Theorem. Since the RHS is a degree $p-1$ polynomial the inequality holds as required.
	\end{proof}

	\begin{lemma}
		\label{lem:DegEpsApprox-or}
		\textbf{($\ff{OR}$ Function).} $\deg_{\epsilon}(\ff{OR}) \leq p\left(\log_p\left(\frac{1}{\epsilon}\right) + 1\right)$. One should note that this upper bound is independent of $n$.
	\end{lemma}
	\begin{proof}
		Consider the polynomial $(\lambda_1x_1 + \cdots + \lambda_nx_n)^{p-1}$ where $\Lambda = (\lambda_1, ..., \lambda_n)$ is chosen uniformly at random from among $\field^n_p$. Note that we take the inner product to the power $p-1$ to obtain a value in $\{0,1\}$, again by Fermat's Little Theorem. Observe that
		\[\Pr_{\vv{x} \in \{0,1\}^n}\left[\ff{OR}(\vv{x}) \neq \left(\Lambda^{\top}\vv{x}\right)^{p-1}\right] = 
			\begin{cases}
				0 &\mbox{if } \vv{x} = \vv{0}\\
				\frac{1}{p} &\mbox{if } \vv{x} \neq \vv{0} 
			\end{cases}\]
		In order to ensure that we have an $\epsilon$-approximate polynomial, we can sample several $\Lambda$s and take their product. Let $\Lambda_1, ..., \Lambda_t \in \field^n_p$ and let $F(\vv{x}) = 1 - \prod_{i = 1}^{t}\left(1 - \left(\Lambda_i^{\top}\vv{x}\right)^{p-1}\right)$. Then
		\[\Pr_{\vv{x} \in \{0,1\}^n}\left[\ff{OR}(\vv{x}) \neq F(\vv{x})\right] \leq \frac{1}{p^t}\]
		since $F(\vv{x}) = 0 \iff (\Lambda_i^{\top}\vv{x})^{p-1} = 0$ for every $i\in [t]$. For $t \geq  \left(\log_p\left(\frac{1}{\epsilon}\right) + 1\right)$, $p^{-t} \leq \epsilon^{-1}$. 
	\end{proof}

	\begin{corollary}
		\label{cor:DegEpsApprox-and}
		\textbf{($\ff{AND}$ Function).} $\deg_{\epsilon}(\ff{AND}) \leq p\left(\log_p\left(\frac{1}{\epsilon}\right) + 1\right)$.
	\end{corollary}
	This follows from the results of $\ff{OR}$ by applying DeMorgan's rule. 
	
	\begin{lemma}
		\label{lem:UB-ApproxDegAc0pFormula1}
		Any $\class{AC}^0[p]$ formula $F$, of size $S$ and depth $d$, can be $\epsilon$-approximated by a degree $O(\log_{p} S/\epsilon)^d$ i.e. $\deg_{\epsilon}(F) \in O(\log_{p} S/\epsilon)^d$. 
	\end{lemma}
	\begin{proof}
		Let the output gate of $F$ be $g \in \{\ff{MOD}_p, \ff{AND}, \ff{OR}\}$. By Lemma \ref{lem:DegEpsApprox-modp}, Lemma \ref{lem:DegEpsApprox-or}, and Corollary \ref{cor:DegEpsApprox-and}, we know that $\deg_{1/8}(g) \in O(p)$.
	\end{proof}

	\begin{corollary}
		\label{cor:UB-ApproxDegaFourthAc0p}
		Every $\class{AC}^0$ function $F$ of depth $d$ and size $S$ satisfies $\deg_{1/4}(F) \in O(\log_p S)^d$.
	\end{corollary}
	
	\subsection{LB: Parity in  \texorpdfstring{$\class{AC}^0[p]$}{AC0[p]}}
	In order to show the upper bound we need a lemma, an inequality, and an improved version of Corollary	\ref{cor:UB-ApproxDegaFourthAc0p}. These are as follows.
	
	\begin{lemma}
		\label{lem:B-EpsilonApproxDegree}
		\textbf{(Kopparty-Srinivasan).} For formula $f$, $\deg_{\epsilon}(f) \leq \deg_{1/4}(f)\log 1/\epsilon$.
	\end{lemma}
	\begin{proof}
		We will construct an $\epsilon$-approximating polynomial of $f$ by composing several ($\log 1/\epsilon$ to be precise) $1/4$-approximating polynomials for $f$ using the majority function.
		
		Let $A$ be a $1/4$-approximating polynomial for $f$, $t$ be a positive integer, and $M(\vv{y}) \coloneqq \ff{MAJ}_t(\vv{y})$ for $\vv{y} \in \{0,1\}^t$. Observe\footnote{It turns out that there is a bijection between $t$-ary Boolean functions $f$ and polynomials in $\FF_p[x_1, ..., x_t]$ of degree at most $t$. It is simplistic to see this in $\FF_2[x_1, ..., x_t]$. Take every $\vv{x} \in \{0,1\}^t$ such that $f(\vv{x}) = 1$ and turn it into a monomial, $\prod_{i = 1}^{t} b_i$ where $b_i = x_i$ if $x_i = 1$ and $b_i = 1 - x_i$ otherwise. For general prime $p$, simply map $\{0,1\}^t \rightarrow \{-1,1\}^t$ and then map the output $\{-1,1\}\rightarrow \{0,1\}$.} that there exists a polynomial for $M$ of degree $t$. For $t$ independent copies of $A$, denoted $A_1, ..., A_t$, the polynomial $M'(\vv{x}) \coloneqq M(A_1(\vv{x}), ..., A_{t}(\vv{x}))$ has degree $t \cdot \deg_{1/4}(f)$. 
		
		Define indicator vectors $X_i$ to denote $A_i(\vv{x}) = f(\vv{x})$. For $X = \sum X_i$, $X$ is a binomial r.v. with probability of success $1/4$ and $n = t$. Thus 
		\[\Pr_{\vv{x} \in \{0,1\}^n} [X \geq t/2] \leq \exp(-t/12).\] 
		When $t \in O(\log (1/\epsilon))$ we have that $\Pr[M'(\vv{x}) \neq f(\vv{x})] \leq \epsilon$. 
	\end{proof}

	\begin{claim}
		\label{claim:INEQ-ApproxDegAc0pFormula2Helper}
		For $d \geq 2$, and arbitrary $a$ and $b$,
		\[\left(\frac{a}{d-1} + 1\right)^{d-1}(b+1) \leq \left(\frac{a+b}{d} + 1\right)^{d}.\]
	\end{claim}
	\begin{proof}
		Let $\phi(a,b)$ be the polynomial
		\[\phi(a, b) = \left(\frac{a+b}{d} + 1\right)^{d} -  \left(\frac{a}{d-1} + 1\right)^{d-1}(b+1).\]
		We will show that $\phi(a,b) \geq 0$ for all $a$ and $b$. First find the extremum of $\phi$ by taking the derivative with respect to $b$,
		\[\frac{\partial}{\partial b} \left(\left(\frac{a+b}{d} + 1\right)^{d} -  \left(\frac{a}{d-1} + 1\right)^{d-1}(b+1)\right) = \left(\frac{a+b}{d} + 1\right)^{d-1} - \left(\frac{a}{d-1} + 1\right)^{d-1}.\]
		Thus the extremum occurs when $b = a/d-1$ with $\phi(a, a/(d-1)) = 0$. Taking the second derivative with respect to $b$, we see that $\phi(a, a/(d-1))$ is a minimum of the function. 
	\end{proof}

	Now for the improved version of Corollary	\ref{cor:UB-ApproxDegaFourthAc0p}. At first blush, it might not look like that much of an improvement since $d$ and $p$ are constants, but this is exactly the form that we require for the parity bound. 
	\begin{lemma}
		\label{lem:UB-ApproxDegaFourthAc0p2}
		For formula $F$ in $\class{AC}^0[p]$ of depth $d$ and size $S$, $\deg_{1/4}(F) \leq O(p\log_p(S)/d)^d$. 
	\end{lemma}
	\begin{proof}
		By induction on $d$. Let the output gate of $F$ be $g \in \{\ff{MOD}_p, \ff{OR}, \ff{AND}\}$ with inputs $F_1, ..., F_m$ of sizes $S_1, ..., S_m$ respectively. Then by Lemma \ref{lem:EpsApprox-degofcompfunction}, 
		\[\deg_{1/4}(F) \leq \deg_{1/8}(g) \cdot \max_{i \in [m]}\deg_{S_i/(8S)}(F_i).\]
		By Lemma \ref{lem:UB-ApproxDegAc0pFormula1}, we have that $\deg_{1/8}(g) \in O(p)$. By Lemma \ref{lem:B-EpsilonApproxDegree}, we have that 
		\begin{align*}
			\deg_{S_i/(8S)}(F_i) &\leq \deg_{1/4}(F_i)\log\left(\frac{8S}{S_i}\right)\\
			 &\leq \log\left(\frac{8S}{S_i}\right)\\
		\end{align*}
		where the second inequality follows from the induction hypothesis. Finally, it is time to use Claim \ref{claim:INEQ-ApproxDegAc0pFormula2Helper}. Don't panic. Just set $a = \log_{p}(S_i/p^{d-1})$ and $b = \log_{p}(S/pS_i)$ to get the inequality
		\[p^{d-1} \left(\frac{\log_{p}(S_i)}{d-1}\right)^{d-1}\leq p^{d-1}\left(\frac{\log_{p}S}{d}\right)^d.\]
		Altogether we have 
		\[\deg_{1/4}(F) \leq \deg_{1/8}(g) \cdot \max_{i \in [m]}\deg_{S_i/(8S)}(F_i) \leq p\left(p^{d-1}\left(\frac{\log_{p}S}{d}\right)^d\right) = \left(\frac{p\log_{p} S}{d}\right)^d\]
		for sufficiently large $n$. 
	\end{proof}

	\begin{theorem}
		\label{thm:UB-BoundPrRandPolyEqParity}
		For prime $p \geq 3$. If $A \in \field_p[x_1, ..., x_n]$ with $\deg(A) = \Delta$, then 
		\[\Pr_{\vv{x} \in \{0,1\}^n}[A(\vv{x}) = \ff{XOR}_n(\vv{x})] \leq \frac{1}{2} + O\left(\frac{\Delta}{\sqrt{n}}\right).\]
	\end{theorem}
	\begin{proof}
		Probably the most involved proof in this section. Uses a couple of clever tricks and something known as a \emph{dimension argument}.
		
		First let $\lambda: \{0,1\} \rightarrow \{1,-1\}$ be the $\lambda(x) = 1 - 2x$. We want to be able to reduce the degree of a polynomial. To this end, it is helpful to have variable from $y_i \in \{-1,1\}$ since $y_i^2 = 1$. Observe that 
		\[\lambda(x_1 \xor \cdots \xor x_n) = \prod_{i = 1}^{n} (-1)^{x_i} = \prod_{i = 1}^{n}\lambda(x_i).\]
		Thus, by substituting $\lambda^{-1}(y_i) = x_i$, 
		\[\lambda\left(\ff{XOR}(\vv{x})\right) = \lambda\left(\ff{XOR}(\lambda^{-1}(\vv{y}))\right)= \prod_{i = 1}^{n}y_i.\]
		
		Let $\tilde{A}(\vv{y}) = \lambda(\ff{XOR}(\lambda^{-1}(\vv{y})))$ and notice that $\deg(\tilde{A}) = \Delta$. It suffices to show that 
		\[\Pr_{\vv{x} \in \{0,1\}^n}\left[\tilde{A}(\vv{y}) = \prod_{i = 1}^{n}y_i\right] \leq \frac{1}{2} + \frac{O(\Delta)}{\sqrt{n}}.\]
		
		Consider the set of vectors $S = \{\vv{y} \in \{0,1\}^n: \tilde{A}(\vv{y}) = \prod_{i \in [n]} y_i\}$. Observe that the probability above is equal to $|S|/2^n$. Thus it suffices to find $|S|$. Consider $\family{F}$, the set of all functions $f: S \rightarrow \FF_p$. Since each $S$ can be any of $|\FF_p| = p$ different values, $|\family{F}| = p^{|S|}$ and $|S| = \log_p |\family{F}|$.
		
		Let us think about the aforementioned function $f$. We claim that $f$ is equivalent to a degree $(n + \Delta)/2$ polynomial over $\FF_p[y_1, ..., y_n]$. To see this, take any polynomial $B$ which matches $f$ over $S$. Since $y_i^2 = 1$ as $y_i \in \{1, -1\}$, multi-linearize $B$. Next take each monomial $\prod_{i \in I}y_i$ with $|I| \geq (n + \Delta)/2$ and substitute 
		\[\tilde{A}(y) \cdot \prod_{j \in [n]\backslash I}y_j.\]
		Notice that this monomial is identical to the original since
		\[\tilde{A}(y) \cdot \prod_{j \in [n]\backslash I}y_j = \left(\prod_{i \in [n]} y_i\right) \cdot \left(\prod_{j \in [n]\backslash I}y_j\right) = \left(\prod_{i \in I}y_i\right)\cdot \left(\prod_{j \in [n]\backslash I}y_j^2\right) = \prod_{i \in I}y_i\]
		and has degree
		\[\deg(A) + (n - |I|) \leq \Delta + n - \frac{(n + \Delta)}{2} = \frac{n + \Delta}{2}.\] 
		Thus we can bound $|S|$ as follows:
		\begin{align*}
			|S| &= \log_{p}|\family{F}|\\
				&\leq \log_{p}\left|\mbox{monomials of degree at most }\frac{n + \Delta}{2}\right|\\
				&= \underbrace{\binom{n}{0} + \binom{n}{1} + \cdots + \binom{n}{n/2}}_{2^{n-1}} + \underbrace{\binom{n}{n/2 + 1} + \cdots + \binom{n}{\frac{n + \Delta}{2}}}_{< \frac{\Delta 2^{n}}{\sqrt{n}}} 
		\end{align*}
		and the probability as follows:
		\[\Pr_{\vv{x} \in \{0,1\}^n}\left[\tilde{A}(\vv{y}) = \prod_{i = 1}^{n}y_i\right] = \frac{|S|}{2^n} \leq \frac{1}{2} + \frac{O(\Delta)}{\sqrt{n}}.\]
	\end{proof}

	\begin{theorem}
		\label{thm:UB-ParityinAc0p}
		An $\ff{XOR}_n$ formula of depth $d$ in $\class{AC}^0[p]$ require size $2^{\Omega(d(n^{1/2d} - 1))}$. 
	\end{theorem}
	\begin{proof}
		By Lemma \ref{lem:UB-ApproxDegaFourthAc0p2}, there exists a random polynomial $A$ of degree $\left(\frac{p\log_p S}{d}\right)^d$ which $1/4$-approximates $\ff{XOR}_n$. This means, for all $\vv{x} \in \{0,1\}^n$, 
		\[\Pr_{A}[A(\vv{x}) = \ff{XOR}_n(\vv{x})] \geq \frac{3}{4}.\]
		Thus there exists an polynomial $A$ which matches $\ff{XOR}_n$ on more than $3/4$ of all vectors $\vv{x}$. However, by Theorem \ref{thm:UB-BoundPrRandPolyEqParity}, we know that 
		\[\Pr_{\vv{x} \in \{0,1\}^n}[A(\vv{x}) = \ff{XOR}_n(\vv{x})] \leq \frac{1}{2} + O\left(\left(\frac{p\log_p S}{d}\right)^d\frac{1}{\sqrt{n}}\right).\]
		It follows that the RHS must be greater or equal to $3/4$. Rearranging this inequality in-terms of $S$, we have that $S = 2^{\Omega(d(n^{1/2d} - 1))}$.	
	\end{proof}

	\subsection{Real Approximating Polynomials in Other Norms}
	Instead of approximating Boolean functions by polynomials over finite fields, we can try to approximate them with polynomials $A \in \RR[x_1, ..., x_n]$. The three primary norms we will consider are $\ell_{0}$, $\ell_{\infty}$, and $\ell_{2}$. For finite fields, only $\ell_{0}$ makes sense.
	
	\subsubsection{\texorpdfstring{$\ell_{0}$}{l0}-Approximation}
	We can find a low-degree $\epsilon$-approximating for $\ff{OR}_n$ in $\ell_0$-norm using what is known as the Valiant Vazirani Isolation Lemma. This lemma seems quite useful in other contexts as well.
	\begin{lemma}
		\label{lem:ValVazIsolationLemma}
		Let $S_0 = [n]$ and $j \in [\log n + 1]$, $S_j$ be a uniformly random subset of $S_{j-1}$. Then for every nonempty subset $X \subseteq [n]$,
		\[\Pr[\exists j: |X \cap S_j| = 1] \geq \frac{1}{6}.\]
	\end{lemma} 
	\begin{proof}
		The key thing to realize is that $[n] = S_0 \supseteq X$ so $|S_0 \cap X| \geq 1$. The only time when the event $E \coloneqq \exists j: |X \cap S_j| = 1$ does not occur is when every set intersects with $X$ too much, or there is some $S_j$ whose intersection with $X$ differs greatly from $S_{j-1} \cap X$. Formally, this means 
		\[\Pr[\forall j: |X \cap S_j| \geq 1] \leq \Pr[|X\cap S_{\log(n) + 1}| \geq 2] + \Pr[\exists j: |X \cap S_{j-1}| \geq 2 \mbox{ and } |X\cap S_j| = 0].\]
		As always, if we can upper bound the two terms on the RHS, then we can upper bound the complement of $E$ and thus lower bound the probability of $E$. 
		
		To bound $\Pr[|X \cap S_{\log(n) + 1}| \geq 2]$ consider the probability that any $x_i \in S_{\log(n) + 1}$:
		\[\Pr[x_i \in S_{\log(n) + 1}] = \Pr[x_i \in S_{\log(n) + 1}|x_i \in S_{\log n}]\cdots\Pr[x_i \in S_1| x_i \in S_0]\Pr[x_i \in S_0] = 2^{-\log(n) - 1}.\]
		Thus $\Pr[|X \cap S_{\log(n) + 1}| \geq 2] \leq |X|2^{-\log(n) - 1} \leq n2^{-\log(n) - 1} = 2^{-1}$.
		
		To bound $\Pr[\exists j: |X \cap S_{j-1}| \geq 2 \mbox{ and } |X\cap S_j| = 0]$ consider all possible values of $|X \cap S_{j-1}|$
		\begin{align*}
			\Pr[\exists j: |X\cap S_j| \geq \mbox{ and } |X\cap S_{j-1}| = 0] &\leq \frac{1}{3}
		\end{align*}
	
		Together we have 
		\[\Pr[\exists j: |X \cap S_j| = 1] \geq 1 - \left(\frac{1}{2} + \frac{1}{3}\right) = \frac{1}{6}\]
		as required.
	\end{proof}

	\begin{lemma}
		\label{lem:UB-DegreeRealApproxPoly-Aspenes}
		(Aspenes et. al. 1993) There exists a random polynomial $A \in \RR[x_1, ..., x_n]$ of degree $O(\log(1/\epsilon)\cdot \log(n))$ such that, for every $\vv{x} \in \{0,1\}^n$
		\[\Pr_{A}[A(\vv{x}) \neq \ff{OR}_n(\vv{x})] \leq \epsilon.\]
	\end{lemma}
	\begin{proof}
		Choose random sets $S_0,..., S_{\log(n) + 1}$ as defined in Lemma \ref{lem:ValVazIsolationLemma}. For $0 \leq j \leq \log(n) + 1$ define the \emph{degree one polynomial}\footnote{Note! This is the \emph{sum} of the terms in $S_j$ and \emph{NOT} the product even though a monomial might seem more natural.} $B_j(\vv{x}) = \sum_{i \in S_j}x_i$. Let 
		\[B(\vv{x}) = 1 - \prod_{i = 1}^{\log(n) + 1}\left(1 - B_j(\vv{x})\right).\]
		Observe that $B(\vv{x})$ is a degree $O(\log n)$ polynomial and $\Pr[B(\vv{x}) \neq \ff{OR}_n(\vv{x})] \leq 1/6$. To see that the latter statement is true consider $\vv{x}$ such that $\ff{OR}_n(\vv{x}) = 0$ and $1$ respectively. 
		
		If $\ff{OR}_n(\vv{x}) = 0$ then $\vv{x} = \vv{0}$. It follows that $B_j(\vv{x}) = 0$ and $B(\vv{x}) = 0$.  If $\ff{OR}_n(\vv{x}) = 1$ then
		\begin{align*}
			\Pr[B(\vv{x}) = \ff{OR}_{n}] 
				&= \Pr[B(\vv{x}) = 1]\\
				&= \Pr[\exists j: B_j(\vv{x}) = 1]\\
				&= \Pr[\exists j: |S_{j} \cap \{x_j: x_j = 1\}| = 1]\\
				&\geq \frac{1}{6}
		\end{align*}
		
		We will take $t$ independent copies of $B$, denoted $B^{(1)}, ..., B^{(t)}$ and combine them together to form
		\[A(\vv{x}) = 1 - \prod_{i = 1}^{t}\left(1 - B^{(i)}(\vv{x})\right)\]
		a degree $O(t\log n)$ real polynomial. Take $t = \log_{5/6}(\epsilon^{-1})$ to be $\epsilon$-approximating\footnote{Notice that this is slightly different from Lemma \ref{lem:B-EpsilonApproxDegree}. Instead of a binomial r.v., you have $t$ independent Bernoulli r.v.s with probability of success $1/6$ and you just need one of them to be successful}. 
	\end{proof}
	
	\subsubsection{\texorpdfstring{$\ell_{\infty}$}{l_infty}-Approximation}
	Let $f$ be a $n$-ary Boolean function. The $\ell_{\infty}$-approximating degree of $f$, denoted $\tilde{\deg}(f)$, is the minimum degree of a real polynomial $\tilde{f} \in \RR[x_1, ..., x_n]$ such that $|f(\vv{x}) - \tilde{f}(\vv{x})| \leq 1/3$ for all $\vv{x} \in \{0,1\}^n$. It is an open problem if there exists an $\class{AC}^0$ function $f: \{0,1\}^n \rightarrow \{0,1\}$ with $\tilde{\deg}(f) = \Omega(n)$.  A bound for depth-$d$ functions, is due to Bun and Thaler with degree $n^{1 - 1/2^{O(d)}}$. Apparently Sherstov has some work on this question as well.
	
	\subsubsection{\texorpdfstring{$\ell_{2}$}{2}-Approximation}
	This is apparently the most natural and well-studied of all polynomial approximations. From what I understand, most of it can be categorized under Fourier analysis. 
	
	\section{LB: Monotone for \texorpdfstring{$k$}{k}-Clique}
	\subsection{Problem Definition}
	Let $G = (V, E)$ be a simple graph with no isolated vertex. For a growing parameter $n$, let $\family{G}$ be the set of all graphs $G$ with $|V_G| \subseteq [n]$. Since graphs with no isolated vertices are determined by their edge sets, identify each graph $G \in \family{G}$ by a vector in $\{0,1\}^{\binom{n}{2}}$. In the following, all functions $f$, $g$, and $h$ will be monotone $\family{G} \rightarrow \{0,1\}$ Boolean functions. 
	
	A graph $X \in \family{G}$ is a \textbf{minterm} of $f$ if $f(X) = 1$ and $f(X') = 0$ for all proper subsets $X' \subsetneq X$. Let $\minterms(f)$ be the set of minterms of $f$.
	
	For $X \in \family{G}$, let $\indf_{X}: \family{G} \rightarrow \{0,1\}$ be the $X$-subgraph indicator r.v. where for graph $G$,
	\[\indf_{X}(G) = 1 \iff X \subseteq G.\]
	Note that every $f$ monotone, is equivalent to the disjunction of all its subgraph indicators over its minterms i.e.
	\[f(G) = \bigvee_{X \in \minterms(g)}\indf_{X}(G).\]
	
	For a fixed graph $H$, with $|V_H| \leq k$, let $\family{G}_H$ be the subset of graphs in $\family{G}$ which are isomorphic to $H$. Define the function $\subf_{H}(G): \family{G} \rightarrow \NN$ such that 
	\[\subf_{H}(G) \coloneqq \sum_{X \in \family{G}_H} \indf_X(G)\]
	so $\subf_{H}(G)$ counts the number of subgraphs of $G$ isomorphic to $H$, in the loosest sense. Let $\minterms_H(f) = \minterms(f) \cap \family{G}_H$. 
	
	For $k \geq 3$, $k-\ff{CLIQUE}: \family{G} \rightarrow \{0,1\}$ is the monotone function defined by $\minterms(k-\ff{CLIQUE}) = \family{G}_{K_k}$. 
	
	Let $\mathcal{Y}$ be a uniform random graph in $\family{G}_{K_k}$. This is our ``Yes''-instance. Let $\mathcal{N}$ be the Erd\"{o}s-Renyi random graph with probability $p = n^{-2/(k-1)}$. Given this particular choice of $p$, is it is unlikely that $\mathcal{N}$ will contain a $k$-clique. This will be our ``No'' instance.
	
	\begin{lemma}
		\label{lem:UB-NoInstanceContainsKClique}
		\[\Pr[\mathcal{N} \mbox{ contains a $k$-clique}] \leq \frac{3}{4}.\]
	\end{lemma}
	\begin{proof}
		This is just a one-liner if you remember the upper bound for the binomial coefficient.
		\[\Pr[\mathcal{N} \mbox{ contains a $k$-clique}] \leq \binom{n}{k}p^{\binom{k}{2}} \leq \left(\frac{en}{k}\right)^k \left(n^{2/(k-1)}\right)^{\binom{n}{2}} = \left(\frac{en}{k}\right)^kn^{-k} = \left(\frac{e}{k}\right)^k \leq \left(\frac{e}{3}\right)^3\leq \frac{3}{4}\]
		where the inequalities are by union bound, upper bound for binomial coefficients, and $k \geq 3$ respectively.
	\end{proof}

	\begin{corollary}
		\label{cor:LB-YesNoInstancesValid}
		\[\Pr[k-\ff{CLIQUE}(\mathcal{Y}) = 1] + \Pr[k-\ff{CLIQUE}(\mathcal{N}) = 0] \geq \frac{5}{4}\]
	\end{corollary}

	We only require the following theorem:
	\begin{theorem}
		\label{thm:UB-YesNoInstancesValid}
		Every monotone circuit $C$ satisfies
		\[\Pr[C(\mathcal{Y}) = 1] + \Pr[C(\mathcal{N}) = 0] \leq 1 + o(1) + \frac{\circuitsize(C)}{\Omega(n^{k/4})}.\]
	\end{theorem}
	
	By combining Lemma \ref{cor:LB-YesNoInstancesValid} and Theorem \ref{thm:UB-YesNoInstancesValid} we can obtain a lower bound on $\circuitsize(C)$. 
	 
	Be warned. The proof of Theorem \ref{thm:UB-YesNoInstancesValid} is a dozy. It requires a slew of new definitions and pretty subtle reasoning throughout, so get ready. 
	 
	\subsection{More Definitions}
	A graph $H$ is \textbf{small} if $|V(H)| < k/2$ and \textbf{medium} if $|V(H)| \geq k/2$ and there exists small graphs $H_1$ and $H_2$ such that $H = H_1 \cup H_2 = H$. \emph{Notice: the union of two small graphs is a small or a medium graph.}
	 
	Let $\epsilon \coloneqq n^{-3k}$. A monotone function $f: \family{G} \rightarrow \{0,1\}$ is \textbf{closed} if, for every small-or-medium graph $X \in \family{G}$, 
	\[\Pr[f(\mathcal{N} \cup X) = 1] \geq 1 - \epsilon \implies f(X) = 1.\]
	\emph{If $f$ and $g$ are closed then so is $f \land g$.} Every monotone function $f$ has a unique \textbf{closure}, denoted $\cl(f)$ which is the minimum closed function such that $f \leq \cl(f)$. 
	 
	A monotone function $f: \family{G} \rightarrow \{0,1\}$ is \textbf{trimmed} if every minterm of $f$ is small. For any $f$, let $\trim(f): \family{G} \rightarrow \{0,1\}$ be the monotone function
	 \[\trim(f)(G) = 1 \iff \exists X \subseteq G: \mbox{$X$ is small and $f(X) = 1$}.\]
	Note that $\trim(f) \leq f$, with equality if and only if $f$ is trimmed \footnote{The name actually makes a lot of sense since, $\trim(f)$ is basically just $f$ with the medium graphs ``trimmed'' off.}. 
	 
	Let $\family{A} \coloneqq\{\trim(\cl(h)): h \mbox{ monotone}\}$. Every function $h \in \family{A}$ is an \textbf{approximator}. Define operations: $\sqcup, \sqcap: \family{A} \times \family{A} \rightarrow \family{A}$ as follows. For approximators $f, g \in \family{A}$,
	\[f\sqcup g\coloneqq \trim(\cl(f \lor g)) \mbox{ and } f \sqcap g\coloneqq \trim(\cl(f\land g)).\]
	Note that every edge indicator function $\indf_e$ is an approximator since $\indf_e = \trim(\cl(\indf_e))$. 
	 
	If $C$ is a monotone $\{\land, \lor\}$-circuit with inputs labeled by edge-indicators $\indf_e$, then the \textbf{approximator circuit} $C^{\family{A}}$ is the corresponding 
	 
	\subsection{Proof Outline}
	Again, our main objective is to prove Theorem \ref{thm:UB-YesNoInstancesValid}. That is, we want to come up with an upper bound on probability that the circuit $C$ behaves as expected on $\mathcal{Y}$ and $\mathcal{N}$. Instead of doing this directly, we will apply total probability to an approximate version of $C$, denoted $C^{\mathcal{A}}$. In particular we will upper bound the sum as follows
	\begin{align*}
		\Pr[C(\mathcal{Y}) = 1] + \Pr[C(\mathcal{N}) = 0] &\leq \Pr[C(\mathcal{Y}) = 1 \land C^{\mathcal{A}}(\mathcal{Y}) = 1] + \Pr[C(\mathcal{Y}) = 1 \land C^{\mathcal{A}}(\mathcal{Y}) = 0] \\
		&\qquad + \Pr[C(\mathcal{N}) = 0 \land C^{\mathcal{A}}(\mathcal{N}) = 1] + \Pr[C(\mathcal{N}) = 0 \land C^{\mathcal{A}}(\mathcal{N}) = 0]\\
		&\leq \Pr[C^{\mathcal{A}}(\mathcal{Y}) = 1] + \Pr[C(\mathcal{Y}) = 1 \land C^{\mathcal{A}}(\mathcal{Y}) = 0] \\
		&\qquad+ \Pr[C(\mathcal{N}) = 0 \land C^{\mathcal{A}}(\mathcal{N}) = 1] + \Pr[C^{\mathcal{A}}(\mathcal{N}) = 0]\\
		&\leq 1 + o(1) + \frac{\circuitsize(C)}{\Omega(n^{k/4})}.
	\end{align*}
	Further we will divide the last inequality into two
	\[\Pr[C^{\mathcal{A}}(\mathcal{Y}) = 1] + \Pr[C^{\mathcal{A}}(\mathcal{N}) = 0] \leq 1 + o(1)\]
	and 
	\[\Pr[C(\mathcal{Y}) = 1 \land C^{\mathcal{A}}(\mathcal{Y}) = 0] + \Pr[C(\mathcal{N}) = 0 \land C^{\mathcal{A}}(\mathcal{N}) = 1] \leq \frac{\circuitsize(C)}{\Omega(n^{k/4})}.\]
	It remains to prove these two inequalities. The bulk of the setup is used for the second one. There you will really need to understand the definition of trims and closures and be able to reason about them confidently. The first inequality is actually just a consequence of a simple lemma. 
	  
	\subsection{Proof Proper}
	Recall that $\mathcal{N}$ is an Erd\"{o}s-Renyi graph with $p = n^{-2/(k-1)}$. 
	\begin{lemma}
		\label{lem:E-NumSubgraphsInNIsomorphicToH}
		For any $H \subseteq K_k$,
		\[\expected[\subf_{H}(\mathcal{N})] = \Theta\left(n^{|V_H|} - 2|E_H|/(k-1)\right) \in \Omega(n^{k/4}).\]
	\end{lemma}
	\begin{proof}
		\[\expected[\subf_{H}(\mathcal{N})] = |\family{G}_{H}| \cdot p^{|E_H|} = \frac{|V_H|!\binom{n}{|V_H|}}{\auto(H)}p^{|E_H|} = O\left(n^{|V_H| - 2|E_H|/(k-1)}\right).\]
		The equalities are as follows. Apply linearity of expectation to sum over the expectation of each graph isomorphic to $H$. Each element of $\family{G}_H$ consists of choosing a permutation of $|V_H|$ vertices and removing their duplicates. The number of automorphisms of $H$ divides $|V_H|$, so $|\auto(H)| \leq |V_H|!$ and $|\family{G}_H| \in \Theta(n^{|V_H|})$.   
	\end{proof}
	Suppose $|V_H| = \lambda k$ for some $\lambda \in [0,1]$. Then we have 
	\[|V_H| - 2|E_H|/(k-1) \geq \lambda k - \frac{\lambda k(\lambda k - 1)}{k-1} = \frac{k^2\lambda (1 - \lambda)}{k-1} \geq k\lambda(1 - \lambda).\]
	When $\lambda$ is close to $1/2$, $\expected[\subf_{H}(\mathcal{N})] \geq n^{k/4}$. For medium graphs, the lower bound is $\Omega(n^{k/4} + \Omega(1/k))$. 
	
	\begin{lemma}
	 	\label{lem:UB-MintermsIsomorphicH}
	 	If $f$ is closed and $H \subseteq K_k$ is small-or-medium, then
	 	\[|\minterms_H(f)| \leq \frac{n^{o(1)}}{p^{|E(H)|}}.\]
	\end{lemma}
	\begin{proof}
		Let $\family{M} = \minterms_H(f)$ and $t = |E(H)|$. We will show a weaker version of this lemma using the Erd\"{o}s-Rado Sunflower\footnote{To get the inequality as shown above, you need to used the ``Approximate Sunflower Lemma" shown in Ben's notes.}. By the lemma, if $|\family{M}| \geq t!s^t$, then there exists a sunflower of size $s$ i.e. $X_1, ..., X_s \in \family{M}$ such that $X_1\backslash Z, X_s\backslash Z$ disjoint where $Z \coloneqq X_1 \cap \cdots \cap X_s$. Since we want a lower bound on $|\family{M}|$ we will assume for a contradiction that the above inequality holds. 
		
		Let $s = \ln(1/\epsilon)/p^{t}$. Then there exists a sunflower $X_1, ..., X_s \in \family{M}$ with core $Z$. Since $Z$ is a proper subset of the minterms of $f$, $f(Z) = 0$. But we can show that $f$ is not closed as
		\begin{align*}
			\Pr[f(\mathcal{N}\cup Z) = 0] &= \Pr\left[\bigwedge_{i = 1}^{s}(X_i\backslash Z) \not\subseteq \mathcal{N}\right]\\
			 &= \prod_{i = 1}^{s}\Pr[(X_i\backslash Z) \not\subseteq \mathcal{N}]\\
			 &= \left(1 - p^{|X_i \backslash Z|}\right)^s\\
			 &\leq \left(1 - p^{t}\right)^s \leq \exp(-ts) < \epsilon. 
		\end{align*}
		The equality in the first line might be the most difficult to see, but if $X_i \backslash Z \subseteq \mathcal{N}$ for some $i$ then $f(\mathcal{N} \cup Z) = 1$. 
	\end{proof}
 	
 	Take $\epsilon = n^{-3k}$. 
 	\begin{lemma}
 		\label{lem:UB-ProbFunctionDiffersFromClosure}
 		For every monotone function $f: \family{G} \rightarrow\{0,1\}$,
 		\[\Pr[f(\mathcal{N}) \land \cl(f)(\mathcal{N}) = 1] \leq O(n^{-2k}).\]
 	\end{lemma}
 	\begin{proof}
 		Since $f(\mathcal{N}) \neq \cl(f)(\mathcal{N})$, we know that $f$ is not closed. Thus we can construct an increasing sequence of monotone functions $f = f_0 < f_1 < \cdots < f_t = \cl(f)$. For each $i \in [t-1]$, there exists a graph $X_i \subset K_k$ such that 
 		\[\Pr[f_{i-1}(\mathcal{N} \cup X_i) = 1] \in [1 - \epsilon, 1).\]
 		Let $f_i = f_{i-1} \lor \indf_{X_i}$ where $\indf_{X_i}$ is the indicator of $X_i$. Since every $X_i$ can be added at most once, $t < \infty$. Upper bound the probability as follows
 		\begin{align*}
 			\Pr[f(\mathcal{N}) = 0 \mbox{ and } \cl(f)(\mathcal{N}) = 1] &= \Pr[f_0(\mathcal{N}) = 0 \mbox{ and } f_t(\mathcal{N}) = 1]\\
 			&= \sum_{i = 0}^{t-1}\Pr[f_i(\mathcal{N} = 0) \mbox{ and } f_{i+1}(\mathcal{N}) = 1]\\
 			&= \sum_{i = 0}^{t-1}\Pr[f_i(\mathcal{N} = 0) \mbox{ and } X_{i+1} \in \mathcal{N}]\\
 			&\leq \sum_{i = 0}^{t-1} \Pr[f_{i}(\mathcal{N} \cup X_i) 0]\\
 			&\leq t\epsilon = O(\epsilon n^{k}) = O(n^{-2k})
 		\end{align*}
 		where the inequality on the last line is due to the probability assumption above. 
 	\end{proof}
 	
 	\begin{lemma}
 		\label{lem:BD-SizeMintermsOfTrimmedClosures}
 		If $f$ and $g$ are trimmed monotone functions, then all minterms of $\cl(f \land g)$ and $\cl(f \lor g)$ are small or medium.
 	\end{lemma}
 	\begin{proof}
 		This proof is more involved than I originally thought. First you should observe that a minterm of $f \land g$ is the union of a minterm of $f$ and one of $g$. Since both $f$ and $g$ are trimmed, the union of their minterms is a small or medium graph. Finally we can write $\cl(f \land g) = (f\land g) \lor X_1 \lor \cdots \lor X_t$ for graphs $X_1, ..., X_t \in \cup_{H \subset K_k} \family{G}_{H}$ which incrementally extend $f\land g$ to its closure (similar to the above lemma). Thus the minterms of $\cl(f\land g)$ are small or medium. A similar argument can be made for $\cl(f\lor g)$.
 	\end{proof}
 	
 	\begin{lemma}
	\label{lem:UB-ProbFunctionDiffersFromTrim}
	Suppose $f$ is closed and every minterm of $f$ is small ormedium. Then 
	\[\Pr[f(\mathcal{Y}) = 1 \land \trim(f)(\mathcal{Y}) = 0] \leq \frac{1}{\Omega(n^{k/4})}.\]
 	\end{lemma}
 	\begin{proof}
 		Reason about the probability as follows\footnote{I have a pretty big issue with this proof. What is $g$ in the following equation?}. If $f(\mathcal{Y}) = 1$ and $\trim(f)(\mathcal{Y}) = 0$, then there exists a medium minterm of $f$ which is a subgraph in $\mathcal{Y}$ i.e. all graphs with a $k$-clique and \emph{nothing else}. 
 		\begin{align*}
 			\Pr[f(\mathcal{Y}) = 0 \land \trim(f)(\mathcal{Y}) = 0] &\leq \Pr\left[\bigvee_{\mbox{medium graphs }H} \bigvee_{X \in \minterms_H(f \land g)}(X \in \mathcal{Y})\right]\\
 			&\leq \sum_{\mbox{medium graphs }H} \sum_{X \in \minterms_H(f \land g)}\Pr\left[X \in \mathcal{Y}\right] &\mbox{(Union Bound)}\\
 			&= \sum_{\mbox{medium graphs }H \subseteq K_k} \sum_{X \in \minterms_H(f \land g)}\frac{\binom{n-|V_H|}{k - |V_H|}}{\binom{n}{k}} &\mbox{($|\mathcal{Y}| = \binom{n}{k}$)}\\
 			&\leq \sum_{\mbox{medium graphs }H \subseteq K_k} |\minterms_H(f \land g)|\cdot \frac{1}{\Omega(n^{|V_H|})}\\
 			&\leq \sum_{\mbox{medium graphs }H \subseteq K_k} \frac{n^{o(1)}}{p^{|E_H|}} \cdot \frac{1}{\Omega(n^{|V_H|})} &\mbox{(Lemma \ref{lem:UB-MintermsIsomorphicH})}\\
 			&= \sum_{\mbox{medium graphs }H \subseteq K_k} \frac{n^{o(1)}}{\Omega(\expected[\subf_H(\mathcal{N})])}\\
 			&\leq \sum_{\mbox{medium graphs }H \subseteq K_k} \frac{n^{o(1)}}{\Omega(n^{k/4} + \Omega(1/k))} &\mbox{(Lemma \ref{lem:E-NumSubgraphsInNIsomorphicToH})}
 		\end{align*}
 		Since there are at most $2^{\binom{k}{2}} = O(1)$ medium graphs up to isomorphism with at most $k$ vertices, we can bound this above by $\Omega(n^{-k/4})$. 
 	\end{proof}
 
 	\begin{lemma}
 		\label{lem:UB-ApproxCircuitInequality}
 		\[\Pr[C(\mathcal{Y}) = 1 \land C^{\mathcal{A}}(\mathcal{Y}) = 0] + \Pr[C(\mathcal{N}) = 0 \land C^{\mathcal{A}}(\mathcal{N}) = 1] \leq \frac{\circuitsize(C)}{\Omega(n^{k/4})}.\]
 	\end{lemma}
 	\begin{proof}
 		This is the crux of the proof right here. We want to show that $C^{\mathcal{A}}$ closely approximates $C$ on both distributions $\mathcal{Y}$ and $\mathcal{N}$. 
 		
 		For error on $\mathcal{Y}$, 
 		\begin{align*}
 			\Pr[(f\land g)(\mathcal{Y}) = 1 \land (f\sqcap g)(\mathcal{Y}) = 0] &= \Pr[(f \land g)(\mathcal{Y}) = 1 \land \trim(\cl(f\land g))(\mathcal{Y}) = 0]\\
 			&\leq \Pr[\cl(f \land g)(\mathcal{Y}) = 1 \land \trim(\cl(f\land g))(\mathcal{Y}) = 0] &\mbox{($f\land g \leq \cl(f\land g))$}\\
 			&\leq \frac{1}{\Omega(n^{k/4})} &\mbox{(Lemma \ref{lem:BD-SizeMintermsOfTrimmedClosures} and  \ref{lem:UB-ProbFunctionDiffersFromTrim})}
 		\end{align*}
 		It follows that
 		\[\Pr[\circuitsize(\mathcal{Y}) \land \circuitsize^{\mathcal{A}}(\mathcal{Y}) = 0] \leq \frac{\circuitsize(C)}{\Omega(n^{k/4})}.\]
 		
 		For error on $\mathcal{N}$,
 		\begin{align*}
 			\Pr[(f\land g)(\mathcal{N}) = 0 \land (f\sqcap g)(\mathcal{N}) = 1] &= \Pr[(f \land g)(\mathcal{Y}) = 1 \land \trim(\cl(f\land g))(\mathcal{Y}) = 0]\\
 			&\leq \Pr[(f \land g)(\mathcal{Y}) = 1 \land \cl(f\land g)(\mathcal{Y}) = 0] &\mbox{($\trim(\cl(f\land g)) \leq \cl(f \land g)$)}\\
 			&\leq \frac{1}{\Omega(n^{2k})} &\mbox{(Lemma \ref{lem:UB-ProbFunctionDiffersFromClosure})}
 		\end{align*}
 		It follows that 
 		\[\Pr[\circuitsize(\mathcal{N}) = 0 \land \circuitsize^{\mathcal{A}}(\mathcal{N}) = 1] \leq \frac{\circuitsize(C)}{\Omega(n^{2k})}.\]
 	\end{proof}
 
 	\begin{lemma}
 		\label{lem:UB-FunctionValues}
 		For every approximator $f \in \mathcal{A}$, we have
 		\[\Pr[f(\mathcal{Y}) = 1] + \Pr[f(\mathcal{N}) = 0] \leq 1 + o(1).\]
 	\end{lemma}
 	\begin{proof}
 		It suffices to show that $\Pr[f(\mathcal{Y}) = 1] \in o(1)$. The proof is remarkably similar to that of Lemma \ref{lem:UB-ProbFunctionDiffersFromTrim}. Since $f$ is an approximator, either $f \equiv 1$ or there exists a monotone function $h$ such that $f = \trim(h)$. In the former case we are done so assume the latter holds. Then
 		\[f = \trim(h) = \bigvee_{\mbox{nonemty small graphs} H}\bigvee_{X \in \minterms_H(h)} \indf_X(\mathcal{Y}).\]
 		Therefore
 		\[\Pr[f(\mathcal{Y} = 1)] \leq \sum_{\mbox{nonemty small graphs} H}\sum_{X \in \minterms_H(h)} \Pr[X\in\mathcal{Y}] \in o(1).\]
 	\end{proof}
%\section{Project}
%	Shrinkage exponent of monotone formulas. 
\end{document}