\documentclass[11pt]{article}
\usepackage{structure}

\begin{document}
	\tableofcontents
	\newpage
	
	\makeheader{\today}{}{Circuit Complexity}{Lily Li}
	
	\section{Administrivia}
	\label{sec:admin}
	\begin{itemize}
		\item \textbf{Instructor:} Ben Rossman. 
		\item \textbf{Course Info:} Available at the \href{http://www.math.toronto.edu/rossman/CSC2429.html}{course website}. Just in case the website is down: lectures are Thursdays from 16:00 to 18:00 in Bahen B026. Office hours are by appointment.
		\item \textbf{Textbook:} \emph{Boolean Function Complexity} by Stasys Junkha. This is available as a free eBook through the University of Toronto library. 
		\item \textbf{Prerequisites:} None, but a previous complexity course is useful. Please read Appendix A.1 of the textbook and understand the material.
		\item \textbf{Workload:} Homework assignment(s), scribe notes, paper report (5 to 10 pages), and presentation if you so choose. No exams.
	\end{itemize}
	
	\section{Basic Definitions}
	\subsection{Boolean Functions}
	\label{section:definitions}
	\begin{definition}
		A \textbf{$n$-ary Boolean function} $f$ is a function of the form $f: \{0,1\}^n \rightarrow \{0,1\}$. Usually we interpret $(0,1)$ as $(\false, \true)$ or as $(1,-1)$ --- this makes sense if you think of it as $(-1)^0$ and $(-1)^{1}$.
	\end{definition}
	
	Let $\{0,1\}^* = \cup_{n \in \NN} \{0,1\}^n$. We typically refer to a family of Boolean function(s) $f: \{0,1\}^* \rightarrow \{0,1\}$. This corresponds to a sequence of functions $f_n: \{0,1\}^n \rightarrow \{0,1\}$ and to a language $L \subseteq \{0, 1\}^*$ described by its characteristic function $f_L: \{0,1\}^* \rightarrow \{0,1\}$.
	
	\begin{example}
		The following are some examples of $n$-ary Boolean functions:
		\begin{enumerate}
			\item $PARITY(x_1, ..., x_n) = \sum_{i = 1}^{n} x_i \mod 2$.
			\item $MOD_p(x_1, ..., x_n) = 1 \iff \sum_{i = 1}^{n} x_i \equiv 0 \mod p$.
			\item $MAJORITY_n(x_1, ..., x_n) = 1 \iff \sum_{i=1}^{n} x_i \geq \ceil{n/2}$.
			\item $k-CLIQUE: \{0,1\}^{\binom{n}{2}} \rightarrow \{0,1\}$. Think of each graph $G$ as an indicator vector $\ind{1}_G$ over its $\binom{n}{2}$ edges. Then $k-CLIQUE(\ind{1}_G) = 1$ if and only if $G$ has a $k$-clique.
		\end{enumerate}
	\end{example}
	
	Let us consider DeMorgan circuits. These contain logical connectives $\{\lor, \land, \lnot\}$, input variables $\{x_1, ..., x_n\}$, and constants $\{0,1\}$. 
	
	\begin{definition}
		\label{def:demorgancircuit}
		A \textbf{$n$-ary DeMorgan circuit} is a finite directed acyclic graph (DAG) with nodes labelled as follows:
		\begin{itemize}
			\item Nodes of in-degree zero (``inputs'') are labelled by a variable or a constant.
			\item Non-input nodes (``gates'') of in-degree one are labelled with $\lnot$. Gates of in-degree two are labelled with $\lor$ or $\land$.
			\item A subset of the nodes are designated as ``outputs'' (default: \emph{the} node with out-degree zero).
		\end{itemize}
		Two circuits are \textbf{equivalent} if they compute the same function.
	\end{definition}
	
	\textbf{Formulas} are tree-like circuits. Since different branches in a formula depend on different copies of the variables, formulas are memory-less. See Figure \ref{fig:circuitandformula}. Proving that formulas are polynomially weaker than circuits is still an open problem.
	
	\begin{figure}[ht]
		\centering
		\begin{tikzpicture}[->, >=stealth', shorten >= 1pt, auto, node distance=4em, baseline=(current bounding box.center)]
		\node[state,accepting]	(out) 					{$\land$};
		\node[state]	(or)[below left of=out]	{$\lor$};
		\node[state]	(not)[below right of=out]{$\lnot$};
		\node[state]	(and)[below right of=not]{$\land$};
		\node[state]	(x1)[below left of=or]	{$x$};
		\node[state]	(y1)[below right of=or]	{$y$};
		\node[state]	(x2)[below left of=and]	{$x$};
		\node[state]	(y2)[below right of=and]{$y$};
		\path 	(x1)	edge	node{} 	(or)
		(y1)	edge	node{}	(or)
		(x2)	edge	node{}	(and)
		(y2)	edge	node{}	(and)
		(or)	edge	node{}	(out)
		(and)	edge	node{}	(not)
		(not)	edge	node{}	(out);
		\end{tikzpicture}
		\qquad
		\begin{tikzpicture}[->, >=stealth', shorten >= 1pt, auto, node distance=4em, baseline=(current bounding box.center)]
		\node[state,accepting]	(out)							{$\land$};
		\node[state]	(not)[below right of=out]		{$\lnot$};
		\node[state]	(and)[below of=not]		{$\land$};
		\node[state]	(or)[left of=and]				{$\lor$};
		\node[state]	(z)[below of=and]				{$z$};	
		\node[state]	(outsub)[left of=z]		 		{$\land$};
		\node[state]	(notsub)[left of=or]			{$\lnot$};
		\node[state]	(andsub)[below of=notsub]		{$\land$};	
		\node[state]	(orsub)[left of=notsub]			{$\lor$};
		\node[state]	(x)[below of=orsub]				{$x$};
		\node[state]	(y)[below of=andsub]			{$y$};
		\path 	(x)		edge	node{} 	(orsub)
		(y)		edge	node{}	(orsub)
		(x)		edge	node{}	(andsub)
		(y)		edge	node{}	(andsub)
		(orsub)	edge	node{}	(outsub)
		(andsub)	edge	node{}	(notsub)
		(notsub)	edge	node{}	(outsub)
		(outsub)	edge	node{}	(or)
		(outsub)	edge	node{}	(and)
		(z)		edge	node{}	(or)
		(z)		edge	node{}	(and)
		(or)	edge	node{}	(out)
		(and)	edge	node{}	(not)
		(not)	edge	node{}	(out);
		\end{tikzpicture}
		\caption{(Left) Formula computing $x \xor y$. (Right) Circuit computing $x \xor y \xor z$.}
		\label{fig:circuitandformula}
	\end{figure}
	
	\begin{definition}
		\label{def:circuitsize}
		The \textbf{size} of a circuit is the number of $\lor$ and $\land$ gates it contains. 
		
		The \textbf{leaf-size} of a formula is the number of leaves in its associated DAG. This is one more than the circuit size as defined above. 
		
		The \textbf{circuit size} of an $n$-ary Boolean function $f: \{0,1\}^n \rightarrow \{0,1\}$, written $\circuitsize(f)$, is the minimum size of a circuit computing $f$. Similarly, the \textbf{formula (leaf) size} of $f$, written $\leafsize(f)$, is the minimum size of a formula computing $f$. 
		
		The \textbf{depth} of a circuit is the maximum number of $\land$ and $\lor$ gates on any input to output path.
	\end{definition}
	
	\begin{example}
		It is a major open problem to compute the circuit and leaf size lower bounds for various Boolean functions. A couple of known results are as follows.
		\begin{center}
			\begin{tabular}{c|c|c}
				$f$ 		& $\leafsize(f)$ 	& $\circuitsize(f)$\\ \hline
				&&\\[-1em]
				$AND_n$ 	& $n$ 				& $n-1$\\ \hline
				&&\\[-1em]
				$PARITY_n$ 	& $\Theta\left(n^2\right)$ 	& $3(n-1)$\\
			\end{tabular}
		\end{center}
		The results for $AND_n$ are tight since the output depends on all the inputs. Improving the gap size between $\leafsize(PARITY_n)$ and $\circuitsize(PARITY_n)$ would separate $\class{NC}_1$ from $\class{P}$.
	\end{example}
	
	\subsection{Other Ways of Measuring Size}
	Other ways of counting the size of a circuit include: (1) counting the number of wires and (2) counting all gate types (including $\lnot$ gates). It turns out that the result of these calculations differ from our definition above by at most a factor of two. It should be easy to see why this is in the former case. Every $\land$ and $\lor$ gate has two incoming wires. Claim \ref{claim:negationnotaproblem} shows this in the latter case.
	
	\begin{definition}
		\label{def:negationnormalform}
		The input to every $\lnot$ gate in a circuit in \textbf{negation normal form} is a variable. See Figure \ref{fig:negationnormalform}.
	\end{definition}
	\begin{figure}[ht]
		\centering
		\begin{tikzpicture}[->, >=stealth', shorten >= 1pt, auto, node distance=4em, baseline=(current bounding box.center)]
		\node[state,accepting]	(not1) 					{$\lnot$};
		\node[state]	(and1)[below of=not1]	{$\land$};
		\node[state]	(x)[below left of=and1]	{$x$};
		\node[state]	(y)[below right of=and1]{$y$};
		\path 	(x)	edge	node{} 	(and1)
		(y)	edge	node{}	(and1)
		(and1)	edge	node{}	(not1);
		\end{tikzpicture}
		\qquad$\implies$\qquad
		\begin{tikzpicture}[->, >=stealth', shorten >= 1pt, auto, node distance=4em, baseline=(current bounding box.center)]
		\node[state,accepting]	(or)					{$\lor$};
		\node[state]	(not1)[below left of=or]{$\lnot$};
		\node[state]	(not2)[below right of=or]{$\lnot$};
		\node[state]	(x)[below of=not1]		{$x$};
		\node[state]	(y)[below of=not2]		{$y$};
		\path 	(x)	edge	node{} 	(not1)
		(y)	edge	node{}	(not2)
		(not1)	edge	node{}	(or)
		(not2)	edge	node{}	(or);
		\end{tikzpicture}
		\caption{Apply DeMorgan's Law to all $\lnot$ gates whose inputs are not literals on the left circuit to get the equivalent right circuit in negation normal form.}
		\label{fig:negationnormalform}
	\end{figure}
	
	\begin{claim}
		\label{claim:negationnotaproblem}
		Every circuit $C$ of size $m$ is equivalent to a circuit in negation normal form of size $\leq 2m$. 
	\end{claim}
	\begin{proof}
		Apply DeMorgan's law to every $\lnot$ gate whose input is not a variable. This switches the order of $\lnot$ and $\land$/$\lor$ in the DAG and adds an additional $\lnot$ gate. By the end of the process we have added at most $m$ $\lnot$ gates.  
	\end{proof}
	
	Thus we can push all $\lnot$ gates to the bottom and interpret the inputs as literals (variables and their negation). We can also modify the definition of leaf-size to only count leaves leading to literals (never-mind the constants).
	
	\subsection{General Basis}
	A \textbf{basis} $B$ is a set of Boolean functions (or ``gate types''). Examples of basis include: 
	\begin{itemize}
		\item DeMorgan basis: $\{\land, \lor, \lnot\}$.
		\item Full binary basis: all Boolean functions $\{0,1\}^2 \rightarrow \{0,1\}$ (for example, you would get $\xor$).
		\item Monotone basis: $\{\land, \lor\}$ (NOT universal).
		\item $\class{AC}^0$ basis: $\{\land^{k}, \lor^k, \lnot: k \in \NN\}$ which are unbounded fan-in $\land$ and $\lor$ gates.
	\end{itemize}
	For a function $f$, let $\leafsize_B(f)$ and $\circuitsize_B(f)$ be the leaf and circuit size of $f$ with formulas and circuits built from gates of basis $B$. A basis is \textbf{universal} if it computes all functions. For two universal basis $B_1$ and $B_2$ it is possible to build a circuit using gates from $B_1$ which simulates any gate from $B_2$. If all functions in $B_1$ and $B_2$ have constant arity, it follows that $\circuitsize_{B_2}(f) = O(\circuitsize_{B_1}(f))$; for formula size, the relation is $\leafsize_{B_2}(f) = \leafsize_{B_1}(f)^{O(1)}$. This polynomial blow-up is unavoidable in some cases. Recall the function $PARITY_n$: $\leafsize_{\{\land, \lor, \lnot\}}(PARITY_n) = \Theta(n^2)$ whereas $\leafsize_{\{\xor\}}(PARITY_n) = n-1$.
	
	\subsection{Models of Computation}
	\label{sec:uniformvsconcrete}
	\begin{definition}
		\label{def:mmodelsofcomputation} 
		A \textbf{uniform model of computation} is a single machine/program with a finite description which operates on all inputs in $\{0,1\}^*$. Examples range from simple finite automata (where we have lower bounds ala the pumping lemma) to complex Turing Machines (lower bounds much harder to come by).
		
		Recall that a language $L \subseteq \{0,1\}^*$ can be interpreted as a sequence of functions $(f_0, f_1, ...)$ where $f_n: \{0,1\}^n \rightarrow \{0,1\}$ and $f_n(\vv{x}) = 1 \iff \vv{x} \in L$ for any $\vv{x} \in \{0,1\}^n$. A \textbf{non-uniform (concrete) model of computation} is a sequence $(C_0, C_1, ...)$ of combinatorial objects (namely circuits) where $C_n$ computes $f_n$. Examples include: circuits in the DeMorgan basis, restricted class of circuits (formulas, monotone model), decision trees, etc.  
	\end{definition}
	
	Observe that the non-uniform model of computation is more powerful than the uniform one since the finite program can be used as every combinatorial objects in the sequence. It follows that lower bounds in the non-uniform model also imply lower bounds in the uniform model. While upper bounds in the uniform model imply upper bounds in the non-uniform model. \emph{We want: unconditional lower bounds.} 
	
	Circuits efficiently simulate Turing Machines. 
	\begin{lemma}
		Any Turing Machine (TM) $M$ with running time $t(n)$ can be simulated by a circuit (family of) of size $O\left(t(n)^2\right)$.
	\end{lemma}
	Exercise for the reader. \emph{Hint: think about configurations of the Turing Machines as a $t(n) \times t(n)$ grid and construct a circuit for every grid cell.} Fischer and Pipenger (1979) proved an $O(t(n)\log t(n))$ upper bound on \emph{oblivious Turing Machines}\footnote{An oblivious TM is one whose head motion depends only on the size of the input and not its particular bits. Take a look at \href{https://rjlipton.wordpress.com/2009/07/28/oblivious-turing-machines-and-a-crock/}{this} blog post for some entertainment.}. It is unknown if we can do better. 
	
	\begin{corollary}
		If there is a super polynomial lower-bound (better than $\Omega(n^c)$ for all constants $c > 0$) on the circuit size of any language in $\class{NP}$, then $\class{P} \neq \class{NP}$. 
	\end{corollary}
	Finding the lower-bound would actually show $\class{NP} \not\subseteq \class{P/poly}$ where $\class{P/poly}$ is the class of languages decidable by $\poly(n)$-size circuits.
	
	We will see some polynomial lower bounds for formulas in the DeMorgan basis later on. As a historical curio, the following is a catalogue of lower bound results for an explicit Boolean function:
	\begin{enumerate}
		\item $\Omega(n^{1.5})$ Subboboskay '61
		\item $\Omega(n^{2})$ Khrapchenko '71
		\item $\Omega(n^{2.5 - o(1)})$ Andreev '83
		\item $\Omega(n^{3 - o(1)})$ H\aa stad '98 (this is the state of the art until very recently). 
	\end{enumerate}
	
	\section{DeMorgan Basis}
	\subsection{Balancing Formulas}
	Next we consider the relationship between circuit size and depth. First observe that every circuit of depth $d$ is equivalent to a formula of size  at most $2^d$. To see this, take the circuit and duplicate any branches that gets reused. The resulting binary tree has at most as many nodes as a perfect binary tree of depth $d$ which itself has circuit size $2^d$.   
	
	The next theorem shows the converse: every formula of size $s$ can be ``balanced'' to obtain a formula of depth $O(\log s)$.
	\begin{theorem}
		\textbf{(Spira 1971).} Every formula with leaf-size $s$ is equivalent to a formula of depth $O(\log s)$ ($2\log_{3/2}(s)$ to be exact) and thus size at most $s^{O(1)}$ ($s^{2/\log_2(3/2)}$).
	\end{theorem}
	\begin{proof}
		By induction on $s$. The base case is trivial. Let $F$ be the original formula and $g$ be some gate. Let $F_g$ be the sub-formula rooted at $g$. For $b \in \{0,1\}$, let $F^{(g \leftarrow b)}$ be the formula with $g$ replaced with the constant value $b$. See Figure \ref{fig:spira1971}. Note that $\leafsize(F) = \leafsize\left(F_g\right) + \leafsize\left(F^{\left(g \leftarrow b\right)}\right)$. Minimize $\leafsize(F)$ by balancing the two terms on the RHS. By Claim \ref{claim:pickgoodgate}, we can find a gate $g$ such that $\frac{s}{3} \leq \leafsize(F_g) \leq \frac{2s}{3}$. 
		
		\fig{spira1971}{1}{Illustration of gate $g$ and formulas $F_g$ and $F^{g \leftarrow b}$.}
		
		Note that $F \equiv \left(F_g \land F^{(g \leftarrow 1)}\right) \lor \left((\lnot F_g) \land F^{(g \leftarrow 0)}\right)$; $F_g$ must evaluate to $0$ or $1$ and the formula does just that. Apply the induction hypothesis to the four formulas $F_g$, $F^{(g \leftarrow 1)}$, $\lnot F_g$, and $F^{(g \leftarrow 0)}$ to get formulas of depth $\leq 2 \log_{3/2}(2s/3)$. The original formula $F$ can only grow by at most depth two so
		\begin{align*}
			\depth(F) &\leq \max\left\{\depth\left(F_g\right), \depth\left(F^{(g \leftarrow 1)}\right), \depth\left(\lnot F_g\right), \depth\left(F^{(g \leftarrow 0)}\right)\right\} + 2\\
			&\leq 2\log_{3/2}\frac{2s}{3} + 2\\ 
			&= (2\log_{3/2}s - 2) + 2\\
			&= 2\log_{3/2}s
		\end{align*} 
		Thus there exists a formula equivalent to $F$ of depth at most $O(\log s)$.
	\end{proof}
	
	\begin{claim}
		\label{claim:pickgoodgate}
		There exists a gate $g$ such that $F_g$ has leaf-size between $\frac{s}{3}$ and $\frac{2s}{3}$ leaves.
	\end{claim}
	\begin{proof}
		Let $r \rightsquigarrow \ell$ be a root to leaf path in the DAG containing the most $\land$/$\lor$ gates. At the root $r$, $\leafsize(F_r) = \leafsize(F) = s$ and at the leaf $\ell$, $\leafsize(F_{\ell}) = 1$. Starting at $r$ and moving down to $\ell$, the successive leaf-sizes can at most halve after each step. Thus there must exists a gate $g$ for which $\frac{s}{3} \leq \leafsize(F_g) \leq \frac{2s}{3}$.
	\end{proof}
	
	\subsection{Circuit Size of General Boolean Functions}
	\label{ssec:circuitsize}
	
	\subsubsection{Upper Bound}
	\label{sssec:circuitsizeupperbd}
	Given a function $f: \{0,1\}^n \rightarrow \{0,1\}$, let us consider some upper bounds for $\circuitsize(f)$.
	\begin{enumerate}
		\item Brute force DNF: $O(n2^n)$. There are $2^n$ rows in the truth table of $f$. Each row specifies the output given the $n$ inputs. Thus a clause with $n-1$ $\land$ gates represents each row of the table. Formally we consider the expression
		\[f(\vv{x}) = \bigvee_{\vv{a} \in f^{-1}(1)} (\vv{x} = \vv{a}) = \bigvee_{\vv{a} \in  \in f^{-1}(1)} \left(l_1 \land l_2 \land \cdots \land l_{n-1} \land l_n\right)\]
		where $l_i = x_i$ if $a_i = 1$ and $l_i = \overline{x}_i$ otherwise.
		\item Function decomposition: $O(2^n)$. Observe that 
		\[f(\vv{x}) \equiv \left(x_n \land f_1(\vv{x})\right) \lor \left(\overline{x}_n \land f_0(\vv{x})\right).\]
		where $f_1 = f(x_1, ..., x_{n-1}, 1)$ and $f_0 = f(x_1, ..., x_{n-1}, 0)$. Thus
		\[\circuitsize(f) \leq \circuitsize(f_1) + \circuitsize(f_0) + 3.\]
		Apply the decomposition recursively to $f_1$ and $f_0$. Generally at step $k$,
		\[\circuitsize(f) \leq \sum_{\vv{a} \in \{0,1\}^k}\circuitsize(f_{\vv{a}}) + 3(2^k - 1)\]
		where $f_{\vv{a}}(\vv{x}) = f(x_1, ..., x_{n-k}, a_1, ..., a_k)$. Since $f(\vv{a})$ is a constant at the $n$\textsuperscript{th} step, $3(2^n - 1)$ is an upper bound on the circuit size of $f$. 
		\item Computation reuse: $O(2^n/n)$. See Theorem \ref{thm:lupanov1958-circuitsizeupperbd} below. 
	\end{enumerate}
	
	Let $ALL_n: \{0,1\}^n \rightarrow \{0,1\}^{2^{2^{n}}}$ be the function which calculates all the $n$-ary Boolean functions at the same time\footnote{To see that the range of $ALL_n$ is indeed $2^{2^{n}}$, recall that the domain of every $n$-ary Boolean function is $2^{n}$. There is a bijection between the set of functions and the power set of $\{0,1\}^n$ (of size $2^{2^{n}}$).}. That is $(ALL_n(\vv{x}))_f \coloneqq f(\vv{x})$ for any $n$-ary Boolean function $f$.
	
	\begin{claim}
		\label{claim:upperbdalln}
		$\circuitsize(ALL_n) \leq O(2^{2^{n}})$.
	\end{claim}
	\begin{proof}
		Similar to the function decomposition analysis. For every function $f$ in the output of $ALL_n$, $f(\vv{x}) \equiv \left(x_n \land f_{1}(\vv{x})\right) \lor \left(\overline{x}_n \land f_{0}(\vv{x})\right)$ where $f_1 = f(x_1, ..., x_{n-1}, 1)$ and $f_0 = f(x_1, ..., x_{n-1}, 0)$. Note that $f_1$ and $f_0$ are outputs of $ALL_{n-1}$. See Figure \ref{fig:circuitsizealln}. Since $ALL_n$ has $2^{2^{n}}$ outputs,
		\[\circuitsize(ALL_n) \leq \circuitsize(ALL_{n-1}) + 3\left(2^{2^{n}}\right) = c\left(2^{2^{n-1}}\right) + 3\left(2^{2^{n}}\right) \in O\left(2^{2^{n}}\right)\]
		for some constant $c$. 
		\fig{circuitsizealln}{1}{Obtaining a circuit for $ALL_n$ from a circuit for $ALL_{n-1}$.}
	\end{proof}
	
	\begin{theorem}
		\label{thm:lupanov1958-circuitsizeupperbd}
		\textbf{(Lupanov 1958).} Every $n$-ary Boolean function has circuit size $O(2^n/n)$
	\end{theorem}
	\begin{proof}
		The key idea is to use $ALL_{n-k}$ in place of $\{f_{\vv{a}}: \vv{a} \in \{0,1\}^k\}$ in the analysis of function decomposition. Formally, we have 
		\[\circuitsize(f) \leq \sum_{\vv{a} \in \{0,1\}^k}\circuitsize(f_{\vv{a}}) + 3(2^k - 1) \leq \circuitsize(ALL_{n-k}) + 3(2^k - 1) \leq O\left(2^{2^{n-k}}\right) + O(2^k)\]
		where the last inequality follows from Claim \ref{claim:upperbdalln}. Observe that the two terms on the RHS are balanced when $k = n - \log(n - \log n)$ since
		\begin{align*}
			O\left(2^{2^{n-k}}\right) + O\left(2^k\right) &= O\left(2^{2^{\log(n - \log n)}}\right) + O\left(2^{n - \log(n - \log n)}\right)\\
			&= O\left(2^{n - \log n}\right)\\
			&= O\left(2^n/n\right)
		\end{align*}
		It follows that the circuit complexity of all $n$-ary Boolean function is bounded above by $O(2^n/n)$.
	\end{proof}
	
	\subsubsection{Lower Bound}
	\label{sssec:circuitsizelowerbd}
	Prior to Lupanov's result above, Shannon showed a matching lower bound. 
	\begin{theorem}
		\label{thm:shannon1949-circuitsizelowerbd}
		\textbf{(Shannon 1949).} Almost all $n$-ary Boolean functions (as $n \rightarrow \infty$) have circuit size $O(2^n/n)$.
	\end{theorem}
	\begin{proof}
		Use the counting argument. Recall the number of $n$-ary Boolean functions is $2^{2^{n}}$ and let $s = \frac{2^n}{n}$. We will show that the number of Boolean functions which can be computed by circuits of size $s$ is $\ll 2^{2^{n}}$. Let $A$ be the set of all $n$-ary circuits with $2n$ literals, $x_1, ..., x_n, \overline{x}_1, ..., \overline{x}_n$, and $s$ gates, denoted $g_1, ..., g_s$. We obtain an upper bound on the number of circuits in $A$ as follows. Each circuit can use any subset of the $s$ gates. Each $\land$/$\lor$ gate can pick two inputs from the $2n$ literals and $s-1$ other gates. If $n$ is sufficiently large (say $n \geq 100$), then $s + 2n < 3s$ so
		\[|A| \leq 2^s(s + 2n)^{2s} \leq 18^ss^{2s}.\] 
		Observe that every $n$-ary function with $\circuitsize(f) \leq s$ is computed by at least $s!$ distinct circuits in $A$ since we can permute the labels on the $s$ gates. Thus the total number of Boolean functions computed by circuits in $A$ is at most $\frac{|A|}{s!}$. Recall that $s! \geq \left(\frac{s}{e}\right)^s$. For $s = \frac{2^n}{n}$,
		\begin{align*}
			\frac{|A|}{s!} \leq \frac{18^ss^{2s}}{\left(s/e\right)^s} \leq 50^s s^{s} = 50^{2^n/n}\left(\frac{2^n}{n}\right)^{2^{n}/n} = \left(\frac{50}{n}\right)^{2^n/n}\left(2^{n}\right)^{2^{n}/n} \leq 2^{2^n-2^n/n}
		\end{align*}
		since $n \geq 100$. Thus at least $2^{s}$ Boolean formulas have circuit size greater than $s$.
	\end{proof}
	
	\subsection{Circuit Size Hierarchy}
	\begin{theorem}
		\label{thm:circuitsizehierarchy}
		If $n \leq s(n) \leq \frac{2^{n-2}}{n}$, then $\class{SIZE}[s] \subsetneqq \class{SIZE}[4s]$.
	\end{theorem}
	\begin{proof}
		Use a combination of Shannon (Theorem \ref{thm:shannon1949-circuitsizelowerbd}) and Lupanov (Theorem \ref{thm:lupanov1958-circuitsizeupperbd}). Pick\footnote{Such an $m$ must exists. When $m = 1$, $2^{m}/m \leq s(n)$ and when $m = n-1$, $2^m/m \geq s(n)$ so there must be some $m$ such that $2^{m}/m \leq s(n)$ and $2^{m+1}/(m+1) \geq s(n)$. If $2^{m+1}/(m+1) \geq 2\cdot s(n)$ then 
			\[s(n) \leq \frac{2^{m}}{m+1} \leq \frac{2^{m}}{m}\]	
			which contradicts our original choice of $m$.} 
		an $m < n$ such that
		\[s(n) \leq \frac{2^m}{m} \leq 2s(n).\]
		By Shannon, there exists a function $f: \{0, 1\}^m \rightarrow \{0,1\}$ such that 
		\[\circuitsize(f) > \frac{2^m}{m} \geq s(n).\]
		Thus $f \notin \class{SIZE}[s]$. By the tight bound from Lupanov's theorem, $\circuitsize(f) \leq 2^m/m + o(2^m/m)$ so 
		\[\circuitsize(f) \leq \frac{2\cdot 2^m}{m} \leq 4s(n)\]
		and $f \in \class{SIZE}[4s]$. 
	\end{proof}
	
\section{Lower Bounds for Explicit Functions}
	\subsection{(DeMorgan) Linear Algebra Method: \texorpdfstring{$\leafsize(PARITY_n) \in \Omega(n^2)$}{L(PARITYn) in Omega(n2)}}
	Let us define $PARITY_n$ as $\XOR_n$ and $1 - PARITY_n$ as $\overline{\XOR}_n$. Recall\footnote{Construct a circuit with $n-1$ $\xor$-gates and substituting three DeMorgan gates $(x \land \lnot y) \lor (\lnot x \land y)$ for each $x \xor y$.} that $\circuitsize(\XOR_n) \leq 3(n-1)$ and $\leafsize(\XOR_n) \leq 2^{2\ceil{\log n}}$. We will show that these bounds are tight. 
	
	Notation: $\lambda(\mm{P})$ is the largest eigenvalue of a symmetric matrix $\mm{P}$. Recall\footnote{I think this can be shown as follows. Take the largest eigen-vector $\vv{x}$ of $\mm{P}$ and decompose it in the eigen-basis of $\mm{Q}$. Then right-multiplying $\mm{P} + \mm{Q}$ by $\vv{x}$.} that 
	\[\lambda(\mm{P} + \mm{Q}) \leq \lambda(\mm{P}) + \lambda(\mm{Q}).\]
	For non-empty $A, B \subseteq \{0,1\}^n$, the matrix $\mm{M} \subseteq \{0,1\}^{A\times B}$ is the matrix 
	\[\mm{M}_{a,b} = \begin{cases}
	1 &\mbox{if $a_i \neq b_i$ for exactly one $i$}\\
	0 &\mbox{otherwise}
	\end{cases}\]
	you can read this as ``the hamming distance of $\vv{a}$ and $\vv{b}$ differs by exactly one''. Note that $\mm{M}^{\intercal}\mm{M} \in \NN^{B \times B}$ with entry $(i,j)$ interpreted as ``the number of vectors $\vv{a} \in A$ such that both $\vv{b}_i$ and $\vv{b}_j$ are one away from $\vv{a}$''. Similarly $\mm{M}\mm{M}^{\intercal} \in \NN^{A \times A}$ with entry $(i,j)$ interpreted as ``the number of vectors $\vv{b} \in B$ such that both $\vv{a}_i$ and $\vv{a}_j$ are one away from $\vv{b}$''. It is a fact from linear algebra that $\mm{M}^{\top}\mm{M}$ and $\mm{M}\mm{M}^{\top}$ have the same non-zero eigen-values. In particular,  $\lambda(\mm{M}^{\intercal}\mm{M}) = \lambda(\mm{M}\mm{M}^{\intercal})$.
	
	\begin{theorem}
		\textbf{(Koutsoupias 1993).} For any $f: \{0, 1\}^n \rightarrow \{0,1\}$, $A \subseteq f^{-1}(0)$, and $B \subseteq f^{-1}(1)$,
		\[\leafsize(f) \geq \lambda(\mm{M}^{\intercal}\mm{M}).\]
	\end{theorem}
	\begin{proof}
		By induction on $\leafsize(f)$. The base case occurs when $\leafsize(f) = 1$ and the circuit only reads in one out of the $n$ variables of the input. W.l.o.g assume that the input to the leaf is $x_1$. Then $f(\vv{x}) = x_1$ or $f(\vv{x}) = 1 - x_1$; assume the former. Let $A = f^{-1}(0)$ and $B = f^{-1}(1)$. Then $A = \{0s: s \in \{0,1\}^{n-1}\}$ and $B = \{1s: s \in \{0,1\}^{n-1}\}$. Recall that entry $(i,j)$ of $\mm{M}^{\top}\mm{M}$ is the number of elements $\vv{a} \in A$ such that both $\vv{b}_i$ and $\vv{b}_j$ differ from $\vv{a}$ by one. Notice that $\vv{a} = 0s$ and $\vv{b} = 1s'$ differ by exactly one if and only if $s = s'$. Thus $\mm{M}^{\top}\mm{M}$ is exactly the identity matrix with dimension $|B| \times |B|$ and $\lambda(\mm{M}^{\top}\mm{M}) = 1$ satisfying the theorem.  
		
		In the inductive step, let $F$ be a formula which computes $f$ of size $\leafsize(f)$. Suppose that $F = F_1 \land F_2$ for some circuits $F_1$ and $F_2$. Let $f_1$ and $f_2$ be the functions computed by $F_1$ and $F_2$ respectively. Notices that $\leafsize(f) = \leafsize(f_1) + \leafsize(f_2)$. Let $A_1 = f_1^{-1}(0)$ and $A_2 = A \backslash A_1$. Since $F = F_1 \land F_2$, $A_2 \subset f_2^{-1}(0)$ as at least one of $F_1$ or $F_2$ must evaluate to $0$. Consider matrices $\mm{M}_1 \in \NN^{A_1 \times B}$ and $\mm{M}_2 
		\in \NN^{A_2 \times B}$. Notice that $\mm{M}^{\top}\mm{M} = \mm{M}_1^{\top}\mm{M}_1 + \mm{M}_2^{\top}\mm{M}_2$ since $A_1 \cup A_2 = A$ and each matrix product counts the number of off-by-one vectors $\vv{a}$ matched to by $\vv{b} \in B$. Then
		\begin{align*}
			\lambda(\mm{M}^{\top}\mm{M}) &= \lambda(\mm{M}_1^{\top}\mm{M}_1 + \mm{M}_2^{\top}\mm{M}_2) &\mbox{(definition)}\\
			&\leq \lambda(\mm{M}_1^{\top}\mm{M}_1) + \lambda(\mm{M}_2^{\top}\mm{M}_2) &\mbox{(symmtric matrix prop.)}\\ 
			&\leq \leafsize(f_1) + \leafsize(f_2) &\mbox{(induction hyp.)}\\ 
			&= \leafsize(f)
		\end{align*} 
		The same is true if $F = F_1 \lor F_2$, but this requires decomposing $B$. Remember however that $\lambda(\mm{M}^{\top}\mm{M}) = \lambda(\mm{M}\mm{M}^{\top})$ so it does not make much of a difference.
	\end{proof}
	
	\begin{corollary}
		\label{cor:khrapchenko1071-quadraticlb}
		\textbf{(Khrapchenko 1971).}
		\[\leafsize(f) \geq \frac{\left(\sum_{\vv{a} \in A}\sum_{\vv{b} \in B} \mm{M}_{a,b}\right)^2}{|A|\cdot|B|}\]
	\end{corollary}
	\begin{proof}
		\footnote{:) I like this} The idea is to write $\lambda(\mm{M}^{\top}\mm{M})$ as a Rayleigh quotient and then substitute in $\ind{1}$ to get that lower bound.
		\begin{align*}
			\lambda\left(\mm{M}^{\top}\mm{M}\right) &= \max_{\vv{z} \in \RR^{B}\backslash \emptyset} \frac{\vv{z}^{\top}\mm{M}^{\top}\mm{M}\vv{z}}{\vv{z}^{\top}\vv{z}}\\
			&\geq \frac{\ind{1}^{\top}\mm{M}^{\top}\mm{M}\ind{1}}{|B|}\\
			&=\frac{\sum_{a \in A}\left(\sum_{b \in B}\mm{M}_{a,b}\right)^2}{|B|}\\
			&\geq \frac{\left(\sum_{a \in A} \sum_{b \in B} \mm{M}_{a,b}\right)^2}{|A|\cdot|B|}
		\end{align*}
		where the last inequality follows by Cauchy-Schwartz\footnote{The application of Cauchy-Schwartz here is subtle. The key is to multiply top and bottom by $\left(\sum_{a \in A} 1^2\right)$ and combine the two sum of squares.}.
	\end{proof}
	
	We use the above Corollary \ref{cor:khrapchenko1071-quadraticlb} to show that $\leafsize(\XOR_n) \geq n^2$. Take $A$ and $B$ to be the set of even and odd strings\footnote{Here the parity of the string $s$ corresponds to the parity of the sum of ones in $s$.} in $\{0,1\}^n$ respectively. Then, by the above,
	\[\leafsize(f) \geq \frac{\left(\sum_{a \in A}\sum_{b \in B} \mm{M}_{a,b}\right)^2}{|A|\cdot|B|} = \frac{\left(n2^{n-1}\right)^2}{2^{n-1}\cdot 2^{n-1}} = n^2.\]
	This technique can achieve gaps of at most $n^2$. Exercise: (1)\footnote{Hint: Take $A = \{s \in \{0,1\}^n:s\mbox{ has exactly $\ceil{n/2}-1$ ones}\}$ and $B = \{t \in \{0,1\}^n:t\mbox{ has exactly $\ceil{n/2}$ ones}\}$.} prove lower-bound $\leafsize(MAJ_n) \geq \Omega(n^2)$ and (2) can you devise an upper bound of $\leafsize(MAJ_n) \leq n^{O(1)}$.
	
	\subsection{(General) Gate Elimination: \texorpdfstring{$\circuitsize(PARITY_n) \in \Omega(n)$}{C(PARITYn) in Omega(n)}}
	\begin{definition}
		For $i \in [n]$ and $b \in \{0,1\}$ the \textbf{1-bit restriction}, $x_i \leftarrow b$ is the $n$-ary function $f^{(x_i \leftarrow b)}$. The substitution can be done \emph{syntactically} for circuits $C$, namely, $C^{(x_i \leftarrow b)}$. The technique is to substitute $x_i \leftarrow b$ and $\overline{x}_i \leftarrow 1 - b$ and performing the relevant simplifications.
	\end{definition}
	
	There are a couple of observations to note. (1) If $C$ computes $f$, then $C^{(x_i \leftarrow b)}$ computes $f^{(x_i \leftarrow b)}$. (2) If $x_i$ appears below a gate in $C$ then for both settings of $b \in \{0,1\}$, $\size\left(C^{(x_i \leftarrow b)}\right) \leq \size(C) - 1$ i.e. any setting of $b$ will knock out one gate in $C$. (2) There exists a setting of $b$ for each gate, such that $\size\left(C^{(x_i \leftarrow b)}\right) \leq \size(C) - 2$ i.e. the setting of $b$ knocks out two gates in $C$.
	
	\begin{theorem}
		\label{thm:schnorr1979-XORlb}
		\textbf{(Schnorr 1979).} $\circuitsize(PARITY_n) \geq 3(n-1)$.
	\end{theorem}
	\begin{proof}
		By induction. The base case where $n = 1$ is trivial. The crucial observation is as follows. If a literal is below $k$ $\land/\lor$ gates (of the same type), then there is a setting of the literal such that you can knock out at least $k$ gates. Just think about the different settings of the literal.
		
		Consider any circuit $C$ which calculates the $PARITY_n$ function. Identify three gates in $C$:
		\begin{enumerate}
			\item A gate whose inputs are two literals. Let these be $x_i$ and $x_j$. 
			\item Pick a literal of the previous gate, say $x_i$. Find another gate with $x_i$ as an input. Suppose such a gate does not exist. Then, by setting $x_j$ appropriately, we could knock out the gate in step $1$ and the output would not depend on $x_i$. This would not calculate the $PARITY_n$ function.
			\item The gate above the one in step 2. Such a gate exists if the gate from step $2$ is not the output of the circuit. Suppose for a contradiction that it was. Then a setting of $x_i$ would fix the output. This would also not calculate the $PARITY_n$ function.
		\end{enumerate}  
		By setting $x_i$ appropriately, we can kills all three gates above. See Figure \ref{fig:schnorr1979}.
		
		\fig{schnorr1979}{0.7}{The three gates that get eliminated when we restrict $x_i$. The actual setting of $x_i$ depends on the gate type.}
		
		By the induction hypothesis, $C^{(x_i \leftarrow b)}$ has at least $3(n - 2)$ gates. Since we were able to eliminate three gates by setting $x_i$, we know that $C$ has to have $3(n-1)$ gates.  
	\end{proof}
	More sophisticated versions of gate elimination allow for slightly better lower bounds. The current record is $5n - o(n)$ for DeMorgan circuits and $\left(3 + \frac{1}{86}\right)n$ for circuits in the full binary basis. 

	\subsection{(DeMorgan) Random Restriction: \texorpdfstring{$\leafsize(ANDREEV_{k,m}) \in \Omega(n^3)$}{L(ANDREEVkm) in Omega(n3)}}
	\subsubsection{Subbotovskaya's Method} 
	\begin{definition}
		A formula $F$ is \textbf{nice} if for every sub-formula of the form $x_i \land F'$, $\overline{x}_i \land F'$, $x_i \lor F'$, $\overline{x}_i \lor F'$, the variable $x_i$ does \emph{not} occur in $F'$.
	\end{definition}
	
	\begin{lemma}
		Every formula is equivalent to a \emph{nice} formula of the same (or less) leaf size.
	\end{lemma}
	\begin{proof}
		Given sub-formulas of the form $x_i \land F$, $\overline{x}_i \land F$, $x_i \lor F$, and $\overline{x}_i \lor F$ where $F$ contains literals $x_i$ or $\overline{x}_i$, repeatedly apply 
		\begin{align*}
			x_i \land F \rightarrow x_i &\land F^{(x_i \leftarrow 1)}\\
			\overline{x}_i \land F \rightarrow \overline{x}_i &\land F^{(x_i \leftarrow 0)}\\
			x_i \lor F \rightarrow x_i &\lor F^{(x_i \leftarrow 0)}\\
			\overline{x}_i \lor F \rightarrow \overline{x}_i &\lor F^{(x_i \leftarrow 1)}\\
		\end{align*}
		This shows that every minimal formula for a function $f$ is \emph{nice}.
	\end{proof}
	
	\begin{lemma}
		\label{lem:onebitrestrictionbd}
		For every $f: \{0,1\}^n \rightarrow \{0,1\}$,
		\[\expected_{i \in [n], b\in \{0,1\}}\left[\leafsize\left(f^{(x_i \leftarrow b)}\right)\right] \leq \left(1 - \frac{1}{n}\right)^{1.5} \leafsize(f).\]
	\end{lemma}
	\begin{proof}
		Let $F$ be a minimal nice formula for $f$. Let $\ell_i$ be the all leaves of $F$ labelled with $x_i$ or $\bar{x}_i$. Then $\leafsize(f) = \sum_{i = 1}^{n} \ell_i$.  Notice that every gate $g$ with a leaf $\lambda$ has an associated sub-formula $F'$ such that $\lambda$ does not occur in $F'$.
		
		For a bit $b \in \{0,1\}$, the random restriction $F^{(x_i \leftarrow b)}$ will kill leaf $x_i$ with probability $1$ and kill all leaves in $F'$ with probability $\frac{1}{2}$. Thus in expectation, $1.5$ leaves are killed under the 1-bit restriction $F^{(x_i \leftarrow b)}$. For each $i \in [n]$ we have
		\[\expected_{b \in \{0,1\}}\left[\leafsize(F) - \leafsize\left(F^{(x_i \leftarrow b)}\right)\right] \geq 1.5 \ell_i.\] 
		Averaging over all choices of $i$, we have that 
		\[\expected_{i \in [n], b \in \{0,1\}}\left[\leafsize(F) - \leafsize\left(F^{(x_i \leftarrow b)}\right)\right] \geq \frac{1.5}{n}\sum_{i = 1}^{n} \ell_i = \frac{1.5\leafsize(F)}{n}.\]
		Rearranging the above, we have
		\[\expected_{i \in [n], b \in \{0,1\}}\left[\leafsize\left(F^{(x_i \leftarrow b)}\right)\right] \leq \left(1 - \frac{1.5}{n}\right)\leafsize(F) \leq \left(1 - \frac{1}{n}\right)^{1.5} \leafsize(F)\]
		where the last inequality follows as $1 - ax \leq (1 - x)^a$.
	\end{proof}
	Apparently, this lemma implies that $\leafsize(\xor_n) \geq n^{1.5}$.
	
	\begin{definition}
		\label{def:restrictions}
		A \textbf{restriction} $\rho$ is a function $\rho: [n] \rightarrow \{0,1,*\}$ which can be thought of as a partial assignment of an $n$-ary Boolean function $f$. Denote the restriction of $f$ under $\rho$ as $f \upharpoonright \rho: \{0,1\}^{\rho^{-1}(*)} \rightarrow \{0,1\}$. Further $\rho$ is a \textbf{$k$-star restriction} if $|\rho^{-1}(*)| = k$. 
		
		Let $p\in [0,1]$. In a \textbf{$p$-random restriction} where you set
		\[R_p(i) = \begin{cases}
		*&\mbox{with probability } p\\
		0&\mbox{with probability } \frac{1-p}{2}\\
		1&\mbox{with probability } \frac{1-p}{2}
		\end{cases}\]
	\end{definition}
	
	\begin{theorem}
		\label{thm:subbotovskaya-restrictionlb}
		Let $f: \{0,1\}^n \rightarrow \{0,1\}$ and let $\rho$ be a uniform random $k$-start restriction. Then 
		\[\expected\left[\leafsize(f\upharpoonright \rho)\right] \leq \left(\frac{k}{n}\right)^{1.5}\leafsize(f).\]
	\end{theorem}
	\begin{proof}
		Repeatedly apply Lemma \ref{lem:onebitrestrictionbd} to restrict $k$ bits to get
		\[\expected\left[f \upharpoonright \rho\right] \leq \left(1 - \frac{1}{n}\right)^{1.5}\cdot\left(1 - \frac{1}{n-1}\right)^{1.5}\cdots\left(1 - \frac{1}{k+1}\right)^{1.5} \leafsize(f) = \left(\frac{k}{n}\right)^{1.5}\leafsize(f).\]
	\end{proof}
	
	\begin{corollary}
		\label{cor:extsubbotovskayatorandrestrictions}
		\textbf{(Subbotovskaya 1961).} 
		\[\expected\left[\leafsize\left(f\upharpoonright R_p\right)\right] \leq O\left(p^{1.5}\leafsize(f) + 1\right).\]
	\end{corollary}
	According to H\aa stad (19 something or other) and Tal (2014), this can be improved to $O(p^2\leafsize(f) + 1)$.
	
	\textbf{Open problem}: what is the shrinkage exponent of monotone formulas? (this is known to be between $2$ and $\left(\log (\sqrt{5}) - 1\right)^{-1} = 3.27$).

	\subsubsection{DEF: Composition of Boolean Functions}
	\begin{definition}
		Let $f:\{0,1\}^k \rightarrow \{0,1\}$ and $g: \{0,1\}^m \rightarrow \{0,1\}$. Let $f \comp g: (\{0,1\}^m)^k \rightarrow \{0,1\}$ is defined as
		\[(f \comp g)(\vv{x}_1, ..., \vv{x}_k) = f\left(g(\vv{x}_1), ..., g(\vv{x})\right).\]
		In essence the composition is of the form $f \comp g = f \circ g^k$. 
	\end{definition}
	Think of the input of the composition as a matrix $\mm{X} \in \{0,1\}^{k \times m}$ with rows $\vv{x}_1, ..., \vv{x}_k$. Apply $g$ to each row, then apply $f$ to the resulting column vector. Observe that $\leafsize(f \comp g) \leq \leafsize(f) \cdot \leafsize(g)$. 
	
	\begin{conjecture}
		\textbf{(KRW).} For all functions $f$ and $g$,
		\[\leafsize\left(f \comp g\right) = \tilde{\Omega}\left(\leafsize(f)\cdot \leafsize(g)\right)\]
		where $\tilde{\Omega}(t(n)) = \Omega(t(n))/\left(\log t(n)\right)^{O(1)}$ for any function $t(n)$.
	\end{conjecture}
	
	The following is an explicit $n$-ary Boolean function for which the lower bound is true.
	\begin{lemma}
		\label{lem:bdcompfwithxor}
		For all $k, m \geq 1$ and $f: \{0,1\}^k \rightarrow \{0,1\}$,
		\[\leafsize\left(f\comp XOR_m\right) \geq \leafsize(f) \cdot \Omega\left(\left(\frac{m}{\log k}\right)^2\right).\]
	\end{lemma}
	\begin{proof}
		Let $p = \frac{2\ln k}{m}$. Apply $R_p$ on $k\times m$ variables of $f \comp XOR_m$. If $R_p$ has a $*$ in every row then 
		\[\leafsize\left(\left(f\comp XOR_m\right) \upharpoonright R_p\right) \geq \leafsize(f)\]
		since a formula which calculates the LHS can be used to calculate the RHS. In particular, if there is a $*$ in some row $i$, then from the perspective of $XOR_m$, the value of row $i$ is undetermined. If every single row is undetermined, then the input to $f$ is undetermined. Thus $\left(f \comp XOR_m\right) \upharpoonright R_p$ would be able to compute $f(s)$ for any $s \in \{0,1\}^k$.
		
		Let $E$ be the event that there exists a $*$ in every row of the input matrix after applying $R_p$. We bound $\Pr[E]$ below by bounding $\Pr\left[\overline{E}\right]$ above. Let $B_i$ be the event that some row $i$ is \emph{bad} i.e. does not have a $*$ after applying $R_p$. Since every element is fixed with probability $1-p$, $\Pr[B_i] = (1 - p)^{m}$ for all $i \in [k]$. Then 
		\[\Pr\left[\overline{E}\right] \leq \sum_{i = 1}^{k} \Pr[B_i] = k(1-p)^{m} \approx k\exp(-pm) \leq \frac{1}{k}\] 
		where the first inequality follows by union-bound and the last by the definition of $p$ above. Thus we have the following lower bound
		\begin{equation}
		\label{eq:lbdlembdcomfwithxor}
		1 - \frac{1}{k} \leq \Pr\left[E\right] 
		\end{equation} 
		
		To get an upper bound for $\Pr[E]$, observe that $\Pr[E] \leq \Pr\left[\leafsize\left(\left(f \comp XOR_m\right) \upharpoonright R_p\right) \geq \leafsize(f)\right]$. Apply Markov's inequality to get
		\[\Pr\left[\leafsize\left(\left(f \comp XOR_m\right) \upharpoonright R_p\right) \geq \leafsize(f)\right] \leq \frac{\expected\left[\leafsize\left(\left(f \comp XOR_m\right) \upharpoonright R_p\right)\right]}{\leafsize(f)}\]
		By the improvement noted after Corollary \ref{cor:extsubbotovskayatorandrestrictions}, $\expected\left[f \upharpoonright R_p\right] \leq O\left(p^2\leafsize(f) + 1\right)$, we have
		\begin{equation}
		\label{eq:ubdlembdcomfwithxor}
		\Pr[E] \leq \frac{\expected\left[\leafsize\left(\left(f \comp XOR_m\right) \upharpoonright R_p\right)\right]}{\leafsize(f)} = O\left(\frac{p^2\leafsize(f \comp XOR_m) + 1}{\leafsize(f)}\right). 
		\end{equation} 
		
		Combining the lower bound from Equation (\ref{eq:lbdlembdcomfwithxor}) and the upper bound from Equation (\ref{eq:ubdlembdcomfwithxor}), we have
		\[1 - \frac{1}{k} \leq \Pr[E] \leq \frac{p^2\leafsize(f \comp XOR_m) + 1}{\leafsize(f)}\]
		Rearrange with respect to $\leafsize(f \comp XOR_m)$, taking care to observe that $1 - \frac{1}{k} - \frac{1}{\leafsize(f)} \in O(1)$, to obtain
		\[\leafsize\left(f \comp XOR_m\right) \geq \frac{\leafsize(f)}{p^2} = \leafsize(f) \cdot \Omega\left(\left(\frac{m}{\log k}\right)^2\right)\]
		as required. 
	\end{proof}
	
	\subsubsection{FUN: \texorpdfstring{$ANDREEV_{k,m}$}{ANDREEVkm}}
	Let us construct an explicit function with cubic lower bound on the leaf size using Lemma \ref{lem:bdcompfwithxor}.
	\begin{definition}
		For parameters $k, m \in \NN$,
		\[ANDREEV_{k,m}: \{\mbox{$k$-ary Boolean function}\} \times \{0,1\}^{k\times m} \rightarrow \{0,1\}\] 
		such that $ANDREEV(f, \mm{X}) = \left(f\comp XOR_m\right)(\mm{X})$.
		
		Think of this as follows: consider the $(k + 1) \times 2^{k}$ table $T$ of $k$-ary Boolean strings and the evaluation of $f$ on these strings. See Table \ref{table:andreev}. The input matrix $\mm{X} \in \{0,1\}^{k \times m}$. Apply the $XOR_m$ function to each row of $\mm{X}$ to obtain a $k$-bit string $s$. Find the column of $T$ corresponding to $s$ and return $f(s)$.
		\begin{table}[!ht]
			\centering
			\begin{tabular}{|c|c|c|}
				\hline
				$f(0^k)$ & $\cdots$ & $f(1^k)$\\
				\hline
				$0$ & $\cdots$ & $1$\\
				$\vdots$ & $\vdots$ & $\vdots$\\
				$0$ & $\cdots$ & $1$\\
				\hline
			\end{tabular}
			\caption{Table $T$ of function $f$.}
			\label{table:andreev}
		\end{table}	
	\end{definition}
	When $m = 1$, $ANDREEV_{k,1}$ is just the multiplexor function. Let $n = 2^k + mk$. Then $ANDREEV_{k,m}$ can be thought of as an $n$-ary Boolean function with $\circuitsize(ANDREEV_{k, m}) = O(n)$. 
	
	\begin{theorem}
		\label{thm:L-DmFormulaCubicExplicit-Andreev}
		For every $f: \{0,1\}^k \rightarrow \{0,1\}$ we have
		\[\leafsize\left(ANDREEV_{k,m}\right)\geq \leafsize\left(f\comp XOR_m\right) \geq \leafsize(f)\cdot\Omega\left(\left(\frac{m}{\log k}\right)^2\right).\]
	\end{theorem}
	\begin{proof}
		By fixing $2^k$ values $f(s)$ for $s \in \{0,1\}^k$, in the formula for $ANDREEV_{k,m}$, we can calculate $f \comp XOR_m$. Thus $\leafsize(ANDREEV_{k,m}) \geq \leafsize(f \comp XOR_m)$. By Shannon's Theorem \ref{thm:shannon1949-circuitsizelowerbd}, there exists a $k$-ary Boolean function $f$ with circuit size, and thus leaf size, $\Omega(2^k/k)$. Let $m = 2^k/k$ and note that $n = 2^k + mk \in \Theta(2^k)$. Then, by Lemma \ref{lem:bdcompfwithxor},
		\[\leafsize(ANDREEV_{k,m}) \geq \Omega\left(\frac{2^k}{k}\right) \cdot \Omega\left(\left(\frac{m}{\log k}\right)^2\right) = \Omega\left(\frac{n^3}{(\log n)^3(\log \log n)^2}\right).\]
		Thus $\leafsize(ANDREEV_{k,m}) \in \tilde{\Omega}(n^3)$.
	\end{proof}
	This lower bound for the $ANDREEV_{m,k}$ is nearly tight since $\leafsize(ANDREEV_{k,m}) \in \tilde{O}(n^3)$. 
	
	\subsection{(Full Binary) Subset Subfunction: \texorpdfstring{$\leafsize(ED_n) \in \Omega(n^2)$}{L(EDn) in Omega(n2)}}
	\subsubsection{DEF: \texorpdfstring{$V$}{V}-Subfunctions}
	Let $B_2$ be the full binary basis (all 2-ary gate types). Unfortunately, the random restriction idea does not work in this setting since it is \emph{not true} that
	\[\expected[\leafsize_{B_2}(f \upharpoonright R_p)] \leq O\left(p^{1 + \epsilon}\leafsize_{B_2}(f) + 1\right)\]
	for any $\epsilon > 0$. Do you see why?\footnote{Let $f$ be the parity function.} 
	
	\begin{definition}
		\label{def:NOTE-Vsubfunctions}
		For $f: \{0,1\}^n \rightarrow \{0,1\}$ and $V \subset [n]$, 
		\[\sub_V(f) = \{f\upharpoonright \rho: \rho:[n] \rightarrow \{0,1,*\} \mbox{ such that } \rho^{-1}(*) = V\}\] 
		be the set of \textbf{$V$-subfunctions} of $f$.
		
		Further, define 
		\[\sub_{V}^{*}(f) = \{f', 1 - f', \underline{0}, \underline{1}: f' \in \sub_V(f)\}\] where $\underline{b}$ is the constant $b$ function for $b \in \{0,1\}$. Note that $|\sub_{V}^{*}(f)| \leq 4\cdot|\sub_{V}(f)|$ (actually $|\sub_{V}^{*}(f)| \leq 2\cdot|\sub_{V}(f)| + 2$). 
		
		Let $F$ be an $n$-ary formula and $V \subset [n]$ as before. Then \textbf{the number of leaves of $F$ labelled by variables in $V$} be denoted $\ell_{V}(F)$. Note that $\leafsize(F) = \ell_V(F) + \ell_{[n]\backslash V}(F)$.
	\end{definition}
	
	\begin{example}
		Consider $MAJ_3(x_1, x_2, x_3)$ and $V = \{1, 2\}$. Then 
		\[\sub_V(MAJ_3) = \{x_1 \land x_2, x_1 \lor x_2\}\]
		when $x_3$ is restricted to $0$ and $1$ respectively. 
	\end{example}
	
	\subsubsection{Nechiporuk's Bound}
	Here are two important properties to note:
	\begin{enumerate}
		\item Suppose $F = \gate(G,H)$ for some $\gate: \{0,1\}^2 \rightarrow \{0,1\}$. Then 
		\[\sub_V(F) \subseteq \{\gate(g, h): g \in \sub_V(G) \mbox{ and } h \in \sub_V(H)\}.\]
		and $|\sub_V^*(F)| \leq |\sub_V^*(G)| \cdot |\sub_V^*(H)|$. This should be pretty obvious. Let $f_V$ be any function in $\sub_V(f)$. Then this function must be equivalent to the composition of the function computed by $\gate$ and some two functions $g \in \sub_V(G)$ and $h \in \sub_V(H)$.
		\item Suppose that $F = \gate(G, H)$ and $\ell_V(H) = 0$. Then $\sub_V^*(F) \subseteq \sub_V^*(G)$. Note that $\ell_V(H)$ means that none of the leaves in $H$ are labelled with any indices from $V$. When considering the $V$-functions of $H$, these can only be the constant functions $\underline{0}$ and $\underline{1}$. When composing the function calculated by $\gate$ with some $g \in \sub_V(G)$ and a function in $\{\underline{0}, \underline{1}\}$ we can only get $\{g, 1-g, \underline{0}, \underline{1}\}$. Thus every function in $\sub_V^*(F)$ is also in $\sub_V^{*}(G)$.
	\end{enumerate}
	
	\begin{lemma}
		\label{lem:U-Vsubfunctions}
		If $F$ is an $n$-ary formula, $V \subseteq [n]$, and $\ell_{V}(F) \geq 1$, then \[|\sub_V^*(F)| \leq 4 \cdot 16^{\ell_V(F) - 1}.\] 
	\end{lemma}
	\begin{proof}
		By induction on $\leafsize(F)$. The base case where $\leafsize(F) = 1$ is trivial. The inductive case is also not that bad considering the two observations above. Suppose $F = \gate(G, H)$. If one of $\ell_V(G) = 0$ or $\ell_V(H) = 0$, then we can use the second observation. W.l.o.g assume $\ell_V(H) = 0$. By the induction hypothesis we have that  
		\[|\sub_V^{*}(F)| \leq |\sub_V^*(G)| \leq 4 \cdot 16^{\ell_V(G) - 1} \leq 4 \cdot 16^{\ell_V(F) - 1}.\]
		
		Next suppose that $\ell_V(G) \geq 1$ and $\ell_V(H) \geq 1$. Then by the induction hypothesis, we have that 
		\[|\sub_V^{*}(F)| \leq |\sub_V^*(G)| \cdot |\sub_V^*(G)| = 16^{\ell_V(G) + \ell_V(H) - 1} = 16^{\ell_V(F) - 1}\leq 4 \cdot 16^{\ell_V(F) - 1}\]
		as required.
	\end{proof}
	
	\begin{corollary}
		\label{cor:U-Vsubfunctions}
		Let $F$ and $V$ be as above, then $|\sub_V(F)| \leq 16^{\ell_V(F)}$.
	\end{corollary}
	This is immediate when $\leafsize(F) \geq 1$. Only $\underline{b}$, $b \in \{0,1\}$, have leaf-size $0$, but $\left|\sub_V\left(\underline{b}\right)\right| = 1 \leq 16^{0}$. 
	
	\begin{theorem}
		\label{thm:L-B2Formula-Nechiporuk}
		\textbf{(Nechiporuk's Bound).} For any $f: \{0,1\}^n \rightarrow \{0,1\}$ and any partition of $[n]$ into $t$ disjoint components $V_1 \uplus \cdots \uplus V_t$
		\[\leafsize_{B_2}(f) \geq \frac{1}{4}\sum_{i = 1}^{t}\log|\sub_{V_i}(f)|.\]
	\end{theorem}
	\begin{proof}
		Direct application of Corollary \ref{cor:U-Vsubfunctions}. Let $F$ be the minimal formula computing $f$. Then
		\[\leafsize_{B_2}(f) = \sum_{i = 1}^{t}\ell_{V_i}(F) \geq \sum_{i = 1}^{t}\log_{16}|\sub_{V_i}(F)| = \frac{1}{4} \sum_{i = 1}^{t}\log|\sub_{V_i}(F)|.\]
	\end{proof}
	
	\subsubsection{FUN: Element Distinctness \texorpdfstring{$ED_n$}{EDn}}
	Let us apply Theorem \ref{thm:L-B2Formula-Nechiporuk} to an explicit function in the full binary basis to get a lower bound.
	\begin{definition}
		\label{def:FUN-elementdistinctness}
		For $k \in \NN$, let $n = 2^k \cdot 2k$. The \textbf{element distinctness} function $ED_n$ is
		\[ED_n: \{0,1\}^{2^k \times 2k} \rightarrow \{0,1\}\] where
		\[ED_n(X_1, ..., X_{2^k}) = \begin{cases}
		1 &\mbox{if $X_1, ..., X_{2k}$ are distinct elements of $\{0,1\}^{2k}$}\\
		0 &\mbox{otherwise}
		\end{cases}\]
		Think of this as being given $2^k$ binary strings of length $2k$ and asked if they are all distinct.
	\end{definition} 
	
	\begin{theorem}
		\label{thm:L-B2FormulaQuadraticExplicit}
		\[\leafsize_{B_2}(ED_n) = \Omega\left(\frac{n^2}{\log n}\right).\]
	\end{theorem}
	\begin{proof}
		Apply Nechiporuk's bound with $V_i$ as a block of length $2k$ corresponding to the coordinates of $X_i$. Remember $n = 2^k + 2k$.  
	\end{proof}
	Exercise: show that $\Omega(n^2/\log n)$ is the limit on the lower bound achievable by Nechiporuk's method.
	

\section{Non-uniformity is More Powerful than Randomness}
	\begin{definition}
		\label{def:randomizedcircuit}
		A \textbf{randomized circuit} for a function $f: \{0,1\}^n \rightarrow \{0,1\}$ is a circuit $C$ with $n + m$ variables $x_1, ..., x_n$ and $y_1, ..., y_m$ (think of $\vv{x}$ as the input and $\vv{y}$ as a random seed) such that for every $\vv{x} \in \{0,1\}^n$
		\[\Pr_{\vv{y} \in \{0,1\}^m}\left[C(\vv{x}, \vv{y}) = 1\right] \begin{cases}
		\geq \frac{2}{3} &\mbox{if $f(\vv{x}) = 1$}\\
		\leq \frac{1}{3} &\mbox{if $f(\vv{x}) = 0$}
		\end{cases}\]
	\end{definition}
	Let $\class{BPP/poly}$ be the class of Boolean functions computable by poly-sized randomized circuits --- think of $\class{BPP/poly}$ as the non-uniform version of $\class{BPP}$. Generally $\frac{1}{3}$ and $\frac{2}{3}$ can be replaced with any $a$, $b$ satisfying $0 < a < b < 1$. 
	
	\begin{theorem}
		\textbf{(Adelman 1978).} If $f$ is computable by poly-size randomized circuit, then it is computable by poly-sized (deterministic) circuits i.e. $\class{BPP/poly} \subseteq \class{P/poly}$.
	\end{theorem}
	\begin{proof}
		The intuition is to improve the probability of success by doing repeated trials, taking the majority, then use the probabilistic method to show that there was a good choice of $\vv{y}$ which we can hard-wired into the circuit.
		
		Let $f$ be the given $n$-ary function with $\class{BPP/poly}$ circuit $C(\vv{x}, \vv{y})$. Recall that $MAJ_k$ has $O(k)$ sized circuits. Construct the composite function $g_k: \{0, 1\}^{n} \times \{0,1\}^{k \times m} \rightarrow \{0,1\}$ such that 
		\[g_k(\vv{x}, \mm{Y}) = MAJ_k\left(C(\vv{x}, \vv{y}_1), ..., C(\vv{x}, \vv{y}_k)\right)\] 
		where each $\vv{y}_i$ is the $i$\textsuperscript{th} row of $\mm{Y}$. Observe that $\circuitsize(g_k) \leq k \cdot \circuitsize(f) + O(k)$ since we can replace each of the $k$ inputs of $MAJ_k$ by a circuit of size $\circuitsize(f)$. If $k$ is $\poly(n)$ then $g_k \in \class{P/poly}$.
		
		On a randomly sampled seed $\vv{y}_i$, let $X_i$ be the indicator r.v. for $C(\vv{x}, \vv{y}_i) \neq f(\vv{x})$. Let $X = X_1 + \cdots + X_k$. Observe that 
		\begin{align*}
			\Pr_{\mm{Y} \in \{0,1\}^{k \times m}}\left[g_k(\vv{x}, \mm{Y}) \neq f(\vv{x})\right] &= \Pr_{\vv{y}_1, ..., \vv{y}_k \in \{0,1\}^{m}}\left[MAJ_k\left(C(\vv{x}, \vv{y}_1), ..., C(\vv{x}, \vv{y}_k)\right) \neq f(\vv{x})\right]\\
			&= \Pr\left[X \geq \frac{k}{2}\right]\\
			&= \Pr\left[X \geq (1 + \epsilon)pk\right]
		\end{align*}
		where $p = \Pr[C(\vv{x}, \vv{y}) \neq f(\vv{x})]$ and $\epsilon = \frac{1 - 2p}{2p}$. From Definition \ref{def:randomizedcircuit}, we have $p = \frac{1}{3}$ and $\epsilon = \frac{1}{2}$. Thus by Chernoff bound we have
		\[\Pr_{\mm{Y} \in \{0,1\}^{k \times m}}\left[g_k(\vv{x}, \mm{Y}) \neq f(\vv{x})\right] = \Pr\left[X \geq (1 + \epsilon)pk\right] \leq \exp\left(\frac{-\epsilon^2pk}{2 + \epsilon}\right) = \exp\left(\frac{-k}{30}\right).\]
		When $k > 30$, $\Pr\left[g_k(\vv{x}, \mm{Y}) \neq f(\vv{x})\right] \leq 2^{-n}$ and there exists a $\mm{Y}$ for which $g_k(\vv{x}, \mm{Y}) = f(\vv{x})$ with probability greater than $1 - 2^{-n}$. Since there are only $2^{n}$ inputs $\vv{x}$, $g_k(\vv{x}, \mm{Y})$ matches $f(\vv{x})$ on every input. Hard-wiring $\mm{Y}$ into the circuit for $g_k$ produces a deterministic $\poly(n)$ circuit for $f$. 
	\end{proof}
	
	\section{Restricted Setting}
	Welp, that is all the bounds that we could get out of the general case. Time to consider some restricted settings.
	
	\subsection{Monotone Circuits}
	\begin{definition}
		A \textbf{monotone circuit}
	\end{definition}
	
		\subsubsection{Upper Bound: Majority}
		Observe that if $MAJ_n(\vv{x}) = 0$, then for some uniformly random bit $x_i$, $\Pr[x_i = 1] = \frac{1}{2} - \frac{1}{2n}$.
		\begin{theorem}
			\textbf{(Valiant 1984).} $MAJ_n$ has poly-sized monotone circuits\footnote{And monotone formulas apparently!}.
		\end{theorem}
		\begin{proof}
			We are going to use the idea of amplification and projections. The idea is to compose $MAJ_3$ with itself $k$ times. 
		\end{proof}
		
		\begin{definition}
			For $f: \{0,1\}^m \rightarrow \{0,1\}$, let $\mu_f: [0,1] \rightarrow [0,1]$ be defined as
			\[\mu_f(p) = \Pr_{y_1, ..., y_m \in Bern(p)}\Pr[f(y_1, ..., y_m) = 1].\]
		\end{definition}
		$\mu$ is particularly nice for monotone, non-constant functions $f$. 
		
		\begin{example}
			$MAJ_3(p) = p^3 + 3p^2(1-p)$.
		\end{example}
		Observe that $\mu_{f \comp g}(p) = \mu_f(\mu_g(p))$. To see this, you should draw out the little tree. 
		
		\begin{lemma}
			There is a constant $c < 3$ such that $\mu_{MAJ_3}^{c\log n}\left(\frac{1}{2} - \frac{1}{2n}\right)$
		\end{lemma}
		
		A striking consequence of this result.
		\begin{definition}
			$f: \{0,1\}^n \rightarrow \{0,1\}$ is a \textbf{Slice functions} if there exists $k \in \{0, ..., n\}$ such that $f(x) = 0$ if $|x| < k$ and $f(x) = 1$ if $|x| > k$.
		\end{definition}
		
		\begin{theorem}
			\textbf{(Berkowitz 1982).} If $f$ is a slice function then...
		\end{theorem}
	
	\subsection{Bounded Depth Circuits: \texorpdfstring{$\class{AC}^0$}{AC0}}
	
	\begin{definition}
		$\class{AC}^0$
	\end{definition}
	
	\textbf{Challenge.} Let $\leafsize_{d}(f)$ be the leaf size of an $n$-ary Boolean function $f$ in $\class{AC}^0$ with depth at most $d$. Observe that $\leafsize_2(PARITY_n) \leq n2^n$ as you can take an ``or'' of $2^n$ ``and'' gates with fan-in $n$ each specifying a setting of the $n$ input variables.
	
	Further note that for $k$ and $n_1, ..., n_k$ where $\sum_{i = 1}^{k} n_i = n$, 
	\begin{equation}
	\label{eq:U-AC0recurrence}
	\leafsize_{d+1}(PARITY_n) \leq 2^{k-1}\sum_{i = 1}^{k}\leafsize_{d}(XOR_{n_i}).
	\end{equation}
	
	(I still need to work out the details of this proof). Using $\leafsize_2(PARITY_n) \leq n2^n$ for the base case and the recurrence shown in Equation (\ref{eq:U-AC0recurrence}), we can show 
	\[\leafsize_{d+1}(PARITY_n) \leq n2^{dn^{1/d}}\]
	for any $n$ and $d \geq 2$. However, when $n$ is a power of $2$, we can get a slightly tighter bound of 
	\[\leafsize_{d+1}(PARITY_n) \leq n2^{d\left(n^{1/d} - 1\right)}.\]
	
	Ben suspects that the above inequality holds for all $n$, not just powers of two, since for $d = \ceil{\log n}$ it is know that $n^{1/d} - 1 = 2^{\log n/ \log n} - 1 = 1$ in the exponent of $2$ is sufficient i.e. it is known that
	\[\leafsize_{\ceil{\log n}}(PARITY_n) \in O(n^2).\]
	Further, Ben believes that it is sufficient to achieve this tighter bound by analyzing the recurrence relation more carefully. 
	
	\section{H\aa stad's Switching Lemma}
	\begin{definition}
		\label{def:decisiontrees}
		A \textbf{decision tree} (DT) is a rooted binary tree whose leaves are labelled by $\{0,1\}$ and whose internal nodes are labelled by variables. The \textbf{depth} of a decision tree is the length of the longest root-to-leaf path. For $f: \{0,1\}^n \rightarrow \{0,1\}$, let $\DTdepth(f)$ denote depth of the minimum depth DT that computes $f$. 
				
		It is useful to consider formulas as a DNF or CNF. The \textbf{width} of a DNF(CNF) formula is the maximum number of literals in any clause. 
	\end{definition}
	
	\begin{definition}
		\label{def:canonicaldecisiontree} 
		Let $F = C_1 \lor \cdots \lor C_m$ be a $k$-DNF with an arbitrary fixed order on the clauses and variables. Let $\rho: \{x_1, ..., x_n\} \rightarrow \{*, 0, 1\}$ be a restriction. Then the \textbf{canonical decision tree} of $F \upharpoonright \rho$, denoted $\CDT(F, \rho)$, is constructed as follows. 
		\begin{enumerate}
			\item Let $F_1 = F \upharpoonright \rho$ and $C_{1,1} \lor \cdots \lor C_{1,k_1}$ be the set of clauses which have not been set to $1$ or $0$ by $\rho$. 
			\item Construct a perfect binary tree where each level of the tree is labelled by a in $C_{1,1}$ in the predetermined order e.g. if $C_{1,1} = x_ix_j$ in $F_1$ and $i < j$, then set the root to $x_i$ and label its two children $x_j$. 
			\item Let $p$ be a path in our partially constructed tree. Denote by $\rho_{p}$ a which sets each variable in $C_{1,1}$ according to the path $p$. Let $\ell$ be one of the $2^{|C_{1,1}|}$ leafs in our partially constructed tree. If $p_{\ell}$ is the root-to-$\ell$ path, append $\CDT(F', \rho_{p})$ to $\ell$. 
		\end{enumerate}
		See the following example. 
	\end{definition}
	
	\begin{example}
		\label{ex:canonicaldecisiontree}
		\textbf{(Constructing a canonical decision tree).} Let our $k$-DNF be
		\[F = \bar{x}_1x_3x_5 \lor x_1x_2\bar{x}_3 \lor x_2\bar{x}_4x_5 \lor x_3x_4\bar{x}_6 \lor x_1\bar{x}_4\bar{x}_7\]
		and our restriction $\rho = \{x_1 \mapsto 1, x_4 \mapsto 0\}$. Apply the restriction $\rho$ to $F$, to obtain the following
		\[F \upharpoonright \rho = \textcolor{red}{0} \lor x_2\bar{x}_3 \lor x_2x_5 \lor \textcolor{red}{0} \lor \bar{x}_4\bar{x}_7.\]
		Look at the first un-falsified clause $x_2\bar{x}_3$ and build a tree with these variables on the first and second levels. For every root-to-leaf path, construct a restriction by setting the variables $x_2$ and $\bar{x}_3$ according to the edge labels. Continue down the tree until all the variables have been added. The first two restrictions are shown in Figure \ref{fig:dtcanonicalform}. 
		
		\begin{figure}[ht]
			\centering
			\begin{tikzpicture}[->, >=stealth', shorten >= 1pt, auto, node distance=6em, baseline=(current bounding box.center)]
			\node[state]	(x2) 					{$x_2$};
			\node[state]	(x30)[below left of=x2]	{$x_3$};
			\node[state]	(x31)[below right of=x2]{$x_3$};
			\node[state]	(x511)[below right of=x31]	{$x_5$};
			\node[state]	(x510)[left of=x511] 	{$x_5$};
			\node[state]	(x400)[below left of=x30]	{$x_4$};
			\node[state]	(x401)[right of=x400]{$x_4$};
			\node			(r)[below = 0pt of x2] {$F \upharpoonright \rho$};
			\node			(r00)[below = 0pt of x510] {$F' \upharpoonright \rho_{1,0}$};
			\node			(r01)[below = 0pt of x511] {$F' \upharpoonright \rho_{1,1}$};
			\node			(r10)[below = 0pt of x400] {$F' \upharpoonright \rho_{0,0}$};
			\node			(r11)[below = 0pt of x401] {$F' \upharpoonright \rho_{0,1}$};
			\path 	(x2)	edge	node{$1$} 			(x31)
							edge	node[swap]{$0$}		(x30)
					(x30)	edge	node[swap]{$0$}		(x400)
							edge	node{$1$}			(x401)
					(x31)	edge	node[swap]{$0$}		(x510)
							edge	node{$1$}			(x511);
			\end{tikzpicture}
			\caption{Given the $3$-DNF $F$ and the restriction $\rho$ construct the canonical DT for $F \upharpoonright \rho$. Let $F' = F \upharpoonright \rho$ and $\rho_{a,b} = \{x_2\mapsto a, x_3\mapsto b\}$.}
			\label{fig:dtcanonicalform}
		\end{figure}
	\end{example}
	
	Observe that every depth $d$ decision tree (DT) is equivalent to a $d$-DNF ($d$-CNF) by tracing every root-to-leaf path in the tree which end in a one (resp. end in a zero then apply DeMorgan's rule). It follows that an $\lor$ of depth $d$ $DT$s is a $d$-DNF. Further, if $f$ is equivalent to both a $k$-DNF and $l$-CNF, then $\DTdepth(f)\leq k \cdot l$. Do you see why this is?
	
	Let $f$ be equivalent to the $k$-DNF $F = A_1 \lor \cdots \lor A_s$ and $\bar{f}$ be equivalent to the $l$-DNF $\bar{F} = B_1 \lor \cdots \lor B_t$. Notice that every pair $(A_i, B_j)$ must share a common variable of opposite sign or else there will be a setting of the variable which simultaneously satisfies $F$ and $\bar{F}$. The proof is by induction on the number of variables. When there is only one variable the claim is true. Suppose $f$ has $n$ variables. Build a decision tree of width $A_1$ which considers all variables in the clause $A_1$. Consider a restriction $\rho$ corresponding to a root-to-leaf path in the DT we have just created. The DNF-width of $F \upharpoonright \rho$ is at most $k$. The DNF-width of $\bar{F}$ is \emph{less than} $l$ since at least one variable of each clause was knocked out. Further, by the induction hypothesis, we have
	\[\DTdepth(f \upharpoonright \rho) \leq \DNFwidth(F \upharpoonright \rho) \cdot \DNFwidth(\bar{F} \upharpoonright \rho).\]
	By appending the restricted decision tree to every leaf of the depth $k$ DT that we created for the variables in $A_1$, we have that
	\[\DTdepth(f) \leq k + \DNFwidth(F \upharpoonright \rho) \cdot \DNFwidth(\bar{F} \upharpoonright \rho) \leq k + k(l-1) = kl.\]
	
	Before looking at the proof of the Switching Lemma, let us consider a simple warm-up pertaining to the depth of a decision tree hit by a random restriction. 
	\begin{theorem}
		If $\DTdepth(f) = k$, then 
		\[\Pr\left[\DTdepth(f \upharpoonright R_p) \geq t\right] \leq (2p)^t \binom{k}{t}.\]
	\end{theorem}
	\begin{proof}
		By induction on $k$. In the base case where $k = 0$, $f$ is a constant function. Suppose that $\DTdepth(f) = d$. Let $T$ be a DT of $f$ of depth $d$ with root labelled $x_1$. Further let $T_0$ and $T_1$ be the left and right sub-trees of $T$ respectively. Either $x_1$ gets restricted or not. If $x_1$ gets restricted, then we can apply the induction hypothesis to one of $T_0$ or $T_1$, w.l.o.g suppose $T_0$. This happens with probability $1 - p$. If $x_1$ is unrestricted then it suffices to find the probability that a restriction of $T_0$ or $T_1$ has depth $\geq t-1$; w.l.o.g. suppose $T_0$ is larger. This happens with probability $p$. Thus 
		\begin{align*}
			\begin{split}
				\Pr\left[\DTdepth(T \upharpoonright R_p) \geq t\right] &= (1-p)\Pr\left[\DTdepth(T_0 \upharpoonright R_p) \geq t\right]+\\ & \qquad\quad p\Pr\left[\left(\DTdepth(T_0 \upharpoonright R_p) \geq t-1\right) \lor \left(\DTdepth(T_1 \upharpoonright R_p) \geq t-1\right)\right]
			\end{split}
			\\
			&\leq (1-p)\Pr\left[\DTdepth(T_0 \upharpoonright R_p)\right] + 2p\Pr\left[\DTdepth(T_0 \upharpoonright R_p) \geq t-1\right]\\
			&\leq (1-p)\cdot(2p)^{t}\binom{d-1}{t} + 2p\cdot(2p)^{t-1}\binom{d-1}{t-1}\\
			&\leq (2p)^{t}\left(\binom{d-1}{t} + \binom{d-1}{t-1}\right)\\
			&= (2p)^{t}\binom{d}{t}
		\end{align*}
		where the second inequality follows from a union bound.
	\end{proof}
	
	\begin{theorem}
		\label{thm:haastadswitchinglemma}
		\textbf{(H\aa stad's Switching Lemma).} If $F$ is a $k$-DNF (or $k$-CNF) then
		\[\Pr[\DTdepth(F \upharpoonright R_p) \geq t] \leq (5pk)^t.\]
	\end{theorem}
	\begin{proof}
		This proof due to Razborov as explained in the exposition by Beame. Let\footnote{In many proofs the set $B_t$ is actually called $BAD_t$. This is illustrative of how you want to think about $B_t$. It is the set of all \emph{bad} restrictions which cannot be converted to a depth $t$ decision tree.} 
		\[B_t \coloneqq \{\rho: \DTdepth(\CDT(F, \rho)) = t\}.\]
		We will show that the probability a random restriction $R_p \in B_t$ is bounded by $O(pk)^t$. Let $n$ be the number of variables and $m$ the number of clauses in $F$.
		
		For each $\rho$ in $BAD_t$ we want to construct a pair $(\rho^*, \textsc{code})$. $\rho^*$ is a fixed restriction setting a further $t$ variables. $\textsc{code}$ is some information needed to uniquely identify $\rho$. Note that $\textsc{code} = \left(\vv{s}, \vv{\pi}, \vv{n}\right)$ where $\vv{s} \in [k]^t$ indicates the location of variables within a clause, $\vv{\pi} \in \{0,1\}^t$ is the appropriate setting of the variable, and $\vv{n} \in \{0,1\}^t$ indicates if we move onto the \textbf{n}ext clause. Then, with the pair $(\rho^*, \textsc{code})$ and $F$ in-hand, we can reconstruct $\rho$.
		
		In the encoding step we are given the $k$-DNF $F$ and the restriction $\rho \in B_t$. We will build $\rho^* = \rho\sigma_1\cdots\sigma_j$ and $\textsc{code} = \gamma_1\cdots\gamma_j$ incrementally.
		\begin{enumerate}
			\item Let $T = T_1 = \CDT(F, \rho)$ and let $C_{1}, ..., C_{j_1}$ be the set of clauses with free variables. Let $p$ be the lexicographically first path of length $t$ in $T$. Such a longest path exists since $\rho \in B_t$ and $f$ is not the constant function.
			\item Let $\sigma_1$ be the unique setting of the free variables in $C_{1}$ such that $C_{1} \upharpoonright \sigma_1 \equiv 1$. Let $\pi_{1}$ record the actual setting of the free variables in $C_{1}$ along path $p$ as well as their location and number. In particular, $\gamma_1 = (\vv{s}_1, \vv{\pi}_1, \vv{n}_1)$ which are the indices, settings, and \emph{is-last-in-clause} properties of the free variables in $C_{1}$.
			
			Consider this first encoding step on Example \ref{ex:canonicaldecisiontree}. Suppose the long path $p$ aims to set $x_2 = 1$ and $x_3 = 1$. Then $\sigma_1 = \{x_2 \mapsto 1, x_3 \mapsto 0\}$, $\vv{s}_1 = [2,3]$, $\vv{\pi}_1 = \{x_2 \mapsto 1, x_3 \mapsto 1\}$, and $\vv{n} = [0,1]$.
			\item Proceed down $p$ past all the free variables in $C_{1}$. Let $T_1$ be the remaining subtree. Repeat the process until all $t$ variable on $p$ have been considered.    
		\end{enumerate}
		
		In the decoding step we are given $F$ and the pair $(\rho^*, \textsc{code})$ where $\rho^* = \rho\sigma_1\cdots\sigma_j$ and $\textsc{code} = \gamma_1\cdots\gamma_j$ and want to reconstruct $\rho$.
		\begin{enumerate}
			\item Evaluate $F \upharpoonright \rho^*$. Let $C_1'$ be the first clause set equal to $1$. Observe that $C_1' = C_{1}$ by construction. Use $\vv{n}_1$ to determine how many variables were set during the encoding step and use $\vv{\pi}_1$ and $\vv{v}_1$ to reset these values in $\rho^*$ so they match the first $|C_1|$ variables on the longest path $p$. Let $\rho^*_1 = \rho\pi_1\sigma_2\cdots\sigma_j$.
			\item Repeat with $\rho^*_1$ in the place of $\rho^*$ until we have used up all $t$ variables.
		\end{enumerate}
		All variables consider in the above procedure were originally stars in $\rho$. 
		
		Suppose that $\rho$ has $s$ stars. Then the set $\mathcal{R}_s$ of all restrictions with $s$ stars is of size $\binom{n}{s}2^{n-s}$. Similarly, the set of all restrictions with $s - t$ stars is of size $\binom{n}{s-t}2^{n-s+t}$. Then, since the codes come from a domain of size $(4\log k)^t$,
		\[\Pr[\DTdepth(F \upharpoonright R_p) \geq t] = \frac{|B_t|}{|\mathcal{R}_s|} \leq  \frac{|\mathcal{R}_{s-t}|(4\log k)^t}{|\mathcal{R}_s|} = \frac{\binom{n}{s-t}2^{n-s+t}(4\log k)^t}{\binom{n}{s}2^{n-s}} \leq (8pk)^t.\]
		Considering all bad restrictions with paths of length at least $t$ increases the above probability by a constant. The eight in the upper bound can be improved to five with a more careful analysis.
	\end{proof}
	
	It is often natural to choose $p = \frac{1}{10k}$ so that the bound is inverse exponential in $t$. 
	\begin{corollary}
		If $F$ is $k$-DNF, then 
		\[\Pr\left[F \upharpoonright R_p \mbox{ is not a $t$-CNF }\right] \leq (5pk)^t.\]
	\end{corollary}
	This corollary follows from Theorem \ref{thm:haastadswitchinglemma} since every depth-$t$ DT is a $t$-CNF ($t$-DNF).   
	
	\subsection{(LB) Parity Circuit-size}
	\begin{theorem}
		\label{thm:dtdepthunderrandomrestriction}
		Let $C$ be an $\class{AC}^0$ circuit of depth $d + 1$ and size $S$. Let $p = 10^{-d - 1}(2\log S)^{-d}$.
		\[\Pr[\DTdepth(C \upharpoonright R_p) \geq \ell] \leq \frac{1}{2^\ell} + \frac{1}{S}.\] 
	\end{theorem}
	\begin{proof}
		The key idea\footnote{Actually I am still missing something here... but I would be hard-pressed to say what it is...} is to repeatedly apply Theorem \ref{thm:haastadswitchinglemma} being careful to choose appropriate values of $k$, $t$ and $p$. Consider the bottom level of $C$ (closest to the literals) and w.l.o.g assume that this is a conjunction ($\land$-gate). Since the fan-in consists of literals, you can think of them as width one clauses. Apply the theorem with $k = 1$, $t = 2\log S$, $p_1 = \frac{1}{10}$. According to the switching lemma, we fail\footnote{That is to say: this node cannot be converted to a short DT under $R_p$.} at each $\land$-node with probability at most $2^{-2\log S} = S^{-2}$. Taking a union bound over at most $S$ $\land$-gates, we fail at this level with probability at most $S^{-1}$. Let $E_1$ be the event that we successfully completed the switch from $1$-CNF to $2\log S$-DNF. Suppose $E_1$ occurs. Then we can collapse this layer of $\lor$-gates with the $\lor$-gates of the level above. 
		
		Apply the switching lemma a further $d-1$ iterations. At step $i$ set $k = 2\log S$, $t = 2\log S$, and $p_{i} = \frac{p_{i-1}}{20\log S}$ each time conditioning on the success of the previous iterations. Observe that at the probability of failure at any iteration is at most $S^{-1}$.
		
		Consider our final iteration. We assumed that $d$ iterations have succeeded so we have w.l.o.g. a $\lor$-gate sitting on top of depth $2\log S$ decisions (i.e. width $2\log S$-CNF clauses). For the final application of the switching lemma we will set $k = 2\log S$, $t = \ell$, and $p_{d+1} = \frac{p_d}{20\log S}$. Thus this iteration fail with probability at most $2^{-\ell}$.
	\end{proof}
	
	\begin{corollary}
		$\circuitsize_{d+1}(XOR_n) = 2^{\Omega(n^{1/d})}$.
	\end{corollary}
	By Theorem \ref{thm:dtdepthunderrandomrestriction}, the probability that the restriction has depth $\geq 1$ is $< 1$. Thus there is some restriction which sets $C$ to a constant. Since $p = 10^{-d-1}(2\log S)^{-d}$ and $S = 2^{\Omega(n^{1/d})}$, there is a good likely-hood that some variables are un-restricted.
	
	\subsection{Switching Lemma for Formulas}
	The lower bound\footnote{A result of Ben's!} of $\Omega(dn^{1/d})$ matches the existing upper bound. 
	
\section{Bounded Depth: \texorpdfstring{$\class{AC}^0[p]$}{AC0[p]}}
	Goal: want an upper bound on the approximate degree of $AND$, $OR$, and $MOD_p$ functions.
	\begin{definition}
		Let $\vv{x} = (x_1,..., x_n)$ then $MOD_p(\vv{x}) = 1$ if and only if $\sum_{i = 1}^{n} x_i \equiv 0 \mod p$. 
	
		Let $A \in \field_p[x_1, ..., x_n]$ be a \textbf{random polynomial} over $\field_p$. The \textbf{degree} of $A$ is the maximum degree of a polynomial in the support for $A$ (think of $A$ as a random variable in a distribution over some polynomials). 
		
		Let $f: \{0,1\}^n \rightarrow \{0,1\}$. $A$ is an $\epsilon$-approximating for $f$ if
		\[\Pr_A[A(x) \neq f(x)] \leq \epsilon \]
		for every $x \in \{0,1\}^n$. 
	\end{definition}
	
	\begin{lemma}
		If $A$ is an $\epsilon$-approximating random polynomial for $f$, then $\exists \alpha \in \support(A)$ such that
		\[\Pr_{\vv{x} \in \{0,1\}^{n}}[\alpha(x) \neq f(x)] \leq \epsilon\]
	\end{lemma}
	\begin{proof}
		Markov inequality then probabilistic method. 
	\end{proof}
	
	\begin{definition}
		The \textbf{$\epsilon$-approximate degree of $f$}, denoted $\deg_{\epsilon}(f)$, is the minimum degree of an $\epsilon$-approximating random polynomial of $f$. 
		
		Note that this value is invariant under negation of the inputs and outputs (please understand why).
	\end{definition}
		
	\begin{lemma}
		\label{lem:epsapproxdegofcompfunction}
		Suppose $f(x) = g(h_1(x), ..., h_m(x))$. Then for any $\delta, \epsilon_1, ..., \epsilon_m$, 
		\[\deg_{\delta + \epsilon_1 + \cdots + \epsilon_m}(f) \leq \deg_{\delta}(g)\max_{i}\deg_{\epsilon_i}(h_i)\]
	\end{lemma}
	\begin{proof}
		Just think through this carefully. I think you can get it.
	\end{proof}
	
	Assume that these are all $n$-ary Boolean functions.
	\begin{enumerate}
		\item[$MOD_p$:]
		\begin{lemma}
			For any $\epsilon > 0$ and $n$,
			\[\deg_{\epsilon}(MOD_p) \leq \deg_{0}(MOD_p) \leq p-1\]
		\end{lemma}
		\begin{proof}
			Give me a degree $p-1$ polynomial which exactly computes $MOD_p$ (for an $n$ bit input vector). \emph{Hint: Fermat's Little Theorem}.
		\end{proof}
		\item[$OR$:] Now we need some random polynomials
		\begin{lemma}
			$\deg_{\epsilon}(OR) \leq p\left(\log_p\left(\frac{1}{\epsilon}\right) + 1\right)$. One should note that this upper bound is independent of $n$.
		\end{lemma}
		\begin{proof}
			So you want to calculate the probability that $OR(x_1, ..., x_n) \neq (\lambda_1x_1 + \cdots + \lambda_nx_n)$ when $\vv{x} = \vv{0}$ and when $\vv{x} \neq \vv{0}$ (\emph{Hint: $0$ for the former case and $\frac{1}{p}$ in the latter.}). This is not good enough so just boost
		\end{proof}
		\item[$AND$:] This is identical to the argument for the $OR$ gate. 
	\end{enumerate}
	
\section{Project Idea}
	Shrinkage exponent of monotone formulas. 
\end{document}