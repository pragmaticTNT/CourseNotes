%
% This is a borrowed LaTeX template file for lecture notes for CS267,
% Applications of Parallel Computing, UCBerkeley EECS Department.
%

\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,amssymb, graphicx}
\usepackage{algpseudocode}
\usepackage{color}
\usepackage{comment}
\usepackage{mathtools}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf Introduction to Geometric Discrepancy
	\hfill Fall 2017} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Section #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribe: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
   \vspace*{4mm}
}
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{problem}[theorem]{Problem}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\vol{\mathsf{vol}}
\newcommand\R{\mathcal{R}}
\newcommand\disc{\mathsf{disc}}
\newcommand\A{\mathcal{A}}
\newcommand\SSet{\mathcal{S}}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\anglebrac{\langle}{\rangle}

\begin{document}
\lecture{1}{Introduction}{Jiri Matousek}{Lily Li}

\section{Discrepancy for Rectangles and Uniform Distribution}
Lets begin with the following problem:
\begin{example}
How can we uniformly distribute $n$ points in a $d$-dimensional unit cube $[0,1]^d$. The case where $d = 1$ is quite straight-forward. Simply place point $i$ at position $i/d + 1/(2d)$ --- evenly spaced. Next consider the case where $d = 2$. Now the precise definition of \emph{uniform} is not so clear. On possible criterion is \textbf{with respect to axis-parallel rectangles}. Let $P$ be an $n$-point set in the unit square $[0,1]^2$ and $R$ is an axis-parallel rectangle where $R = [a_1, b_1) \times [a_2, b_2) \subseteq [0,1]^2$. Then $P$ is \textbf{uniformly distributed} if the number of points of $P$ that fall in $R$ is approximately $n \cdot \vol (R)$ where $\vol (R)$ is the area of $R$ (further note that the choice of $P$ is uniformly and independently at random thus $n\cdot \vol (R)$ is just the expected number of points in $R$, there is no adversary picking the worst point set $P$ possible). 

Let $P$ be \textbf{justly distributed} if the deviation (defined below) 
\[\left| n \cdot \vol(R) - |P \cap R| \right|\]
is at most $c$ --- some constant --- for all axis-parallel rectangles $R$. As it turns out justly distribution is impossible for sufficiently large sets.  There will always exist some rectangle $R$ for which the magnitude of irregularity grows to infinite as $n \rightarrow \infty$.
\end{example}

From this example we can extract some useful notation. Let the \textbf{deviation} of $P$ from a uniform distribution on a particular rectangle $R$ be 
\[D(P, R) = n \cdot \vol(R) - |P \cap R|.\]
Let $\R_2$ be the set of all axis-parallel rectangles in the unit square. Then
\[D(P, \R_2) = \sup_{R \in \R_2} |D(P, R)|\]
is the \textbf{discrepancy of $P$} for axis-parallel rectangles. Further, the function
\[D(n, \R_2) = \inf_{P \subset [0,1]^2, |P| = n} D(P, \R_2)\]
quantifies the smallest possible discrepancy of an $n$-point set.

\begin{problem}
Is $D(n, \R_2)$ bounded above by a constant for all $n$? (or is $\lim\sup_{n \rightarrow \infty} D(n, \R_2) = \infty$)
\end{problem}

Remark that your intuitive notion of what is or is not evenly distributed should be kept in check. The triangle lattice arrangement of points looks pretty even, but has pretty bad discrepancy (about $\sqrt{n}$). It is possible that good discrepancy results from a distribution which puts two points very close together.  

\subsection{Uniform Distribution of Infinite Sequences}
Previously we thought about a finite number of point in unit cubes of increasing dimension. Now let us consider what would happen if we have infinitely many points. Again consider the one-dimensional interval $I = [0,1]$ and a infinite sequence $u = (u_1, u_2, ...)$ points in $I$. Uniformity here means that as the points of $u$ are added one by one, they \emph{sweep out} all subintervals of $I$ as evenly as possible. 

\begin{definition}
$u = (u_1, u_2, ...)$ is \textbf{uniformly distributed} in $[0,1]$ if for each subinterval $[a, b) \subset [0,1]$,
\[\lim_{n\rightarrow \infty} \left( \frac{\left|\{u_1, ..., u_n\} \cap [a, b)\right|}{n} \right) = b-a\]

Note: for any Riemann-integrable function $f: [0,1] \rightarrow \R$, 
\[\lim_{n\rightarrow \infty} \left( \frac{\sum_{i=1}^{n}f(u_i)}{n} \right) = \int_{0}^{1} f(x) dx\]
(do you still remember what a Riemann-integrable function is? Actually it is just normal integrable function, they just need to be continuous everywhere).

Further note: the trigonomiatric polynoimals of the form
\[f(x) = \sum_{k=0}^{n} \left( a_k \sin(2\pi k x) + b_k \cos (2\pi kx) \right)\]
with $a_0, ..., a_n$ and $b_0, ..., b_n$ real or complex are also sufficient for testing the uniform distribution. 

\begin{theorem}
(Weyl's criterion) A sequence $u = (u_1, u_2, ...)$ is uniformly distributed in $[0,1]$ if and only if for all integers $k \neq 0$,
\[\lim_{n\rightarrow\infty} \left( \frac{\sum_{j=1}^{n} e^{2\pi i ku_j}}{n} \right) = \int_{0}^{1} e^{2\pi iku_j} dx = 0\]
\end{theorem}

An application of the above criterion is the following:
\begin{theorem}
For each irrational number $\alpha$, the sequence $u = (u_1, u_2, ...)$ given by $u_n = \{\alpha n\}$ is uniformly distributed in $[0,1]$, where $\{x\}$ is the fractional part of $x$. 
\end{theorem}
\begin{proof}
Lets be honest, we are not really sure what is going on here... well, no actually it is not that bad. If you take Weyl's criterion as a given --- which is a pretty reasonable thing to do really --- then this proof is just a simple application of the criterion. In fact is seems tailor made for this problem since the notation wipes out the annoying ``fractional part" function.
\end{proof}
\end{definition}

\subsection{Dynamic Setting}
Not all uniformly distributed sequences are made equal. Discrepancy was originally introduced to measure the amount of non-uniformity in an infinite sequence.

\begin{definition}
\textbf{Discrepancy of an infinite sequence $u$} in $[0,1]$ is defined as 
\[\Delta(u, n) = \sup_{0\leq a\leq b\leq 1} \left| n(b-a) - \left| \{u_1, ..., u_n\} \cap [a, b) \right| \right| \]
\end{definition}

In-fact, as we will soon see, there is a very tight connection between the two-dimensional and infinite one-dimensional problems.

\section{Combinatorial Discrepancy}
Let $X$ be an $n$-element set and $\SSet$ a system of subsets of $X$ called a \textbf{set system}. Our goal is to color $X$ red or blue so that every set of $\SSet$ has roughly the same number of red and blue points.

\begin{definition}
A \textbf{coloring} of $X$ is a mapping $\chi: X \rightarrow \{-1, +1\}$. 

The \textbf{discrepancy} of the set system $\SSet$ is the maximum deviation from an even split over all sets of $\SSet$. Formally the \textbf{discrepancy} of $\SSet$, denoted $\disc(\SSet)$ is the minimum overall colorings $\chi$ of
\[\disc(\chi, S) = \max_{S \in \SSet} |\chi(S)|.\]
Note that $\chi(S) = \sum_{x \in S} \chi(x)$ so taking the norm makes sense.  
\end{definition}

This type of discrepancy is often denoted \emph{combinatorial discrepancy} to contrast with the type defined in the previous section known as \emph{Lebesgue-measure discrepancy}.

Examples of when the geometry sneaks into the problem involves the following: given an $n$-point set $P$, color each $p \in P$ red or blue to minimize the maximum the difference between the number of points of each color in a halfplane $h$ over all halfplanes.

\begin{definition}
If $(X, \A)$ is a set system ($X$ might be infinite) and $Y \subset X$ is a set. Then the \textbf{set system induced by $\A$ on $Y$} (sometimes called the \textbf{trace} of $\A$ on $Y$) is the set system:
\[\A|_{Y} = \{A \cap Y: A \in \A\}\]
Typically $\A \subset \R^d$ and the combinatorial discrepancies set system $\A|_{P}$ where $P \subseteq \R^d$ is a finite set we are considering. Often, for notational simplicity $\disc(P, \A) = \disc(\A|_{P})$. Explicitly $\disc(P, \A)$ is the minimum over all colorings $\chi: P \rightarrow \{-1, 1\}$ of 
\[\disc(\chi, P, \A) = \max_{A \in \A} |\chi(P \cap A)|.\]
The \textbf{discrepancy function of $\A$} is
\[\disc(n, \A) = \max_{|P| = n} \disc(P, \A)\] 
\end{definition}

We will show later that upper bounds for combinatorial discrepancy for some class $\A$ imply upper bounds for the Lebesgue-measure discrepancy for $\A$ (the reverse does not hold). 

\subsection{$\epsilon$-approximations}
From probability theory, the notion of an $\epsilon$-approximation is a generalization of both types of discrepancy we have discussed so far. Let $X$ be some finite or infinite ground set, $\mu$ is a measure on $X$ with $\mu (X) < \infty$, and $\SSet$ is a system of $\mu$-measurable subsets of $X$. Let $\epsilon \in [0,1]$ be a real number. Then $Y \subseteq X$ is an $\epsilon$-approximation for the set system $(X, \SSet)$ with respect to measure $\mu$ if for all $S \in \SSet$,
\[\left| \frac{|Y \cap S|}{|Y|} - \frac{\mu(S)}{\mu(X)} \right| \leq \epsilon\]
\emph{Notice that this notion is indeed quite similar to that of discrepancy, particularly combinatorial discrepancy. Here we want the ratio of points in $Y$ which are also in $S$ to be proportional to the ratio of $S$ in the entire population.} General we let $\mu$ be the counting measure i.e. $\mu(S) = |S|$.

According to the book, the connection to the Lebesgue-measure discrepancy is the obvious one:
\begin{proposition}
If $\A$ is a class of Lebesgue-measurable sets in $\R^d$ and $P \subset [0,1]^d$ is an $n$-point set, then $D(P, \A) \leq \epsilon n$ if and only if $P$ is an $\epsilon$-approximation for $(\R^d, \A)$ with respect to the measure $\vol_{\square} = \vol(\A \cap [0,1]^d)$.
\end{proposition}

Here we will go into a little foray into measure theory: recall
\begin{definition}

\end{definition} 
Let $X$ be a set and $\sigma$ a $\sigma$-algebra --- collection of subsets of $X$ including the empty subset which is closed under complement, countable unions and countable intersections --- over $X$. A function $\mu: \sigma \rightarrow R$ is a \textbf{measure} if it has the following properties:
\begin{enumerate}
\item Non-negativity: for all $S \in \Sigma: \mu (S) \geq 0$
\item Null empty set: $\mu (\emptyset) = 0$ (and for all non-empty sets, $S, \mu(S) \neq 0$?)
\item Countable additivity (aka $\sigma$-additivity): for all countable collections $\{S_i\}_{i=1}^{\infty}$ of pairwise disjoint sets in $\Sigma$: $\mu (\cup_{k=1}^{\infty} S_k) = \sum_{k=1}^{\infty} \mu (E_k)$.
\end{enumerate}

The pair $(X, \Sigma)$ is a \textbf{measurable space} and the members of $\Sigma$ are measurable sets. Further the triple $(X, \Sigma, \mu)$ is called a \textbf{measure space} (think: it is a \emph{measurable space} with a measure).

\begin{lemma}
(Combinatorial discrepancy and $\epsilon$-approximations) Let $\SSet$ be a system  of subsets of a $2n$-point set $X$.
	\begin{enumerate}
	\item If $Y \subset X$ is an $n$-point set that is an $\epsilon$-approximation for $(X, \SSet)$ then $\disc(\SSet) \leq 2\epsilon n$
	\item If $\SSet$ is such that $X \in \SSet$ and $\disc(\SSet) \leq \epsilon n$ then there exists an $n$-point set $Y \subset X$ such that $Y$ is an $\epsilon$-approximation for $(X, \SSet)$.  
	\end{enumerate}
\end{lemma}
\begin{proof}
To prove part (1) we take the $n$-point set $Y \subset X$ and provide a coloring $\chi$ such that $\disc(\chi, \SSet) \leq 2\epsilon n$ (notice since $\disc (\SSet)$ is taken to be the minimum over all colorings, it suffices to find one coloring which is upper bounded by $2\epsilon n$). The coloring is the obvious one: $\chi(p) = 1$ if $p \in Y$ and $\chi(p) = -1$ otherwise. Now we can calculate $\chi(S)$ for all $S \in \SSet$
\[\chi(S) = \sum_{x \in S} \chi(x) = |Y \cap S| - (|S| - |Y \cap S|) = 2|Y\cap S| - |S|.\]
Since $Y$ is an $\epsilon$-approximation for $(X, \SSet)$:
\[\left| \frac{|Y\cap S|}{|Y|} - \frac{|S|}{|X|} \right| = \frac{1}{2n}\left| 2|Y \cap S| - |S| \right| \leq \epsilon. \]
Notice that we basically get what we want if we move the $2n$ to the other side since $| \chi(S) | \leq 2\epsilon n$.

Now for part (2) we need the opposite direction to be true. So given that there exists a coloring $\chi$ such that $|\chi(S)| \leq \epsilon n$ for all $S \in \SSet$ (including $X$) we need to construct a $n$-point set $Y$ which is an $\epsilon$-approximation for $(X, \SSet)$. The subset to take should be inspired from the first part namely we will take $Y' = \arg\max \{\chi^{-1}(1), \chi^{-1}(-1)\}$ (we need to take the larger set because the subset we actually want has size $n$). Next we will use the fact that $X \in \SSet$ to get a bound on $Y'$:
\[|\chi(X)| = |2|Y' \cap X| - |X|| = 2|Y'| - 2n \leq \epsilon n\]
thus $|Y'| - n \leq \epsilon/2$. 
Now we can show that a subset $Y \subset Y'$ of size $n$ is an $\epsilon$-approximation of $(X, \SSet)$ since for any $S \in \SSet$
\begin{align*}
\left| \frac{|Y \cap S|}{|Y|} - \frac{|S|}{|X|} \right| &= \frac{1}{2n}\left| 2|Y \cap S| - |S| \right| \\
&= \frac{1}{2n}\left| 2|Y \cap S| - |S| + 2|Y' \cap S| - 2|Y' \cap S| \right| \\
&\leq \frac{1}{n}\left| |Y \cap S| - |Y' \cap S| \right| + \frac{1}{2n}\left| 2|Y' \cap S| - |S| \right| &\mbox{ By triangle inequality}\\
&\leq \frac{n\epsilon}{2n} + \frac{\chi(S)}{2n} \\
&\leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
\end{align*}   
\end{proof}

\begin{proposition}
If $Y_0$ is an $\epsilon$-approximation for $(X, \SSet)$ with respect to measure $\mu$ and $Y_1$ is a $\delta$-approximation for set system $(Y_0, \SSet|_{Y_0})$. Then $Y_1$ is an $(\epsilon + \delta)$-approximation for $(X, \SSet)$ with respect to $\mu$. 
\end{proposition}

\begin{lemma}
(Transference Lemma) Let $\A$ be a class of Lebesgue measurable sets in $\R^d$ containing a set $A_0$ with $[0,  1]^d \subseteq A_0$. Suppose that $D(n, \A) = o(n)$ as $n \rightarrow \infty$, and that $\disc(n, \A) \leq f(n)$ for all $n$, where $f(n)$ is a function satisfying $f(2n) \leq (2-\delta)f(n)$ for all $n$ and some fixed $\delta > 0$. Then $D(n, \A) = O(f(n))$.

On the other hand, if $D(n, \A) = o(n)$ and $D(n, \A) \geq f(n)$ for all $n$, with a class $\A$ and a function $f(n)$ as above, then $\disc(n, \A) \geq cf(n)$ holds for infinitely many $n$ with a suitable constant $c = c(\delta) > 0$.
\end{lemma}
\begin{proof}
We begin by taking note of some suggestive features of the lemma definition: first note that $A_0$ is not used anywhere in the proof (why it is necessary then?) further, why does the function $f$ need to be sub-linear? With these thoughts in mind we are going prove the first part of the lemma. Remember we want to show that $D(n, \A) = O(\disc(n, \A))$. Thus it helps to pick an $n$ and work from there. 

What can we do with some random $n$? Lets define a constant $\epsilon = f(n)/n$. Since $\epsilon < 1$ and $D(n, \A) = o(n)$ we can find some $k$ such that
\[\frac{D(2^k n, \A)}{2^kn} \leq \epsilon.\]
The reason for doing this is so that we can use the second part of lemma 1.11 to halve the number of points in each iteration. We get the hint for this from the $A_0$ since the lemma requires that the ground set is a member of the set system (with $[0,1]^d \subset A_0$ this is no problem). The above inequality really says that there exists some point set $P_0$ of size $2^kn$ such that $P_0$ is an $\epsilon$ approximation of $(\R^d, \A)$ with measure $\vol_{\square}$. Reduce the size of the point set by halving as described above). Generally at step $i+1$ we have the set system $(P_i, \A|_{P_i})$ and we take a coloring $\chi_i$ with discrepancy at most $f(|P_i|)|$. Use lemma 1.11 (ii) to find a subset $P_{i+1} \subset P_i$ of $2^{k-1-i}n$ point and which is an $\epsilon_{i}$ approximation for $(P_i, \A|_{P_i})$ where $\epsilon_{i} = f(2^kn)/2^kn$. After $k$ iterations we get a set $P_k$ of size $n$ which is a $\eta$-approximation of the original set system $(\R^d, \A)$ where 
\[\eta = \epsilon + \sum_{k-1}^{i=0} \epsilon_i = \frac{f(n)}{n} + \sum^{k}_{i=1} \frac{f(2^in)}{2^in} \leq \frac{f(n)}{n}\left( 1 + \sum_{i=j}^{\infty} \left(\frac{2-\delta}{2}\right)^j \right) \in O\left(\frac{f(n)}{n}\right)\]  
by proposition 1.12. Thus $D(n, \A) \leq O(\disc(n, \A))$ by an application of proposition 1.9.

Now we will show the second part of the lemma in the following
\end{proof}

\end{document}
