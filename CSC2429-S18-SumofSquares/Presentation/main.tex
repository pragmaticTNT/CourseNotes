%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Article Notes
% LaTeX Template
% Version 1.0 (1/10/15)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@latextemplates.com)
% Christopher Eliot (christopher.eliot@hofstra.edu)
% Anthony Dardis (anthony.dardis@hofstra.edu)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
10pt, % Default font size is 10pt, can alternatively be 11pt or 12pt
letterpaper, % Alternatively letterpaper for US letter
onecolumn, % Alternatively twocolumn
% Alternatively landscape
]{article}

\input{structure.tex} % Input the file specifying the document layout and structure

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\articletitle{Goemans and Williamson Algorithm for $\MAXCUT$ and Gronthendieck's Inequality} % The title of the article
\articlecitation{\cite{goemans1995improved}} % The BibTeX citation key from your bibliography

\datenotesstarted{January 29, 2018} % The date when these notes were first made
\docdate{\datenotesstarted; rev. \today} % The date when the notes were lasted updated (automatically the current date)

\docauthor{Kevan Hollbach, Lily Li} % Your name

%----------------------------------------------------------------------------------------

\begin{document}

\pagestyle{myheadings} % Use custom headers
\markright{\doctitle} % Place the article information into the header

%----------------------------------------------------------------------------------------
%	PRINT ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\thispagestyle{plain} % Plain formatting on the first page

\printtitle % Print the title

%----------------------------------------------------------------------------------------
%	ARTICLE NOTES
%----------------------------------------------------------------------------------------
\section{Semi-Definite Programming Review}

\section{Goemans Williamson Algorithm (Standard SDP)}
Let $G = (V,E)$ be a weighted undirected graph with $|V| = n$ and where each edge $(i,j)$ has weight $w_{ij} \geq 0$. The goal of the $\MAXCUT$ problem is to find a partition $(S, V-S)$ which maximizes the sum of the edge weights of edges crossing the partition. 

We can formulate the problem as the following quadratic program:
\begin{align*}
\mbox{Maximize: }\qquad & \sum_{(i,j)\in E}\frac{w_{ij}(1-x_ix_j)}{2}\\
\mbox{Subject to: }\qquad & x_i\in \{-1, +1\}, \mbox{ for }i \in [n]
\end{align*}
where $x_i$ is associated with vertex $v_i$ and $x_ix_j = 1$ if and only if $v_i$ and $v_j$ are placed in the same set. Let $OPT$ denote the optimum solution to this quadratic program.

Next we introduce the vector programming relaxation of the above quadratic program:
\begin{align*}
\mbox{Maximize: }\qquad & \sum_{(i,j)\in E}\frac{w_{ij}(1-\mathbf{u_i} \cdot \mathbf{u_j})}{2}\\
\mbox{Subject to: }\qquad & \norm{\mathbf{u}_i}^2 = 1 \mbox{ and } \mathbf{u}_i \in \R^n, \mbox{ for }i \in [n].
\end{align*}
To see that this is indeed a relaxation, take $\mathbf{u}_i = (\underbrace{x_i, 0, ..., 0}_{n})$ for each $i \in [n]$. Observe these $\mathbf{u}_i$'s satisfy the constraints ($ \norm{\mathbf{u}_i}^2 = 1$ and $\mathbf{u}_i \in \R^n$) and $\mathbf{u_i} \cdot \mathbf{u_j} = x_ix_j$. Thus, if $OPT_{VP}$ denotes the optimum solution to this vector program then $OPT_{VP} \geq OPT$.

The above vector program is equivalent to the following semidefinite program:
\begin{align*}
\mbox{Maximize: }\qquad & \sum_{(i,j)\in E}\frac{w_{ij}(1-x_{ij})}{2}\\
\mbox{Subject to: }\qquad & x_{i,i} = 1 \mbox{ for }i \in [n] \mbox{ and } X \succeq 0
\end{align*}
where $X$ has entry $x_ij$ in row $i$ column $j$ (two see that these two forms are equivalent remark that $X \succeq 0$ if and only if $X = U^TU$). Solve the semi-definite program in polynomial time and obtain some optimal solution $X^*$.

Cholesky factorize $X^*$ into $(U^*)^TU^*$ and let the columns of $U^*$, $\mathbf{u}_i^* \in \R^n$, be the solutions to the vector program. We want to round each $\mathbf{u}_i^*$ to some $x_i^* \in \{-1,+1\}$. Then $x_i = x_i^*$ will be a solution to our original quadratic program. Do this in the following randomized manner: pick $\mathbf{r} = (r_1, ..., r_n)$ by drawing each $r_i$ independently from the distribution $\Ncal(0,1)$. Then let
\[x_i^* = \begin{cases}
1 &\mathbf{u}_i^* \cdot \mathbf{r} \geq 0\\
-1 &\mbox{otherwise}
\end{cases}.\]
It is helpful to have the geometric picture in mind: each $\mathbf{u}_i^*$ is a vector which lies on the $n$-dimensional unit sphere. The hyper-plane with normal $\mathbf{r}$ split the sphere in-half. All $\mathbf{u}_i^*$ in the same half of the sphere gets mapped to the same value $c \in \{-1,1\}$ and all the $\mathbf{u}_j^*$ in the other half gets mapped to $-c$. 

To show the constant of approximation we need to consider the probability that an edge $e_{ij}$ gets cut. This is equivalent to the probability that $\mathbf{u}_i$ and $\mathbf{u}_j$ fall in different halves of the sphere cut by the hyper-plane normal to $\mathbf{r}$. Consider the projecting of the normalized vector $\mathbf{r}$ onto the span of $\{\mathbf{u}_{i}, \mathbf{u}_j\}$. See Figure \ref{fig:projection}. 

\begin{figure}[ht]
\centering
	\includegraphics[scale=1]{projection.eps}
\caption{Shaded part denote regions where the normalized $\mathbf{r}$ can lie such that $\mathbf{u}_i\cdot \mathbf{r}$ and $\mathbf{u}_i\cdot \mathbf{r}$ have different sign.}
\label{fig:projection}
\end{figure}

Thus the probability that $\mathbf{r} \cdot \mathbf{u}_i$ and $\mathbf{r} \cdot \mathbf{u}_j$ have different sign is $\frac{\theta}{\pi}$. Since $\theta = \arccos(\mathbf{u}_i\cdot\mathbf{u}_j)$,
\begin{align}
\label{form:probedgeincut}
\Prob[e_{ij} \mbox{ is in the cut}] = \frac{\arccos(\mathbf{u}_i\cdot\mathbf{u}_j)}{\pi}.
\end{align}

We state without proof that
\begin{align}
\label{form:calclowerbound}
\frac{\arccos(x)}{\pi} \geq 0.878 \frac{1-x}{2}
\end{align}
for $x \in [-1,1]$ --- it helps to observe that the constant approximately minimizes $f(x) = \frac{2\arccos(x)}{\pi(1-x)}$ on the interval $[-1,1]$. Thus the expected sum of weights obtained by the algorithm is 
\begin{align*}
\E[W] &= \sum_{(i,j)\in E}w_{ij}\Prob[(i,j)\mbox{ is in the cut}]\\
&= \sum_{(i,j)\in E}w_{ij}\frac{\arccos(\mathbf{u}_i\cdot\mathbf{u}_j)}{\pi} &\mbox{by } \ref{form:probedgeincut}\\
&\geq 0.878 \cdot \left(\sum_{(i,j)\in E}w_{ij}\frac{1 - x_{ij}}{2}\right) &\mbox{by } \ref{form:calclowerbound}\\ 
&= 0.878 \cdot OPT_{VP} \geq 0.878 \cdot OPT
\end{align*}
Since we can find a cut of size $0.878\cdot OPT_{VP}$, $OPT \geq 0.878\cdot OPT_{VP} \geq 0.878\cdot OPT$. 

\section{SOS (Lassarre Hierarchy) Review}

\section{Pseudo-Distribution}
\begin{definition}
Let $f: \{0,1\}^n \rightarrow \R$. The \textbf{formal expectation} of $f$ with respect to another function $\mu$ (not necessarily a probability distribution since it could be negative is 
\[\tilde{\E}_{\mu}f = \sum_{x \in \{0,1\}^n}\mu(x)\cdot f(x)\]

A \textbf{degree-$d$ pseudo-distribution} over $\{0,1\}^n$ is a function $\mu: \{0,1\}^n \rightarrow \R$ such that every formal expectation with respect to $\mu$ satisfies $\tilde{\E}_{\mu} 1 = 1$ and for every polynomial $f$ of degree at most $\frac{d}{2}$
\[\tilde{E}_{\mu}f^2 \geq 0.\]

Let a formal expectation with respect to a pseudo distribution $\mu$ of degree $d$ is a \emph{degree $d$ pseudo-expectation}.
\end{definition}

\section{Vector Representation}

Let $y$ be a feasible solution in $\text{SOS}_t(K)$. We can equivalently represent $y$ as vectors $\{v_I\}$, $|I| \le t$, such that $y_{I \cup J} = \langle v_I, v_J \rangle$ for all $|I|,|J| \le t$. This representation arises from the Cholesky decomposition of the moment matrix, $M_t(y)$, into 

\section{Goemans Williamson Algorithm (SOS $t = 3$)}
We are now ready to present the GW algorithm through the lens of a level three SOS program. Let our graph $G = (V, E)$ be as the above with $|V| = n$ and where each edge $(i,j)$ has weight $w_{ij} \geq 0$. Formulate $\MAXCUT$ as the following integer linear program:
\begin{align*}
\mbox{Maximize: }\qquad & \sum_{(i,j)\in E}w_{ij}z_{ij}\\
\mbox{Subject to: }\qquad & \max(x_i - x_j, x_j - x_i) \leq z_{ij} \leq \min(x_i + x_j, 2 - x_i - x_j) \mbox{ for }(i,j) \in E,\\
&x_i, z_{i,j}\in \{0, 1\} \mbox{ for }i \in [n]\mbox{ and }(i,j) \in E
\end{align*}
where $x_i$ is the indicator variable for a vertex chosen to be in set $S$ of the partition and $z_{ij}$ is the indicator variable for an edge crossing the cut $(S, V-S)$. Observe that $z_{ij} = |x_i - x_j|$. 

Let $K$ be the feasible region of the LP relaxation of the above integer program. For any graph we can set $x_i = \frac{1}{2}$ and $z_{i,j} = 1$ in the LP and obtain a value of $\sum_{(i,j)\in E}w_{ij}$. In particular, since the max cut of a complete graph on $n$-vertices with unit weight edges is at most $\frac{|E|}{2}$ while the output to the relaxation is $|E|$, the integrality gap of this LP scheme is 2.

Suppose instead that we started with $\mathbf{y} \in SOS_3(K)$. Pay particular attention to the elements of $\mathbf{y}$ indexed by the degree one monomials $x_i$ and $z_{i,j}$, and the multinomials $x_ix_j$ for all $(i,j) \in E$. Denoted these as $y_{i}$, $\zeta_{ij}$, and $y_{ij}$ respectively. $\mathbf{y}$ has the form
\[\mathbf{y} = [y_{\emptyset}, y_1, ..., y_{n}, \zeta_{i,j}, ..., y_{i,j}, ...]\]

\begin{lemma}
For any edge $(i,j) \in E$, $\zeta_{i,j} = y_i + y_j - 2y_{i,j}$. 
\end{lemma}
\begin{proof}

\end{proof}

Using the vector representation (add reference), there exists vectors $\mathbf{v}_i$ such that $\anglebrac{\mathbf{v}_i, \mathbf{v}_j} = y_{i,j}$ for all $i,j \in [n]$. Recall however that the angle between any two vectors $\mathbf{v}_i$ and $\mathbf{v}_j$ is between $0$ and $\frac{\pi}{2}$ so applying hyper-plane rounding on the $\mathbf{v}_i$'s would be sub-optimal (we want the vectors to be more spread out so that a random hyper-plane would be more likely to separate a pair of verticies belonging to different sets).

Perform the vector transformation $\mathbf{u}_i = 2\mathbf{v}_i -\mathbf{v}_{\emptyset}$. Observe that $\mathbf{u}_i$ is a unit vector on the sphere centered at the origin. See Figure. In essence this transformation takes $[0,1]^n$ vectors $\mathbf{v}_i$ to $[-1,1]^n$ vectors $\mathbf{u}_i$ before rounding $\mathbf{u}_i$ to $\pm 1$.  


\begin{lemma}
The vectors $\mathbf{u}_i = 2 \mathbf{v}_i - \mathbf{v}_{\emptyset}$ form a solution to the vector program (add reference to the above) where $\zeta_{i,j} = \frac{1 - \mathbf{u}_i\cdot\mathbf{u}_j}{2}$.
\end{lemma}
\begin{proof}
We need to show that $\mathbf{u}_i$ is a unit vector and that $z_{i,j} = \frac{1 - \mathbf{u}_i\cdot\mathbf{u}_j}{2}$ holds.
Observe that
\[\mathbf{u}_i^2 = (2 \mathbf{v}_i - \mathbf{v}_{\emptyset})^2 = 4\mathbf{v}_i^2 - 4\mathbf{v}_i\mathbf{v}_{\emptyset} + \mathbf{v}_{\emptyset}^2 = 1\]
since $\anglebrac{\mathbf{v}_i, \mathbf{v}_i} = y_i = \anglebrac{\mathbf{v}_i, \mathbf{v}_{\emptyset}}$ and $\mathbf{v}_{\emptyset}^2 = y_{\emptyset} = 1$. Further
\[\frac{1 - \mathbf{u}_i\cdot\mathbf{u}_j}{2} = \frac{1 - \left(2 \mathbf{v}_i - \mathbf{v}_{\emptyset}\right)\cdot\left(2 \mathbf{v}_j - \mathbf{v}_{\emptyset}\right)}{2} = \mathbf{v}_i\cdot\mathbf{v}_{\emptyset} + \mathbf{v}_j\mathbf{v}_{\emptyset} - 2 \mathbf{v}_j\cdot\mathbf{v}_i = y_{i} + y_{j} + 2y_{i,j} = \zeta_{i,j}.\]
\end{proof}

\section{Grothendieck's Inequality}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\renewcommand{\refname}{Reference} % Change the default bibliography title

\bibliography{bibliography} % Input your bibliography file

%----------------------------------------------------------------------------------------

\end{document}